{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp57\n",
    "\n",
    "exp03にkmeansの特徴量を追加\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22604\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22604\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp57_kmens_xgb/'\n",
    "    seed = 144\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"xgb\"\n",
    "    ver = \"exp57\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "# def train_and_evaluate(train, test):\n",
    "#     # Label encode categorical features\n",
    "#     cat_features = [\n",
    "#         \"B_30\",\n",
    "#         \"B_38\",\n",
    "#         \"D_114\",\n",
    "#         \"D_116\",\n",
    "#         \"D_117\",\n",
    "#         \"D_120\",\n",
    "#         \"D_126\",\n",
    "#         \"D_63\",\n",
    "#         \"D_64\",\n",
    "#         \"D_66\",\n",
    "#         \"D_68\"\n",
    "#     ]\n",
    "#     cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "#     for cat_col in cat_features:\n",
    "#         encoder = LabelEncoder()\n",
    "#         train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "#         test[cat_col] = encoder.transform(test[cat_col])\n",
    "#     # Round last float features to 2 decimal place\n",
    "#     num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "#     num_cols = [col for col in num_cols if 'last' in col]\n",
    "#     for col in num_cols:\n",
    "#         train[col + '_round2'] = train[col].round(2)\n",
    "#         test[col + '_round2'] = test[col].round(2)\n",
    "#     # Get the difference between last and mean\n",
    "#     num_cols = [col for col in train.columns if 'last' in col]\n",
    "#     num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "#     for col in num_cols:\n",
    "#         try:\n",
    "#             train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "#             test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "#         except:\n",
    "#             pass\n",
    "#     # Transform float64 and float32 to float16\n",
    "#     num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "#     for col in tqdm(num_cols):\n",
    "#         train[col] = train[col].astype(np.float16)\n",
    "#         test[col] = test[col].astype(np.float16)\n",
    "#     # Get feature list\n",
    "#     features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': CFG.metric,\n",
    "#         'boosting': CFG.boosting_type,\n",
    "#         'seed': CFG.seed,\n",
    "#         'num_leaves': 100,\n",
    "#         'learning_rate': 0.01,\n",
    "#         'feature_fraction': 0.20,\n",
    "#         'bagging_freq': 10,\n",
    "#         'bagging_fraction': 0.50,\n",
    "#         'n_jobs': -1,\n",
    "#         'lambda_l2': 2,\n",
    "#         'min_data_in_leaf': 40,\n",
    "#         }\n",
    "#     # Create a numpy array to store test predictions\n",
    "#     test_predictions = np.zeros(len(test))\n",
    "#     # Create a numpy array to store out of folds predictions\n",
    "#     oof_predictions = np.zeros(len(train))\n",
    "#     kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "#         print(' ')\n",
    "#         print('-'*50)\n",
    "#         print(f'Training fold {fold} with {len(features)} features...')\n",
    "#         x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "#         y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "#         lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "#         lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "#         model = lgb.train(\n",
    "#             params = params,\n",
    "#             train_set = lgb_train,\n",
    "#             num_boost_round = 10500,#10500\n",
    "#             valid_sets = [lgb_train, lgb_valid],\n",
    "#             early_stopping_rounds = 1500,\n",
    "#             verbose_eval = 500,\n",
    "#             feval = lgb_amex_metric\n",
    "#             )\n",
    "#         # Save best model\n",
    "#         joblib.dump(model, f'../output/Amex LGBM Dart CV 0.7977/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "#         # Predict validation\n",
    "#         val_pred = model.predict(x_val)\n",
    "#         # Add to out of folds array\n",
    "#         oof_predictions[val_ind] = val_pred\n",
    "#         # Predict the test set\n",
    "#         test_pred = model.predict(test[features])\n",
    "#         test_predictions += test_pred / CFG.n_folds\n",
    "#         # Compute fold metric\n",
    "#         score = amex_metric(y_val, val_pred)\n",
    "#         print(f'Our fold {fold} CV score is {score}')\n",
    "#         del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "#         gc.collect()\n",
    "#     # Compute out of folds metric\n",
    "#     score = amex_metric(train[CFG.target], oof_predictions)\n",
    "#     print(f'Our out of folds CV score is {score}')\n",
    "#     # Create a dataframe to store out of folds predictions\n",
    "#     oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "#     oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "#     # Create a dataframe to store test prediction\n",
    "#     test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "#     test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81e96cc-3556-4978-9d8b-c7148c48d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_trian = pd.read_pickle('../feature/exp14_kmeans/kmeans_feature_train.pkl')\n",
    "kmeans_test = pd.read_pickle('../feature/exp14_kmeans/kmeans_feature_test.pkl')\n",
    "\n",
    "train = train.merge(kmeans_trian,on = \"customer_ID\",how = \"left\")\n",
    "test = test.merge(kmeans_test,on = \"customer_ID\",how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d5aa37-ce2a-4fd7-a211-d4665fe081b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.fillna(value=0, inplace=True)\n",
    "test.fillna(value=0, inplace=True)\n",
    "\n",
    "## infを含むデータを外れ値（－１００００）に置換\n",
    "train = train.replace([np.inf, -np.inf],100000000000)\n",
    "test = test.replace([np.inf, -np.inf],100000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1472 features...\n",
      "[01:14:57] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68724\ttrain-amex:0.71710\tvalid-logloss:0.68727\tvalid-amex:0.70807\n",
      "[100]\ttrain-logloss:0.37633\ttrain-amex:0.77432\tvalid-logloss:0.38051\tvalid-amex:0.75928\n",
      "[200]\ttrain-logloss:0.27902\ttrain-amex:0.78571\tvalid-logloss:0.28689\tvalid-amex:0.76585\n",
      "[300]\ttrain-logloss:0.24100\ttrain-amex:0.79498\tvalid-logloss:0.25204\tvalid-amex:0.77080\n",
      "[400]\ttrain-logloss:0.22393\ttrain-amex:0.80231\tvalid-logloss:0.23768\tvalid-amex:0.77485\n",
      "[500]\ttrain-logloss:0.21499\ttrain-amex:0.80871\tvalid-logloss:0.23103\tvalid-amex:0.77771\n",
      "[600]\ttrain-logloss:0.20947\ttrain-amex:0.81408\tvalid-logloss:0.22752\tvalid-amex:0.78011\n",
      "[700]\ttrain-logloss:0.20513\ttrain-amex:0.81904\tvalid-logloss:0.22531\tvalid-amex:0.78275\n",
      "[800]\ttrain-logloss:0.20127\ttrain-amex:0.82443\tvalid-logloss:0.22376\tvalid-amex:0.78381\n",
      "[900]\ttrain-logloss:0.19793\ttrain-amex:0.82942\tvalid-logloss:0.22264\tvalid-amex:0.78537\n",
      "[1000]\ttrain-logloss:0.19510\ttrain-amex:0.83358\tvalid-logloss:0.22178\tvalid-amex:0.78582\n",
      "[1100]\ttrain-logloss:0.19235\ttrain-amex:0.83784\tvalid-logloss:0.22105\tvalid-amex:0.78749\n",
      "[1200]\ttrain-logloss:0.18978\ttrain-amex:0.84201\tvalid-logloss:0.22041\tvalid-amex:0.78887\n",
      "[1300]\ttrain-logloss:0.18745\ttrain-amex:0.84596\tvalid-logloss:0.21991\tvalid-amex:0.78934\n",
      "[1400]\ttrain-logloss:0.18525\ttrain-amex:0.84929\tvalid-logloss:0.21950\tvalid-amex:0.78948\n",
      "[1500]\ttrain-logloss:0.18315\ttrain-amex:0.85277\tvalid-logloss:0.21916\tvalid-amex:0.79014\n",
      "[1600]\ttrain-logloss:0.18128\ttrain-amex:0.85589\tvalid-logloss:0.21890\tvalid-amex:0.79034\n",
      "[1700]\ttrain-logloss:0.17927\ttrain-amex:0.85919\tvalid-logloss:0.21864\tvalid-amex:0.79089\n",
      "[1800]\ttrain-logloss:0.17738\ttrain-amex:0.86273\tvalid-logloss:0.21842\tvalid-amex:0.79126\n",
      "[1900]\ttrain-logloss:0.17562\ttrain-amex:0.86567\tvalid-logloss:0.21823\tvalid-amex:0.79118\n",
      "[2000]\ttrain-logloss:0.17387\ttrain-amex:0.86893\tvalid-logloss:0.21806\tvalid-amex:0.79154\n",
      "[2100]\ttrain-logloss:0.17202\ttrain-amex:0.87221\tvalid-logloss:0.21792\tvalid-amex:0.79094\n",
      "[2200]\ttrain-logloss:0.17049\ttrain-amex:0.87474\tvalid-logloss:0.21779\tvalid-amex:0.79061\n",
      "[2300]\ttrain-logloss:0.16872\ttrain-amex:0.87816\tvalid-logloss:0.21765\tvalid-amex:0.79131\n",
      "[2400]\ttrain-logloss:0.16708\ttrain-amex:0.88120\tvalid-logloss:0.21754\tvalid-amex:0.79160\n",
      "[2484]\ttrain-logloss:0.16576\ttrain-amex:0.88362\tvalid-logloss:0.21746\tvalid-amex:0.79106\n",
      "Our fold 0 CV score is 0.7908223621378787\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1472 features...\n",
      "[02:03:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68728\ttrain-amex:0.72033\tvalid-logloss:0.68723\tvalid-amex:0.71076\n",
      "[100]\ttrain-logloss:0.37637\ttrain-amex:0.77398\tvalid-logloss:0.38026\tvalid-amex:0.76206\n",
      "[200]\ttrain-logloss:0.27919\ttrain-amex:0.78450\tvalid-logloss:0.28643\tvalid-amex:0.76954\n",
      "[300]\ttrain-logloss:0.24125\ttrain-amex:0.79344\tvalid-logloss:0.25131\tvalid-amex:0.77489\n",
      "[400]\ttrain-logloss:0.22427\ttrain-amex:0.80053\tvalid-logloss:0.23680\tvalid-amex:0.77951\n",
      "[500]\ttrain-logloss:0.21527\ttrain-amex:0.80683\tvalid-logloss:0.23003\tvalid-amex:0.78283\n",
      "[600]\ttrain-logloss:0.20966\ttrain-amex:0.81259\tvalid-logloss:0.22653\tvalid-amex:0.78470\n",
      "[700]\ttrain-logloss:0.20529\ttrain-amex:0.81784\tvalid-logloss:0.22439\tvalid-amex:0.78641\n",
      "[800]\ttrain-logloss:0.20149\ttrain-amex:0.82374\tvalid-logloss:0.22292\tvalid-amex:0.78796\n",
      "[900]\ttrain-logloss:0.19809\ttrain-amex:0.82914\tvalid-logloss:0.22182\tvalid-amex:0.78922\n",
      "[1000]\ttrain-logloss:0.19518\ttrain-amex:0.83342\tvalid-logloss:0.22095\tvalid-amex:0.79017\n",
      "[1100]\ttrain-logloss:0.19243\ttrain-amex:0.83758\tvalid-logloss:0.22025\tvalid-amex:0.79141\n",
      "[1200]\ttrain-logloss:0.18990\ttrain-amex:0.84195\tvalid-logloss:0.21972\tvalid-amex:0.79171\n",
      "[1300]\ttrain-logloss:0.18750\ttrain-amex:0.84567\tvalid-logloss:0.21923\tvalid-amex:0.79267\n",
      "[1400]\ttrain-logloss:0.18520\ttrain-amex:0.84929\tvalid-logloss:0.21882\tvalid-amex:0.79321\n",
      "[1500]\ttrain-logloss:0.18301\ttrain-amex:0.85300\tvalid-logloss:0.21846\tvalid-amex:0.79331\n",
      "[1600]\ttrain-logloss:0.18101\ttrain-amex:0.85644\tvalid-logloss:0.21818\tvalid-amex:0.79328\n",
      "[1700]\ttrain-logloss:0.17906\ttrain-amex:0.85960\tvalid-logloss:0.21793\tvalid-amex:0.79336\n",
      "[1800]\ttrain-logloss:0.17712\ttrain-amex:0.86290\tvalid-logloss:0.21767\tvalid-amex:0.79400\n",
      "[1900]\ttrain-logloss:0.17525\ttrain-amex:0.86645\tvalid-logloss:0.21747\tvalid-amex:0.79432\n",
      "[2000]\ttrain-logloss:0.17351\ttrain-amex:0.86962\tvalid-logloss:0.21729\tvalid-amex:0.79460\n",
      "[2100]\ttrain-logloss:0.17169\ttrain-amex:0.87299\tvalid-logloss:0.21712\tvalid-amex:0.79514\n",
      "[2200]\ttrain-logloss:0.17007\ttrain-amex:0.87596\tvalid-logloss:0.21699\tvalid-amex:0.79512\n",
      "[2300]\ttrain-logloss:0.16837\ttrain-amex:0.87898\tvalid-logloss:0.21686\tvalid-amex:0.79580\n",
      "[2400]\ttrain-logloss:0.16674\ttrain-amex:0.88207\tvalid-logloss:0.21675\tvalid-amex:0.79606\n",
      "[2500]\ttrain-logloss:0.16514\ttrain-amex:0.88482\tvalid-logloss:0.21664\tvalid-amex:0.79606\n",
      "[2600]\ttrain-logloss:0.16362\ttrain-amex:0.88761\tvalid-logloss:0.21655\tvalid-amex:0.79643\n",
      "[2700]\ttrain-logloss:0.16214\ttrain-amex:0.89043\tvalid-logloss:0.21647\tvalid-amex:0.79640\n",
      "[2800]\ttrain-logloss:0.16056\ttrain-amex:0.89323\tvalid-logloss:0.21641\tvalid-amex:0.79613\n",
      "[2900]\ttrain-logloss:0.15909\ttrain-amex:0.89595\tvalid-logloss:0.21631\tvalid-amex:0.79625\n",
      "[3000]\ttrain-logloss:0.15776\ttrain-amex:0.89826\tvalid-logloss:0.21625\tvalid-amex:0.79671\n",
      "[3100]\ttrain-logloss:0.15626\ttrain-amex:0.90075\tvalid-logloss:0.21620\tvalid-amex:0.79690\n",
      "[3200]\ttrain-logloss:0.15484\ttrain-amex:0.90318\tvalid-logloss:0.21615\tvalid-amex:0.79677\n",
      "[3300]\ttrain-logloss:0.15339\ttrain-amex:0.90573\tvalid-logloss:0.21610\tvalid-amex:0.79660\n",
      "[3400]\ttrain-logloss:0.15200\ttrain-amex:0.90809\tvalid-logloss:0.21606\tvalid-amex:0.79663\n",
      "[3500]\ttrain-logloss:0.15071\ttrain-amex:0.91032\tvalid-logloss:0.21602\tvalid-amex:0.79688\n",
      "[3600]\ttrain-logloss:0.14930\ttrain-amex:0.91265\tvalid-logloss:0.21598\tvalid-amex:0.79668\n",
      "[3700]\ttrain-logloss:0.14800\ttrain-amex:0.91479\tvalid-logloss:0.21594\tvalid-amex:0.79726\n",
      "[3800]\ttrain-logloss:0.14676\ttrain-amex:0.91689\tvalid-logloss:0.21591\tvalid-amex:0.79707\n",
      "[3900]\ttrain-logloss:0.14540\ttrain-amex:0.91936\tvalid-logloss:0.21588\tvalid-amex:0.79731\n",
      "[4000]\ttrain-logloss:0.14402\ttrain-amex:0.92182\tvalid-logloss:0.21585\tvalid-amex:0.79747\n",
      "[4100]\ttrain-logloss:0.14268\ttrain-amex:0.92411\tvalid-logloss:0.21583\tvalid-amex:0.79723\n",
      "[4200]\ttrain-logloss:0.14146\ttrain-amex:0.92613\tvalid-logloss:0.21581\tvalid-amex:0.79711\n",
      "[4300]\ttrain-logloss:0.14026\ttrain-amex:0.92804\tvalid-logloss:0.21580\tvalid-amex:0.79730\n",
      "[4400]\ttrain-logloss:0.13904\ttrain-amex:0.93014\tvalid-logloss:0.21580\tvalid-amex:0.79724\n",
      "[4500]\ttrain-logloss:0.13791\ttrain-amex:0.93182\tvalid-logloss:0.21579\tvalid-amex:0.79691\n",
      "[4600]\ttrain-logloss:0.13662\ttrain-amex:0.93392\tvalid-logloss:0.21579\tvalid-amex:0.79685\n",
      "[4700]\ttrain-logloss:0.13544\ttrain-amex:0.93578\tvalid-logloss:0.21577\tvalid-amex:0.79699\n",
      "[4800]\ttrain-logloss:0.13426\ttrain-amex:0.93766\tvalid-logloss:0.21574\tvalid-amex:0.79693\n",
      "[4814]\ttrain-logloss:0.13411\ttrain-amex:0.93788\tvalid-logloss:0.21574\tvalid-amex:0.79708\n",
      "Our fold 1 CV score is 0.7967374574282142\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1472 features...\n",
      "[03:35:49] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68726\ttrain-amex:0.71709\tvalid-logloss:0.68724\tvalid-amex:0.71419\n",
      "[100]\ttrain-logloss:0.37679\ttrain-amex:0.77409\tvalid-logloss:0.37931\tvalid-amex:0.75970\n",
      "[200]\ttrain-logloss:0.27972\ttrain-amex:0.78550\tvalid-logloss:0.28482\tvalid-amex:0.76776\n",
      "[300]\ttrain-logloss:0.24180\ttrain-amex:0.79394\tvalid-logloss:0.24938\tvalid-amex:0.77301\n",
      "[400]\ttrain-logloss:0.22481\ttrain-amex:0.80132\tvalid-logloss:0.23458\tvalid-amex:0.77745\n",
      "[500]\ttrain-logloss:0.21589\ttrain-amex:0.80752\tvalid-logloss:0.22762\tvalid-amex:0.78150\n",
      "[600]\ttrain-logloss:0.21041\ttrain-amex:0.81271\tvalid-logloss:0.22394\tvalid-amex:0.78433\n",
      "[700]\ttrain-logloss:0.20609\ttrain-amex:0.81766\tvalid-logloss:0.22162\tvalid-amex:0.78713\n",
      "[800]\ttrain-logloss:0.20222\ttrain-amex:0.82303\tvalid-logloss:0.21996\tvalid-amex:0.78872\n",
      "[900]\ttrain-logloss:0.19896\ttrain-amex:0.82765\tvalid-logloss:0.21877\tvalid-amex:0.78995\n",
      "[1000]\ttrain-logloss:0.19607\ttrain-amex:0.83203\tvalid-logloss:0.21782\tvalid-amex:0.79074\n",
      "[1100]\ttrain-logloss:0.19328\ttrain-amex:0.83652\tvalid-logloss:0.21706\tvalid-amex:0.79200\n",
      "[1200]\ttrain-logloss:0.19073\ttrain-amex:0.84060\tvalid-logloss:0.21645\tvalid-amex:0.79305\n",
      "[1300]\ttrain-logloss:0.18836\ttrain-amex:0.84421\tvalid-logloss:0.21597\tvalid-amex:0.79313\n",
      "[1400]\ttrain-logloss:0.18607\ttrain-amex:0.84788\tvalid-logloss:0.21552\tvalid-amex:0.79376\n",
      "[1500]\ttrain-logloss:0.18387\ttrain-amex:0.85193\tvalid-logloss:0.21518\tvalid-amex:0.79484\n",
      "[1600]\ttrain-logloss:0.18182\ttrain-amex:0.85559\tvalid-logloss:0.21486\tvalid-amex:0.79562\n",
      "[1700]\ttrain-logloss:0.17991\ttrain-amex:0.85860\tvalid-logloss:0.21460\tvalid-amex:0.79574\n",
      "[1800]\ttrain-logloss:0.17800\ttrain-amex:0.86186\tvalid-logloss:0.21437\tvalid-amex:0.79583\n",
      "[1900]\ttrain-logloss:0.17614\ttrain-amex:0.86532\tvalid-logloss:0.21418\tvalid-amex:0.79594\n",
      "[2000]\ttrain-logloss:0.17432\ttrain-amex:0.86858\tvalid-logloss:0.21400\tvalid-amex:0.79634\n",
      "[2100]\ttrain-logloss:0.17259\ttrain-amex:0.87179\tvalid-logloss:0.21386\tvalid-amex:0.79622\n",
      "[2200]\ttrain-logloss:0.17094\ttrain-amex:0.87486\tvalid-logloss:0.21374\tvalid-amex:0.79662\n",
      "[2300]\ttrain-logloss:0.16926\ttrain-amex:0.87800\tvalid-logloss:0.21361\tvalid-amex:0.79660\n",
      "[2400]\ttrain-logloss:0.16757\ttrain-amex:0.88106\tvalid-logloss:0.21350\tvalid-amex:0.79700\n",
      "[2500]\ttrain-logloss:0.16601\ttrain-amex:0.88401\tvalid-logloss:0.21340\tvalid-amex:0.79663\n",
      "[2600]\ttrain-logloss:0.16438\ttrain-amex:0.88722\tvalid-logloss:0.21330\tvalid-amex:0.79715\n",
      "[2700]\ttrain-logloss:0.16280\ttrain-amex:0.88980\tvalid-logloss:0.21321\tvalid-amex:0.79713\n",
      "[2800]\ttrain-logloss:0.16134\ttrain-amex:0.89245\tvalid-logloss:0.21314\tvalid-amex:0.79743\n",
      "[2900]\ttrain-logloss:0.15979\ttrain-amex:0.89519\tvalid-logloss:0.21309\tvalid-amex:0.79732\n",
      "[3000]\ttrain-logloss:0.15836\ttrain-amex:0.89786\tvalid-logloss:0.21302\tvalid-amex:0.79766\n",
      "[3100]\ttrain-logloss:0.15698\ttrain-amex:0.90015\tvalid-logloss:0.21296\tvalid-amex:0.79808\n",
      "[3200]\ttrain-logloss:0.15558\ttrain-amex:0.90254\tvalid-logloss:0.21291\tvalid-amex:0.79801\n",
      "[3300]\ttrain-logloss:0.15418\ttrain-amex:0.90478\tvalid-logloss:0.21287\tvalid-amex:0.79803\n",
      "[3400]\ttrain-logloss:0.15284\ttrain-amex:0.90712\tvalid-logloss:0.21284\tvalid-amex:0.79806\n",
      "[3500]\ttrain-logloss:0.15148\ttrain-amex:0.90951\tvalid-logloss:0.21279\tvalid-amex:0.79795\n",
      "[3600]\ttrain-logloss:0.15008\ttrain-amex:0.91175\tvalid-logloss:0.21276\tvalid-amex:0.79817\n",
      "[3700]\ttrain-logloss:0.14870\ttrain-amex:0.91424\tvalid-logloss:0.21274\tvalid-amex:0.79806\n",
      "[3800]\ttrain-logloss:0.14744\ttrain-amex:0.91639\tvalid-logloss:0.21269\tvalid-amex:0.79828\n",
      "[3900]\ttrain-logloss:0.14613\ttrain-amex:0.91864\tvalid-logloss:0.21269\tvalid-amex:0.79786\n",
      "[4000]\ttrain-logloss:0.14478\ttrain-amex:0.92080\tvalid-logloss:0.21268\tvalid-amex:0.79816\n",
      "[4100]\ttrain-logloss:0.14355\ttrain-amex:0.92295\tvalid-logloss:0.21265\tvalid-amex:0.79817\n",
      "[4200]\ttrain-logloss:0.14232\ttrain-amex:0.92507\tvalid-logloss:0.21263\tvalid-amex:0.79879\n",
      "[4300]\ttrain-logloss:0.14112\ttrain-amex:0.92704\tvalid-logloss:0.21260\tvalid-amex:0.79883\n",
      "[4400]\ttrain-logloss:0.13986\ttrain-amex:0.92912\tvalid-logloss:0.21257\tvalid-amex:0.79901\n",
      "[4500]\ttrain-logloss:0.13873\ttrain-amex:0.93097\tvalid-logloss:0.21258\tvalid-amex:0.79898\n",
      "[4600]\ttrain-logloss:0.13743\ttrain-amex:0.93318\tvalid-logloss:0.21257\tvalid-amex:0.79865\n",
      "[4700]\ttrain-logloss:0.13625\ttrain-amex:0.93495\tvalid-logloss:0.21256\tvalid-amex:0.79860\n",
      "[4770]\ttrain-logloss:0.13542\ttrain-amex:0.93627\tvalid-logloss:0.21256\tvalid-amex:0.79900\n",
      "Our fold 2 CV score is 0.7987576779281107\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1472 features...\n",
      "[05:06:14] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68724\ttrain-amex:0.71325\tvalid-logloss:0.68727\tvalid-amex:0.70704\n",
      "[100]\ttrain-logloss:0.37613\ttrain-amex:0.77499\tvalid-logloss:0.38104\tvalid-amex:0.75693\n",
      "[200]\ttrain-logloss:0.27883\ttrain-amex:0.78589\tvalid-logloss:0.28756\tvalid-amex:0.76395\n",
      "[300]\ttrain-logloss:0.24078\ttrain-amex:0.79488\tvalid-logloss:0.25272\tvalid-amex:0.76974\n",
      "[400]\ttrain-logloss:0.22376\ttrain-amex:0.80257\tvalid-logloss:0.23822\tvalid-amex:0.77329\n",
      "[500]\ttrain-logloss:0.21483\ttrain-amex:0.80882\tvalid-logloss:0.23154\tvalid-amex:0.77614\n",
      "[600]\ttrain-logloss:0.20926\ttrain-amex:0.81393\tvalid-logloss:0.22804\tvalid-amex:0.77853\n",
      "[700]\ttrain-logloss:0.20490\ttrain-amex:0.81895\tvalid-logloss:0.22588\tvalid-amex:0.78089\n",
      "[800]\ttrain-logloss:0.20096\ttrain-amex:0.82465\tvalid-logloss:0.22434\tvalid-amex:0.78341\n",
      "[900]\ttrain-logloss:0.19751\ttrain-amex:0.82983\tvalid-logloss:0.22322\tvalid-amex:0.78477\n",
      "[1000]\ttrain-logloss:0.19453\ttrain-amex:0.83457\tvalid-logloss:0.22238\tvalid-amex:0.78576\n",
      "[1100]\ttrain-logloss:0.19161\ttrain-amex:0.83913\tvalid-logloss:0.22163\tvalid-amex:0.78667\n",
      "[1200]\ttrain-logloss:0.18907\ttrain-amex:0.84330\tvalid-logloss:0.22107\tvalid-amex:0.78692\n",
      "[1300]\ttrain-logloss:0.18669\ttrain-amex:0.84737\tvalid-logloss:0.22060\tvalid-amex:0.78812\n",
      "[1400]\ttrain-logloss:0.18429\ttrain-amex:0.85143\tvalid-logloss:0.22022\tvalid-amex:0.78817\n",
      "[1500]\ttrain-logloss:0.18223\ttrain-amex:0.85502\tvalid-logloss:0.21988\tvalid-amex:0.78881\n",
      "[1600]\ttrain-logloss:0.18013\ttrain-amex:0.85849\tvalid-logloss:0.21963\tvalid-amex:0.78899\n",
      "[1700]\ttrain-logloss:0.17812\ttrain-amex:0.86200\tvalid-logloss:0.21937\tvalid-amex:0.78918\n",
      "[1800]\ttrain-logloss:0.17625\ttrain-amex:0.86524\tvalid-logloss:0.21918\tvalid-amex:0.78931\n",
      "[1900]\ttrain-logloss:0.17439\ttrain-amex:0.86845\tvalid-logloss:0.21901\tvalid-amex:0.78908\n",
      "[2000]\ttrain-logloss:0.17259\ttrain-amex:0.87183\tvalid-logloss:0.21885\tvalid-amex:0.78903\n",
      "[2100]\ttrain-logloss:0.17079\ttrain-amex:0.87506\tvalid-logloss:0.21871\tvalid-amex:0.78941\n",
      "[2200]\ttrain-logloss:0.16914\ttrain-amex:0.87798\tvalid-logloss:0.21860\tvalid-amex:0.78977\n",
      "[2300]\ttrain-logloss:0.16753\ttrain-amex:0.88105\tvalid-logloss:0.21851\tvalid-amex:0.78987\n",
      "[2400]\ttrain-logloss:0.16584\ttrain-amex:0.88392\tvalid-logloss:0.21842\tvalid-amex:0.79007\n",
      "[2500]\ttrain-logloss:0.16420\ttrain-amex:0.88692\tvalid-logloss:0.21834\tvalid-amex:0.79022\n",
      "[2600]\ttrain-logloss:0.16274\ttrain-amex:0.88947\tvalid-logloss:0.21827\tvalid-amex:0.79004\n",
      "[2700]\ttrain-logloss:0.16127\ttrain-amex:0.89204\tvalid-logloss:0.21823\tvalid-amex:0.78995\n",
      "[2800]\ttrain-logloss:0.15972\ttrain-amex:0.89470\tvalid-logloss:0.21818\tvalid-amex:0.79060\n",
      "[2900]\ttrain-logloss:0.15822\ttrain-amex:0.89731\tvalid-logloss:0.21813\tvalid-amex:0.79073\n",
      "[3000]\ttrain-logloss:0.15671\ttrain-amex:0.89986\tvalid-logloss:0.21808\tvalid-amex:0.79087\n",
      "[3100]\ttrain-logloss:0.15532\ttrain-amex:0.90233\tvalid-logloss:0.21804\tvalid-amex:0.79110\n",
      "[3200]\ttrain-logloss:0.15391\ttrain-amex:0.90470\tvalid-logloss:0.21799\tvalid-amex:0.79154\n",
      "[3300]\ttrain-logloss:0.15252\ttrain-amex:0.90723\tvalid-logloss:0.21798\tvalid-amex:0.79163\n",
      "[3400]\ttrain-logloss:0.15102\ttrain-amex:0.90982\tvalid-logloss:0.21795\tvalid-amex:0.79160\n",
      "[3500]\ttrain-logloss:0.14962\ttrain-amex:0.91238\tvalid-logloss:0.21790\tvalid-amex:0.79198\n",
      "[3600]\ttrain-logloss:0.14828\ttrain-amex:0.91486\tvalid-logloss:0.21788\tvalid-amex:0.79203\n",
      "[3700]\ttrain-logloss:0.14699\ttrain-amex:0.91704\tvalid-logloss:0.21785\tvalid-amex:0.79204\n",
      "[3800]\ttrain-logloss:0.14572\ttrain-amex:0.91902\tvalid-logloss:0.21783\tvalid-amex:0.79203\n",
      "[3900]\ttrain-logloss:0.14456\ttrain-amex:0.92100\tvalid-logloss:0.21782\tvalid-amex:0.79227\n",
      "[4000]\ttrain-logloss:0.14318\ttrain-amex:0.92320\tvalid-logloss:0.21779\tvalid-amex:0.79199\n",
      "[4100]\ttrain-logloss:0.14182\ttrain-amex:0.92542\tvalid-logloss:0.21779\tvalid-amex:0.79199\n",
      "[4200]\ttrain-logloss:0.14053\ttrain-amex:0.92763\tvalid-logloss:0.21779\tvalid-amex:0.79219\n",
      "[4300]\ttrain-logloss:0.13935\ttrain-amex:0.92947\tvalid-logloss:0.21779\tvalid-amex:0.79213\n",
      "[4400]\ttrain-logloss:0.13812\ttrain-amex:0.93132\tvalid-logloss:0.21779\tvalid-amex:0.79215\n",
      "[4440]\ttrain-logloss:0.13761\ttrain-amex:0.93209\tvalid-logloss:0.21779\tvalid-amex:0.79217\n",
      "Our fold 3 CV score is 0.7919596557228819\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1472 features...\n",
      "[06:28:37] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68726\ttrain-amex:0.71739\tvalid-logloss:0.68727\tvalid-amex:0.71107\n",
      "[100]\ttrain-logloss:0.37637\ttrain-amex:0.77479\tvalid-logloss:0.38014\tvalid-amex:0.75909\n",
      "[200]\ttrain-logloss:0.27907\ttrain-amex:0.78601\tvalid-logloss:0.28638\tvalid-amex:0.76626\n",
      "[300]\ttrain-logloss:0.24099\ttrain-amex:0.79467\tvalid-logloss:0.25161\tvalid-amex:0.77257\n",
      "[400]\ttrain-logloss:0.22399\ttrain-amex:0.80235\tvalid-logloss:0.23727\tvalid-amex:0.77594\n",
      "[500]\ttrain-logloss:0.21511\ttrain-amex:0.80795\tvalid-logloss:0.23069\tvalid-amex:0.77903\n",
      "[600]\ttrain-logloss:0.20953\ttrain-amex:0.81336\tvalid-logloss:0.22724\tvalid-amex:0.78124\n",
      "[700]\ttrain-logloss:0.20528\ttrain-amex:0.81856\tvalid-logloss:0.22513\tvalid-amex:0.78266\n",
      "[800]\ttrain-logloss:0.20160\ttrain-amex:0.82444\tvalid-logloss:0.22364\tvalid-amex:0.78367\n",
      "[900]\ttrain-logloss:0.19817\ttrain-amex:0.82949\tvalid-logloss:0.22249\tvalid-amex:0.78480\n",
      "[1000]\ttrain-logloss:0.19528\ttrain-amex:0.83378\tvalid-logloss:0.22162\tvalid-amex:0.78638\n",
      "[1100]\ttrain-logloss:0.19255\ttrain-amex:0.83809\tvalid-logloss:0.22087\tvalid-amex:0.78748\n",
      "[1200]\ttrain-logloss:0.19002\ttrain-amex:0.84216\tvalid-logloss:0.22028\tvalid-amex:0.78788\n",
      "[1300]\ttrain-logloss:0.18766\ttrain-amex:0.84625\tvalid-logloss:0.21980\tvalid-amex:0.78922\n",
      "[1400]\ttrain-logloss:0.18548\ttrain-amex:0.84941\tvalid-logloss:0.21941\tvalid-amex:0.78938\n",
      "[1500]\ttrain-logloss:0.18325\ttrain-amex:0.85337\tvalid-logloss:0.21907\tvalid-amex:0.78967\n",
      "[1600]\ttrain-logloss:0.18115\ttrain-amex:0.85694\tvalid-logloss:0.21875\tvalid-amex:0.79107\n",
      "[1700]\ttrain-logloss:0.17924\ttrain-amex:0.86019\tvalid-logloss:0.21850\tvalid-amex:0.79087\n",
      "[1800]\ttrain-logloss:0.17724\ttrain-amex:0.86340\tvalid-logloss:0.21824\tvalid-amex:0.79164\n",
      "[1900]\ttrain-logloss:0.17545\ttrain-amex:0.86668\tvalid-logloss:0.21805\tvalid-amex:0.79207\n",
      "[2000]\ttrain-logloss:0.17362\ttrain-amex:0.87013\tvalid-logloss:0.21787\tvalid-amex:0.79213\n",
      "[2100]\ttrain-logloss:0.17187\ttrain-amex:0.87316\tvalid-logloss:0.21771\tvalid-amex:0.79234\n",
      "[2200]\ttrain-logloss:0.17027\ttrain-amex:0.87594\tvalid-logloss:0.21758\tvalid-amex:0.79274\n",
      "[2300]\ttrain-logloss:0.16858\ttrain-amex:0.87892\tvalid-logloss:0.21746\tvalid-amex:0.79268\n",
      "[2400]\ttrain-logloss:0.16696\ttrain-amex:0.88168\tvalid-logloss:0.21734\tvalid-amex:0.79300\n",
      "[2500]\ttrain-logloss:0.16539\ttrain-amex:0.88447\tvalid-logloss:0.21722\tvalid-amex:0.79342\n",
      "[2600]\ttrain-logloss:0.16379\ttrain-amex:0.88750\tvalid-logloss:0.21715\tvalid-amex:0.79372\n",
      "[2700]\ttrain-logloss:0.16231\ttrain-amex:0.88988\tvalid-logloss:0.21706\tvalid-amex:0.79346\n",
      "[2800]\ttrain-logloss:0.16076\ttrain-amex:0.89254\tvalid-logloss:0.21697\tvalid-amex:0.79329\n",
      "[2900]\ttrain-logloss:0.15920\ttrain-amex:0.89536\tvalid-logloss:0.21689\tvalid-amex:0.79285\n",
      "[3000]\ttrain-logloss:0.15780\ttrain-amex:0.89811\tvalid-logloss:0.21683\tvalid-amex:0.79310\n",
      "[3093]\ttrain-logloss:0.15651\ttrain-amex:0.90051\tvalid-logloss:0.21675\tvalid-amex:0.79301\n",
      "Our fold 4 CV score is 0.7927988900735398\n",
      "Our out of folds CV score is 0.021295564071441137\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "# for cat_col in cat_features:\n",
    "# #     print(cat_col)\n",
    "#     encoder = LabelEncoder()\n",
    "#     train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "#     test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "\n",
    "xgb_parameters={\n",
    "        'max_depth': 9,#7 # optuna 9\n",
    "        'eta': 0.03,\n",
    "        'subsample': 0.88,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'objective': 'binary:logistic',\n",
    "        'learning_rate' : 0.008971755769136095,#0.026582608139330333\n",
    "        'tree_method': 'hist',#gpu_hist\n",
    "        # 'predictor': 'gpu_predictor',\n",
    "        'random_state': 42,\n",
    "        'gamma': 1.5,\n",
    "        'min_child_weight': 19,#8 # optuna 19\n",
    "        'lambda': 70,\n",
    "    }\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "# oof_predictions = np.zeros(len(train))\n",
    "oof_predictions = []\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "# epoch = [10000,7500,7500,8500,10500]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(data=x_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(data=test[features])\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "  \n",
    "    model = xgb.train(\n",
    "            xgb_parameters,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=9999,#9999\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            early_stopping_rounds=500,\n",
    "            feval=xgb_amex,\n",
    "            maximize=True,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Save best model\n",
    "    model.save_model(f'{CFG.output_dir}{CFG.model}_{CFG.ver}_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.xgb')\n",
    "#     joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(dvalid)\n",
    "    # Add to out of folds array\n",
    "    # oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(dtest)\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, dtrain, dtest ,dvalid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_614_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    f\"{CFG.ver}_{CFG.model}_oof\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_144_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.7941411572626582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute out of folds metric\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    f\"{CFG.ver}_{CFG.model}_oof\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_144_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "score = amex_metric(oof_df.target, oof_df.exp57_xgb_oof)\n",
    "\n",
    "print(f'Our out of folds CV score is {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp57_xgb_oof\n"
     ]
    }
   ],
   "source": [
    "print(f\"{CFG.ver}_{CFG.model}_oof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7fd5f-93df-42fb-afd4-07a17a6b8832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
