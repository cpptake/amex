{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# Amex LGBM Dart CV 0.7977\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21824\\1627570893.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21824\\1627570893.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 903/903 [02:13<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1188 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.225227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198249\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1180\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.336491\ttraining's amex_metric: 0.778155\tvalid_1's binary_logloss: 0.338307\tvalid_1's amex_metric: 0.77185\n",
      "[1000]\ttraining's binary_logloss: 0.245837\ttraining's amex_metric: 0.794435\tvalid_1's binary_logloss: 0.251082\tvalid_1's amex_metric: 0.782064\n",
      "[1500]\ttraining's binary_logloss: 0.221934\ttraining's amex_metric: 0.807645\tvalid_1's binary_logloss: 0.230916\tvalid_1's amex_metric: 0.790246\n",
      "[2000]\ttraining's binary_logloss: 0.208596\ttraining's amex_metric: 0.820032\tvalid_1's binary_logloss: 0.22228\tvalid_1's amex_metric: 0.793156\n",
      "[2500]\ttraining's binary_logloss: 0.201879\ttraining's amex_metric: 0.82984\tvalid_1's binary_logloss: 0.219511\tvalid_1's amex_metric: 0.79512\n",
      "[3000]\ttraining's binary_logloss: 0.195118\ttraining's amex_metric: 0.838883\tvalid_1's binary_logloss: 0.217329\tvalid_1's amex_metric: 0.796744\n",
      "[3500]\ttraining's binary_logloss: 0.18893\ttraining's amex_metric: 0.848594\tvalid_1's binary_logloss: 0.215813\tvalid_1's amex_metric: 0.798559\n",
      "[4000]\ttraining's binary_logloss: 0.183471\ttraining's amex_metric: 0.857952\tvalid_1's binary_logloss: 0.214905\tvalid_1's amex_metric: 0.799323\n",
      "[4500]\ttraining's binary_logloss: 0.178157\ttraining's amex_metric: 0.866908\tvalid_1's binary_logloss: 0.214204\tvalid_1's amex_metric: 0.80018\n",
      "[5000]\ttraining's binary_logloss: 0.172886\ttraining's amex_metric: 0.876158\tvalid_1's binary_logloss: 0.213663\tvalid_1's amex_metric: 0.799895\n",
      "[5500]\ttraining's binary_logloss: 0.168148\ttraining's amex_metric: 0.88429\tvalid_1's binary_logloss: 0.21336\tvalid_1's amex_metric: 0.800346\n",
      "[6000]\ttraining's binary_logloss: 0.164114\ttraining's amex_metric: 0.892125\tvalid_1's binary_logloss: 0.213143\tvalid_1's amex_metric: 0.800606\n",
      "[6500]\ttraining's binary_logloss: 0.159839\ttraining's amex_metric: 0.899092\tvalid_1's binary_logloss: 0.212836\tvalid_1's amex_metric: 0.800279\n",
      "[7000]\ttraining's binary_logloss: 0.154912\ttraining's amex_metric: 0.907064\tvalid_1's binary_logloss: 0.212571\tvalid_1's amex_metric: 0.801119\n",
      "[7500]\ttraining's binary_logloss: 0.150251\ttraining's amex_metric: 0.91468\tvalid_1's binary_logloss: 0.212407\tvalid_1's amex_metric: 0.800968\n",
      "[8000]\ttraining's binary_logloss: 0.146098\ttraining's amex_metric: 0.921653\tvalid_1's binary_logloss: 0.21228\tvalid_1's amex_metric: 0.801167\n",
      "[8500]\ttraining's binary_logloss: 0.142514\ttraining's amex_metric: 0.928169\tvalid_1's binary_logloss: 0.212171\tvalid_1's amex_metric: 0.801857\n",
      "[9000]\ttraining's binary_logloss: 0.138483\ttraining's amex_metric: 0.934165\tvalid_1's binary_logloss: 0.212081\tvalid_1's amex_metric: 0.801486\n",
      "[9500]\ttraining's binary_logloss: 0.134851\ttraining's amex_metric: 0.939694\tvalid_1's binary_logloss: 0.211978\tvalid_1's amex_metric: 0.801643\n",
      "[10000]\ttraining's binary_logloss: 0.131292\ttraining's amex_metric: 0.944939\tvalid_1's binary_logloss: 0.211912\tvalid_1's amex_metric: 0.801705\n",
      "[10500]\ttraining's binary_logloss: 0.128161\ttraining's amex_metric: 0.949552\tvalid_1's binary_logloss: 0.211887\tvalid_1's amex_metric: 0.80178\n",
      "Our fold 0 CV score is 0.8017798107751999\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1188 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.281137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198350\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1180\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.33618\ttraining's amex_metric: 0.780123\tvalid_1's binary_logloss: 0.339399\tvalid_1's amex_metric: 0.76395\n",
      "[1000]\ttraining's binary_logloss: 0.245224\ttraining's amex_metric: 0.7956\tvalid_1's binary_logloss: 0.252761\tvalid_1's amex_metric: 0.775\n",
      "[1500]\ttraining's binary_logloss: 0.221181\ttraining's amex_metric: 0.809026\tvalid_1's binary_logloss: 0.233106\tvalid_1's amex_metric: 0.781356\n",
      "[2000]\ttraining's binary_logloss: 0.207737\ttraining's amex_metric: 0.82206\tvalid_1's binary_logloss: 0.224832\tvalid_1's amex_metric: 0.785284\n",
      "[2500]\ttraining's binary_logloss: 0.20097\ttraining's amex_metric: 0.831546\tvalid_1's binary_logloss: 0.222282\tvalid_1's amex_metric: 0.787729\n",
      "[3000]\ttraining's binary_logloss: 0.194286\ttraining's amex_metric: 0.84056\tvalid_1's binary_logloss: 0.220325\tvalid_1's amex_metric: 0.788687\n",
      "[3500]\ttraining's binary_logloss: 0.188045\ttraining's amex_metric: 0.850082\tvalid_1's binary_logloss: 0.21896\tvalid_1's amex_metric: 0.790753\n",
      "[4000]\ttraining's binary_logloss: 0.182604\ttraining's amex_metric: 0.859134\tvalid_1's binary_logloss: 0.218171\tvalid_1's amex_metric: 0.791898\n",
      "[4500]\ttraining's binary_logloss: 0.177267\ttraining's amex_metric: 0.868503\tvalid_1's binary_logloss: 0.217554\tvalid_1's amex_metric: 0.791775\n",
      "[5000]\ttraining's binary_logloss: 0.172026\ttraining's amex_metric: 0.877071\tvalid_1's binary_logloss: 0.217144\tvalid_1's amex_metric: 0.791942\n",
      "[5500]\ttraining's binary_logloss: 0.167304\ttraining's amex_metric: 0.884989\tvalid_1's binary_logloss: 0.216873\tvalid_1's amex_metric: 0.792581\n",
      "[6000]\ttraining's binary_logloss: 0.163236\ttraining's amex_metric: 0.892968\tvalid_1's binary_logloss: 0.216651\tvalid_1's amex_metric: 0.793456\n",
      "[6500]\ttraining's binary_logloss: 0.159\ttraining's amex_metric: 0.900016\tvalid_1's binary_logloss: 0.216441\tvalid_1's amex_metric: 0.792687\n",
      "[7000]\ttraining's binary_logloss: 0.154075\ttraining's amex_metric: 0.907966\tvalid_1's binary_logloss: 0.216224\tvalid_1's amex_metric: 0.793615\n",
      "[7500]\ttraining's binary_logloss: 0.149428\ttraining's amex_metric: 0.915433\tvalid_1's binary_logloss: 0.216089\tvalid_1's amex_metric: 0.793467\n",
      "[8000]\ttraining's binary_logloss: 0.145282\ttraining's amex_metric: 0.922476\tvalid_1's binary_logloss: 0.216038\tvalid_1's amex_metric: 0.793041\n",
      "[8500]\ttraining's binary_logloss: 0.141711\ttraining's amex_metric: 0.928785\tvalid_1's binary_logloss: 0.215962\tvalid_1's amex_metric: 0.792669\n",
      "[9000]\ttraining's binary_logloss: 0.137688\ttraining's amex_metric: 0.935152\tvalid_1's binary_logloss: 0.215932\tvalid_1's amex_metric: 0.792994\n",
      "[9500]\ttraining's binary_logloss: 0.134058\ttraining's amex_metric: 0.940825\tvalid_1's binary_logloss: 0.215872\tvalid_1's amex_metric: 0.793755\n",
      "[10000]\ttraining's binary_logloss: 0.130473\ttraining's amex_metric: 0.946147\tvalid_1's binary_logloss: 0.21587\tvalid_1's amex_metric: 0.793298\n",
      "[10500]\ttraining's binary_logloss: 0.12736\ttraining's amex_metric: 0.950428\tvalid_1's binary_logloss: 0.215889\tvalid_1's amex_metric: 0.793974\n",
      "Our fold 1 CV score is 0.7939736011807267\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1188 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.362437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198295\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1180\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.336172\ttraining's amex_metric: 0.779018\tvalid_1's binary_logloss: 0.33984\tvalid_1's amex_metric: 0.767239\n",
      "[1000]\ttraining's binary_logloss: 0.245149\ttraining's amex_metric: 0.795038\tvalid_1's binary_logloss: 0.253063\tvalid_1's amex_metric: 0.777542\n",
      "[1500]\ttraining's binary_logloss: 0.22125\ttraining's amex_metric: 0.808745\tvalid_1's binary_logloss: 0.23316\tvalid_1's amex_metric: 0.784623\n",
      "[2000]\ttraining's binary_logloss: 0.207891\ttraining's amex_metric: 0.821456\tvalid_1's binary_logloss: 0.224628\tvalid_1's amex_metric: 0.788421\n",
      "[2500]\ttraining's binary_logloss: 0.201231\ttraining's amex_metric: 0.83094\tvalid_1's binary_logloss: 0.221931\tvalid_1's amex_metric: 0.790466\n",
      "[3000]\ttraining's binary_logloss: 0.194519\ttraining's amex_metric: 0.840241\tvalid_1's binary_logloss: 0.219755\tvalid_1's amex_metric: 0.79271\n",
      "[3500]\ttraining's binary_logloss: 0.188366\ttraining's amex_metric: 0.849955\tvalid_1's binary_logloss: 0.218319\tvalid_1's amex_metric: 0.794242\n",
      "[4000]\ttraining's binary_logloss: 0.182912\ttraining's amex_metric: 0.858947\tvalid_1's binary_logloss: 0.217374\tvalid_1's amex_metric: 0.795125\n",
      "[4500]\ttraining's binary_logloss: 0.177601\ttraining's amex_metric: 0.868615\tvalid_1's binary_logloss: 0.216711\tvalid_1's amex_metric: 0.795572\n",
      "[5000]\ttraining's binary_logloss: 0.172327\ttraining's amex_metric: 0.877327\tvalid_1's binary_logloss: 0.21615\tvalid_1's amex_metric: 0.795587\n",
      "[5500]\ttraining's binary_logloss: 0.167605\ttraining's amex_metric: 0.885758\tvalid_1's binary_logloss: 0.215696\tvalid_1's amex_metric: 0.796117\n",
      "[6000]\ttraining's binary_logloss: 0.163555\ttraining's amex_metric: 0.892855\tvalid_1's binary_logloss: 0.21547\tvalid_1's amex_metric: 0.796717\n",
      "[6500]\ttraining's binary_logloss: 0.159268\ttraining's amex_metric: 0.90021\tvalid_1's binary_logloss: 0.215183\tvalid_1's amex_metric: 0.797143\n",
      "[7000]\ttraining's binary_logloss: 0.154348\ttraining's amex_metric: 0.907715\tvalid_1's binary_logloss: 0.214975\tvalid_1's amex_metric: 0.797209\n",
      "[7500]\ttraining's binary_logloss: 0.149698\ttraining's amex_metric: 0.91541\tvalid_1's binary_logloss: 0.214763\tvalid_1's amex_metric: 0.79763\n",
      "[8000]\ttraining's binary_logloss: 0.145548\ttraining's amex_metric: 0.922039\tvalid_1's binary_logloss: 0.214687\tvalid_1's amex_metric: 0.79699\n",
      "[8500]\ttraining's binary_logloss: 0.142014\ttraining's amex_metric: 0.928659\tvalid_1's binary_logloss: 0.214671\tvalid_1's amex_metric: 0.796783\n",
      "[9000]\ttraining's binary_logloss: 0.137987\ttraining's amex_metric: 0.934707\tvalid_1's binary_logloss: 0.21459\tvalid_1's amex_metric: 0.796616\n",
      "[9500]\ttraining's binary_logloss: 0.13438\ttraining's amex_metric: 0.940259\tvalid_1's binary_logloss: 0.214572\tvalid_1's amex_metric: 0.796853\n",
      "[10000]\ttraining's binary_logloss: 0.130818\ttraining's amex_metric: 0.945553\tvalid_1's binary_logloss: 0.214536\tvalid_1's amex_metric: 0.797452\n",
      "[10500]\ttraining's binary_logloss: 0.127712\ttraining's amex_metric: 0.949907\tvalid_1's binary_logloss: 0.214558\tvalid_1's amex_metric: 0.797006\n",
      "Our fold 2 CV score is 0.7970060203949336\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1188 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.231770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198267\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1180\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.33589\ttraining's amex_metric: 0.779652\tvalid_1's binary_logloss: 0.340536\tvalid_1's amex_metric: 0.763204\n",
      "[1000]\ttraining's binary_logloss: 0.244979\ttraining's amex_metric: 0.796656\tvalid_1's binary_logloss: 0.254104\tvalid_1's amex_metric: 0.773153\n",
      "[1500]\ttraining's binary_logloss: 0.221041\ttraining's amex_metric: 0.809846\tvalid_1's binary_logloss: 0.234272\tvalid_1's amex_metric: 0.778973\n",
      "[2000]\ttraining's binary_logloss: 0.20766\ttraining's amex_metric: 0.822464\tvalid_1's binary_logloss: 0.225836\tvalid_1's amex_metric: 0.783833\n",
      "[2500]\ttraining's binary_logloss: 0.200935\ttraining's amex_metric: 0.831574\tvalid_1's binary_logloss: 0.223121\tvalid_1's amex_metric: 0.78703\n",
      "[3000]\ttraining's binary_logloss: 0.194205\ttraining's amex_metric: 0.840655\tvalid_1's binary_logloss: 0.220937\tvalid_1's amex_metric: 0.788445\n",
      "[3500]\ttraining's binary_logloss: 0.18801\ttraining's amex_metric: 0.850729\tvalid_1's binary_logloss: 0.21946\tvalid_1's amex_metric: 0.789753\n",
      "[4000]\ttraining's binary_logloss: 0.182575\ttraining's amex_metric: 0.860145\tvalid_1's binary_logloss: 0.218622\tvalid_1's amex_metric: 0.790132\n",
      "[4500]\ttraining's binary_logloss: 0.177273\ttraining's amex_metric: 0.868542\tvalid_1's binary_logloss: 0.217936\tvalid_1's amex_metric: 0.791327\n",
      "[5000]\ttraining's binary_logloss: 0.171985\ttraining's amex_metric: 0.877699\tvalid_1's binary_logloss: 0.21738\tvalid_1's amex_metric: 0.791436\n",
      "[5500]\ttraining's binary_logloss: 0.167296\ttraining's amex_metric: 0.885813\tvalid_1's binary_logloss: 0.217057\tvalid_1's amex_metric: 0.792007\n",
      "[6000]\ttraining's binary_logloss: 0.163255\ttraining's amex_metric: 0.893221\tvalid_1's binary_logloss: 0.216901\tvalid_1's amex_metric: 0.792222\n",
      "[6500]\ttraining's binary_logloss: 0.158995\ttraining's amex_metric: 0.899837\tvalid_1's binary_logloss: 0.216704\tvalid_1's amex_metric: 0.792622\n",
      "[7000]\ttraining's binary_logloss: 0.154079\ttraining's amex_metric: 0.907626\tvalid_1's binary_logloss: 0.216392\tvalid_1's amex_metric: 0.792793\n",
      "[7500]\ttraining's binary_logloss: 0.149455\ttraining's amex_metric: 0.915396\tvalid_1's binary_logloss: 0.21626\tvalid_1's amex_metric: 0.792833\n",
      "[8000]\ttraining's binary_logloss: 0.14529\ttraining's amex_metric: 0.922133\tvalid_1's binary_logloss: 0.216109\tvalid_1's amex_metric: 0.792857\n",
      "[8500]\ttraining's binary_logloss: 0.141749\ttraining's amex_metric: 0.92885\tvalid_1's binary_logloss: 0.216013\tvalid_1's amex_metric: 0.793062\n",
      "[9000]\ttraining's binary_logloss: 0.13771\ttraining's amex_metric: 0.934672\tvalid_1's binary_logloss: 0.21597\tvalid_1's amex_metric: 0.793225\n",
      "[9500]\ttraining's binary_logloss: 0.134078\ttraining's amex_metric: 0.940244\tvalid_1's binary_logloss: 0.215941\tvalid_1's amex_metric: 0.793785\n",
      "[10000]\ttraining's binary_logloss: 0.130483\ttraining's amex_metric: 0.945691\tvalid_1's binary_logloss: 0.21592\tvalid_1's amex_metric: 0.793752\n",
      "[10500]\ttraining's binary_logloss: 0.127399\ttraining's amex_metric: 0.950317\tvalid_1's binary_logloss: 0.2159\tvalid_1's amex_metric: 0.794266\n",
      "Our fold 3 CV score is 0.7942663393802515\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1188 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.224374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 198318\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1180\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.336413\ttraining's amex_metric: 0.778308\tvalid_1's binary_logloss: 0.339418\tvalid_1's amex_metric: 0.765856\n",
      "[1000]\ttraining's binary_logloss: 0.245665\ttraining's amex_metric: 0.795293\tvalid_1's binary_logloss: 0.25235\tvalid_1's amex_metric: 0.777481\n",
      "[1500]\ttraining's binary_logloss: 0.221677\ttraining's amex_metric: 0.808902\tvalid_1's binary_logloss: 0.232084\tvalid_1's amex_metric: 0.782948\n",
      "[2000]\ttraining's binary_logloss: 0.208354\ttraining's amex_metric: 0.820734\tvalid_1's binary_logloss: 0.223403\tvalid_1's amex_metric: 0.788289\n",
      "[2500]\ttraining's binary_logloss: 0.201653\ttraining's amex_metric: 0.8303\tvalid_1's binary_logloss: 0.22061\tvalid_1's amex_metric: 0.790854\n",
      "[3000]\ttraining's binary_logloss: 0.19492\ttraining's amex_metric: 0.839536\tvalid_1's binary_logloss: 0.218403\tvalid_1's amex_metric: 0.793365\n",
      "[3500]\ttraining's binary_logloss: 0.188717\ttraining's amex_metric: 0.849314\tvalid_1's binary_logloss: 0.217019\tvalid_1's amex_metric: 0.793837\n",
      "[4000]\ttraining's binary_logloss: 0.18324\ttraining's amex_metric: 0.858944\tvalid_1's binary_logloss: 0.216182\tvalid_1's amex_metric: 0.795117\n",
      "[4500]\ttraining's binary_logloss: 0.177872\ttraining's amex_metric: 0.867719\tvalid_1's binary_logloss: 0.21547\tvalid_1's amex_metric: 0.796326\n",
      "[5000]\ttraining's binary_logloss: 0.172578\ttraining's amex_metric: 0.87639\tvalid_1's binary_logloss: 0.214891\tvalid_1's amex_metric: 0.796966\n",
      "[5500]\ttraining's binary_logloss: 0.167882\ttraining's amex_metric: 0.885057\tvalid_1's binary_logloss: 0.214578\tvalid_1's amex_metric: 0.797231\n",
      "[6000]\ttraining's binary_logloss: 0.163859\ttraining's amex_metric: 0.892134\tvalid_1's binary_logloss: 0.214407\tvalid_1's amex_metric: 0.797006\n",
      "[6500]\ttraining's binary_logloss: 0.159584\ttraining's amex_metric: 0.899298\tvalid_1's binary_logloss: 0.214189\tvalid_1's amex_metric: 0.797424\n",
      "[7000]\ttraining's binary_logloss: 0.154671\ttraining's amex_metric: 0.906989\tvalid_1's binary_logloss: 0.213959\tvalid_1's amex_metric: 0.797678\n",
      "[7500]\ttraining's binary_logloss: 0.150038\ttraining's amex_metric: 0.914838\tvalid_1's binary_logloss: 0.213845\tvalid_1's amex_metric: 0.797954\n",
      "[8000]\ttraining's binary_logloss: 0.145857\ttraining's amex_metric: 0.922162\tvalid_1's binary_logloss: 0.21369\tvalid_1's amex_metric: 0.797996\n",
      "[8500]\ttraining's binary_logloss: 0.142297\ttraining's amex_metric: 0.928637\tvalid_1's binary_logloss: 0.213583\tvalid_1's amex_metric: 0.798134\n",
      "[9000]\ttraining's binary_logloss: 0.138262\ttraining's amex_metric: 0.934573\tvalid_1's binary_logloss: 0.213513\tvalid_1's amex_metric: 0.799344\n",
      "[9500]\ttraining's binary_logloss: 0.134642\ttraining's amex_metric: 0.940109\tvalid_1's binary_logloss: 0.213465\tvalid_1's amex_metric: 0.799292\n",
      "[10000]\ttraining's binary_logloss: 0.131046\ttraining's amex_metric: 0.945463\tvalid_1's binary_logloss: 0.213462\tvalid_1's amex_metric: 0.798803\n",
      "[10500]\ttraining's binary_logloss: 0.127917\ttraining's amex_metric: 0.949998\tvalid_1's binary_logloss: 0.213523\tvalid_1's amex_metric: 0.799091\n",
      "Our fold 4 CV score is 0.7990909419364174\n",
      "Our out of folds CV score is 0.7972918775396904\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../input/amex-fe/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': CFG.metric,\n",
    "        'boosting': CFG.boosting_type,\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40,\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,#10500\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 1500,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'../output/Amex LGBM Dart CV 0.7977/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    \n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5aa37-ce2a-4fd7-a211-d4665fe081b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce41aca-d2f7-48c8-bcf9-de91fc0533ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a799e7-7bba-44f3-aa16-6944104072ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
