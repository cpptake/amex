{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diff„ÅÆXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1824\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1824\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp51_615_lgb_lagdiff_c3_statedate/'\n",
    "    seed = 615\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp51\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 2025)\n",
      "(924621, 2024)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "train = pd.read_pickle('../feature/exp50_lagdiff_c3_statedate/train_lagdiff_c3_statedate.pkl')\n",
    "test = pd.read_pickle('../feature/exp50_lagdiff_c3_statedate/test_lagdiff_c3_statedate.pkl')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f272d-2129-4b46-a8de-80d78b84a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.058083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329992\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.478023\ttraining's amex_metric: 0.767321\tvalid_1's binary_logloss: 0.479542\tvalid_1's amex_metric: 0.757126\n",
      "[200]\ttraining's binary_logloss: 0.40403\ttraining's amex_metric: 0.772058\tvalid_1's binary_logloss: 0.406683\tvalid_1's amex_metric: 0.759745\n",
      "[300]\ttraining's binary_logloss: 0.368785\ttraining's amex_metric: 0.775378\tvalid_1's binary_logloss: 0.372196\tvalid_1's amex_metric: 0.762345\n",
      "[400]\ttraining's binary_logloss: 0.325168\ttraining's amex_metric: 0.779614\tvalid_1's binary_logloss: 0.329639\tvalid_1's amex_metric: 0.764534\n",
      "[500]\ttraining's binary_logloss: 0.304956\ttraining's amex_metric: 0.782429\tvalid_1's binary_logloss: 0.310063\tvalid_1's amex_metric: 0.767265\n",
      "[600]\ttraining's binary_logloss: 0.278448\ttraining's amex_metric: 0.786227\tvalid_1's binary_logloss: 0.284643\tvalid_1's amex_metric: 0.769343\n",
      "[700]\ttraining's binary_logloss: 0.262036\ttraining's amex_metric: 0.790168\tvalid_1's binary_logloss: 0.26919\tvalid_1's amex_metric: 0.771671\n",
      "[800]\ttraining's binary_logloss: 0.249784\ttraining's amex_metric: 0.793726\tvalid_1's binary_logloss: 0.257904\tvalid_1's amex_metric: 0.77527\n",
      "[900]\ttraining's binary_logloss: 0.240558\ttraining's amex_metric: 0.797149\tvalid_1's binary_logloss: 0.249635\tvalid_1's amex_metric: 0.77656\n",
      "[1000]\ttraining's binary_logloss: 0.236534\ttraining's amex_metric: 0.800155\tvalid_1's binary_logloss: 0.246316\tvalid_1's amex_metric: 0.777242\n",
      "[1100]\ttraining's binary_logloss: 0.233152\ttraining's amex_metric: 0.802709\tvalid_1's binary_logloss: 0.243558\tvalid_1's amex_metric: 0.778177\n",
      "[1200]\ttraining's binary_logloss: 0.228752\ttraining's amex_metric: 0.805191\tvalid_1's binary_logloss: 0.239959\tvalid_1's amex_metric: 0.778877\n",
      "[1300]\ttraining's binary_logloss: 0.223579\ttraining's amex_metric: 0.807864\tvalid_1's binary_logloss: 0.235814\tvalid_1's amex_metric: 0.780288\n",
      "[1400]\ttraining's binary_logloss: 0.219787\ttraining's amex_metric: 0.81052\tvalid_1's binary_logloss: 0.233002\tvalid_1's amex_metric: 0.781557\n",
      "[1500]\ttraining's binary_logloss: 0.218274\ttraining's amex_metric: 0.812708\tvalid_1's binary_logloss: 0.232109\tvalid_1's amex_metric: 0.781935\n",
      "[1600]\ttraining's binary_logloss: 0.215931\ttraining's amex_metric: 0.814771\tvalid_1's binary_logloss: 0.230613\tvalid_1's amex_metric: 0.78245\n",
      "[1700]\ttraining's binary_logloss: 0.213043\ttraining's amex_metric: 0.817289\tvalid_1's binary_logloss: 0.228748\tvalid_1's amex_metric: 0.783948\n",
      "[1800]\ttraining's binary_logloss: 0.210776\ttraining's amex_metric: 0.81946\tvalid_1's binary_logloss: 0.227389\tvalid_1's amex_metric: 0.784562\n",
      "[1900]\ttraining's binary_logloss: 0.208158\ttraining's amex_metric: 0.821702\tvalid_1's binary_logloss: 0.225916\tvalid_1's amex_metric: 0.785886\n",
      "[2000]\ttraining's binary_logloss: 0.205127\ttraining's amex_metric: 0.824909\tvalid_1's binary_logloss: 0.224319\tvalid_1's amex_metric: 0.786618\n",
      "[2100]\ttraining's binary_logloss: 0.20375\ttraining's amex_metric: 0.82738\tvalid_1's binary_logloss: 0.223805\tvalid_1's amex_metric: 0.787533\n",
      "[2200]\ttraining's binary_logloss: 0.202742\ttraining's amex_metric: 0.829918\tvalid_1's binary_logloss: 0.223581\tvalid_1's amex_metric: 0.787859\n",
      "[2300]\ttraining's binary_logloss: 0.201213\ttraining's amex_metric: 0.832344\tvalid_1's binary_logloss: 0.223026\tvalid_1's amex_metric: 0.788826\n",
      "[2400]\ttraining's binary_logloss: 0.199728\ttraining's amex_metric: 0.83431\tvalid_1's binary_logloss: 0.222488\tvalid_1's amex_metric: 0.789268\n",
      "[2500]\ttraining's binary_logloss: 0.198291\ttraining's amex_metric: 0.836526\tvalid_1's binary_logloss: 0.222055\tvalid_1's amex_metric: 0.789681\n",
      "[2600]\ttraining's binary_logloss: 0.196319\ttraining's amex_metric: 0.838935\tvalid_1's binary_logloss: 0.221352\tvalid_1's amex_metric: 0.789965\n",
      "[2700]\ttraining's binary_logloss: 0.194771\ttraining's amex_metric: 0.841176\tvalid_1's binary_logloss: 0.220896\tvalid_1's amex_metric: 0.790074\n",
      "[2800]\ttraining's binary_logloss: 0.193377\ttraining's amex_metric: 0.843313\tvalid_1's binary_logloss: 0.220543\tvalid_1's amex_metric: 0.790207\n",
      "[2900]\ttraining's binary_logloss: 0.191878\ttraining's amex_metric: 0.84586\tvalid_1's binary_logloss: 0.220209\tvalid_1's amex_metric: 0.790694\n",
      "[3000]\ttraining's binary_logloss: 0.190526\ttraining's amex_metric: 0.848039\tvalid_1's binary_logloss: 0.2199\tvalid_1's amex_metric: 0.791056\n",
      "[3100]\ttraining's binary_logloss: 0.189471\ttraining's amex_metric: 0.850075\tvalid_1's binary_logloss: 0.219754\tvalid_1's amex_metric: 0.791235\n",
      "[3200]\ttraining's binary_logloss: 0.188145\ttraining's amex_metric: 0.852181\tvalid_1's binary_logloss: 0.219511\tvalid_1's amex_metric: 0.791391\n",
      "[3300]\ttraining's binary_logloss: 0.186839\ttraining's amex_metric: 0.854391\tvalid_1's binary_logloss: 0.21927\tvalid_1's amex_metric: 0.791744\n",
      "[3400]\ttraining's binary_logloss: 0.185319\ttraining's amex_metric: 0.856553\tvalid_1's binary_logloss: 0.21894\tvalid_1's amex_metric: 0.791915\n",
      "[3500]\ttraining's binary_logloss: 0.184044\ttraining's amex_metric: 0.858699\tvalid_1's binary_logloss: 0.218707\tvalid_1's amex_metric: 0.791937\n",
      "[3600]\ttraining's binary_logloss: 0.183141\ttraining's amex_metric: 0.860486\tvalid_1's binary_logloss: 0.218593\tvalid_1's amex_metric: 0.792226\n",
      "[3700]\ttraining's binary_logloss: 0.182087\ttraining's amex_metric: 0.862008\tvalid_1's binary_logloss: 0.218469\tvalid_1's amex_metric: 0.792744\n",
      "[3800]\ttraining's binary_logloss: 0.181379\ttraining's amex_metric: 0.863841\tvalid_1's binary_logloss: 0.218413\tvalid_1's amex_metric: 0.79289\n",
      "[3900]\ttraining's binary_logloss: 0.180264\ttraining's amex_metric: 0.865864\tvalid_1's binary_logloss: 0.218264\tvalid_1's amex_metric: 0.792297\n",
      "[4000]\ttraining's binary_logloss: 0.179009\ttraining's amex_metric: 0.867483\tvalid_1's binary_logloss: 0.218076\tvalid_1's amex_metric: 0.792643\n",
      "[4100]\ttraining's binary_logloss: 0.177636\ttraining's amex_metric: 0.869479\tvalid_1's binary_logloss: 0.217874\tvalid_1's amex_metric: 0.792251\n",
      "[4200]\ttraining's binary_logloss: 0.176236\ttraining's amex_metric: 0.8717\tvalid_1's binary_logloss: 0.217656\tvalid_1's amex_metric: 0.79274\n",
      "[4300]\ttraining's binary_logloss: 0.175484\ttraining's amex_metric: 0.873596\tvalid_1's binary_logloss: 0.217625\tvalid_1's amex_metric: 0.792731\n",
      "[4400]\ttraining's binary_logloss: 0.174189\ttraining's amex_metric: 0.875424\tvalid_1's binary_logloss: 0.217462\tvalid_1's amex_metric: 0.792763\n",
      "[4500]\ttraining's binary_logloss: 0.173097\ttraining's amex_metric: 0.877153\tvalid_1's binary_logloss: 0.217321\tvalid_1's amex_metric: 0.792866\n",
      "[4600]\ttraining's binary_logloss: 0.171708\ttraining's amex_metric: 0.879305\tvalid_1's binary_logloss: 0.217157\tvalid_1's amex_metric: 0.79353\n",
      "[4700]\ttraining's binary_logloss: 0.170683\ttraining's amex_metric: 0.881192\tvalid_1's binary_logloss: 0.217107\tvalid_1's amex_metric: 0.793759\n",
      "[4800]\ttraining's binary_logloss: 0.16964\ttraining's amex_metric: 0.88297\tvalid_1's binary_logloss: 0.217017\tvalid_1's amex_metric: 0.793678\n",
      "[4900]\ttraining's binary_logloss: 0.168785\ttraining's amex_metric: 0.884531\tvalid_1's binary_logloss: 0.216982\tvalid_1's amex_metric: 0.793716\n",
      "[5000]\ttraining's binary_logloss: 0.167918\ttraining's amex_metric: 0.886437\tvalid_1's binary_logloss: 0.216931\tvalid_1's amex_metric: 0.793993\n",
      "[5100]\ttraining's binary_logloss: 0.16702\ttraining's amex_metric: 0.887797\tvalid_1's binary_logloss: 0.216888\tvalid_1's amex_metric: 0.79419\n",
      "[5200]\ttraining's binary_logloss: 0.165863\ttraining's amex_metric: 0.889521\tvalid_1's binary_logloss: 0.216815\tvalid_1's amex_metric: 0.793836\n",
      "[5300]\ttraining's binary_logloss: 0.165019\ttraining's amex_metric: 0.89108\tvalid_1's binary_logloss: 0.216782\tvalid_1's amex_metric: 0.793744\n",
      "[5400]\ttraining's binary_logloss: 0.163837\ttraining's amex_metric: 0.89296\tvalid_1's binary_logloss: 0.216714\tvalid_1's amex_metric: 0.793939\n",
      "[5500]\ttraining's binary_logloss: 0.162948\ttraining's amex_metric: 0.894597\tvalid_1's binary_logloss: 0.216659\tvalid_1's amex_metric: 0.793724\n",
      "[5600]\ttraining's binary_logloss: 0.16207\ttraining's amex_metric: 0.896232\tvalid_1's binary_logloss: 0.216605\tvalid_1's amex_metric: 0.794097\n",
      "[5700]\ttraining's binary_logloss: 0.161013\ttraining's amex_metric: 0.897792\tvalid_1's binary_logloss: 0.216553\tvalid_1's amex_metric: 0.794297\n",
      "[5800]\ttraining's binary_logloss: 0.160104\ttraining's amex_metric: 0.899422\tvalid_1's binary_logloss: 0.216511\tvalid_1's amex_metric: 0.794414\n",
      "[5900]\ttraining's binary_logloss: 0.159309\ttraining's amex_metric: 0.900826\tvalid_1's binary_logloss: 0.21646\tvalid_1's amex_metric: 0.794052\n",
      "[6000]\ttraining's binary_logloss: 0.158223\ttraining's amex_metric: 0.90247\tvalid_1's binary_logloss: 0.216455\tvalid_1's amex_metric: 0.794062\n",
      "[6100]\ttraining's binary_logloss: 0.157408\ttraining's amex_metric: 0.903875\tvalid_1's binary_logloss: 0.216408\tvalid_1's amex_metric: 0.793783\n",
      "[6200]\ttraining's binary_logloss: 0.156655\ttraining's amex_metric: 0.905448\tvalid_1's binary_logloss: 0.21641\tvalid_1's amex_metric: 0.793739\n",
      "[6300]\ttraining's binary_logloss: 0.155674\ttraining's amex_metric: 0.90684\tvalid_1's binary_logloss: 0.216352\tvalid_1's amex_metric: 0.793688\n",
      "[6400]\ttraining's binary_logloss: 0.154943\ttraining's amex_metric: 0.908124\tvalid_1's binary_logloss: 0.216371\tvalid_1's amex_metric: 0.793829\n",
      "[6500]\ttraining's binary_logloss: 0.153692\ttraining's amex_metric: 0.909709\tvalid_1's binary_logloss: 0.216317\tvalid_1's amex_metric: 0.793472\n",
      "[6600]\ttraining's binary_logloss: 0.152794\ttraining's amex_metric: 0.911449\tvalid_1's binary_logloss: 0.216277\tvalid_1's amex_metric: 0.793675\n",
      "[6700]\ttraining's binary_logloss: 0.15168\ttraining's amex_metric: 0.91291\tvalid_1's binary_logloss: 0.216219\tvalid_1's amex_metric: 0.793644\n",
      "[6800]\ttraining's binary_logloss: 0.150716\ttraining's amex_metric: 0.914316\tvalid_1's binary_logloss: 0.216189\tvalid_1's amex_metric: 0.793692\n",
      "[6900]\ttraining's binary_logloss: 0.150209\ttraining's amex_metric: 0.915799\tvalid_1's binary_logloss: 0.216175\tvalid_1's amex_metric: 0.794044\n",
      "[7000]\ttraining's binary_logloss: 0.149449\ttraining's amex_metric: 0.917236\tvalid_1's binary_logloss: 0.216141\tvalid_1's amex_metric: 0.794265\n",
      "[7100]\ttraining's binary_logloss: 0.148654\ttraining's amex_metric: 0.918629\tvalid_1's binary_logloss: 0.216115\tvalid_1's amex_metric: 0.794569\n",
      "[7200]\ttraining's binary_logloss: 0.147742\ttraining's amex_metric: 0.919931\tvalid_1's binary_logloss: 0.216055\tvalid_1's amex_metric: 0.794311\n",
      "[7300]\ttraining's binary_logloss: 0.146867\ttraining's amex_metric: 0.921365\tvalid_1's binary_logloss: 0.216024\tvalid_1's amex_metric: 0.794231\n",
      "[7400]\ttraining's binary_logloss: 0.145808\ttraining's amex_metric: 0.92269\tvalid_1's binary_logloss: 0.215965\tvalid_1's amex_metric: 0.794141\n",
      "[7500]\ttraining's binary_logloss: 0.145053\ttraining's amex_metric: 0.924324\tvalid_1's binary_logloss: 0.215965\tvalid_1's amex_metric: 0.793892\n",
      "[7600]\ttraining's binary_logloss: 0.144427\ttraining's amex_metric: 0.925362\tvalid_1's binary_logloss: 0.215972\tvalid_1's amex_metric: 0.794415\n",
      "[7700]\ttraining's binary_logloss: 0.143734\ttraining's amex_metric: 0.926503\tvalid_1's binary_logloss: 0.21599\tvalid_1's amex_metric: 0.794574\n",
      "[7800]\ttraining's binary_logloss: 0.142851\ttraining's amex_metric: 0.928057\tvalid_1's binary_logloss: 0.215971\tvalid_1's amex_metric: 0.793925\n",
      "[7900]\ttraining's binary_logloss: 0.14175\ttraining's amex_metric: 0.929433\tvalid_1's binary_logloss: 0.21592\tvalid_1's amex_metric: 0.794227\n",
      "[8000]\ttraining's binary_logloss: 0.140884\ttraining's amex_metric: 0.930514\tvalid_1's binary_logloss: 0.215888\tvalid_1's amex_metric: 0.794826\n",
      "[8100]\ttraining's binary_logloss: 0.139878\ttraining's amex_metric: 0.931949\tvalid_1's binary_logloss: 0.21585\tvalid_1's amex_metric: 0.794732\n",
      "[8200]\ttraining's binary_logloss: 0.138949\ttraining's amex_metric: 0.933649\tvalid_1's binary_logloss: 0.215797\tvalid_1's amex_metric: 0.794164\n",
      "[8300]\ttraining's binary_logloss: 0.138093\ttraining's amex_metric: 0.934844\tvalid_1's binary_logloss: 0.215776\tvalid_1's amex_metric: 0.79436\n",
      "[8400]\ttraining's binary_logloss: 0.137463\ttraining's amex_metric: 0.936203\tvalid_1's binary_logloss: 0.215787\tvalid_1's amex_metric: 0.793958\n",
      "[8500]\ttraining's binary_logloss: 0.136676\ttraining's amex_metric: 0.937508\tvalid_1's binary_logloss: 0.215781\tvalid_1's amex_metric: 0.794213\n",
      "[8600]\ttraining's binary_logloss: 0.136097\ttraining's amex_metric: 0.938748\tvalid_1's binary_logloss: 0.215798\tvalid_1's amex_metric: 0.793996\n",
      "[8700]\ttraining's binary_logloss: 0.135532\ttraining's amex_metric: 0.939735\tvalid_1's binary_logloss: 0.215778\tvalid_1's amex_metric: 0.794362\n",
      "[8800]\ttraining's binary_logloss: 0.134753\ttraining's amex_metric: 0.940915\tvalid_1's binary_logloss: 0.215753\tvalid_1's amex_metric: 0.794242\n",
      "[8900]\ttraining's binary_logloss: 0.133757\ttraining's amex_metric: 0.942203\tvalid_1's binary_logloss: 0.215769\tvalid_1's amex_metric: 0.794296\n",
      "[9000]\ttraining's binary_logloss: 0.133164\ttraining's amex_metric: 0.943258\tvalid_1's binary_logloss: 0.215728\tvalid_1's amex_metric: 0.794499\n",
      "[9100]\ttraining's binary_logloss: 0.132374\ttraining's amex_metric: 0.94436\tvalid_1's binary_logloss: 0.215714\tvalid_1's amex_metric: 0.794399\n",
      "[9200]\ttraining's binary_logloss: 0.131743\ttraining's amex_metric: 0.945279\tvalid_1's binary_logloss: 0.215719\tvalid_1's amex_metric: 0.794039\n",
      "[9300]\ttraining's binary_logloss: 0.131033\ttraining's amex_metric: 0.946336\tvalid_1's binary_logloss: 0.215697\tvalid_1's amex_metric: 0.79472\n",
      "[9400]\ttraining's binary_logloss: 0.130196\ttraining's amex_metric: 0.947602\tvalid_1's binary_logloss: 0.215702\tvalid_1's amex_metric: 0.795097\n",
      "[9500]\ttraining's binary_logloss: 0.129306\ttraining's amex_metric: 0.948774\tvalid_1's binary_logloss: 0.215692\tvalid_1's amex_metric: 0.794891\n",
      "[9600]\ttraining's binary_logloss: 0.128552\ttraining's amex_metric: 0.94989\tvalid_1's binary_logloss: 0.215677\tvalid_1's amex_metric: 0.795358\n",
      "[9700]\ttraining's binary_logloss: 0.127794\ttraining's amex_metric: 0.950962\tvalid_1's binary_logloss: 0.215664\tvalid_1's amex_metric: 0.79507\n",
      "[9800]\ttraining's binary_logloss: 0.127134\ttraining's amex_metric: 0.952112\tvalid_1's binary_logloss: 0.21566\tvalid_1's amex_metric: 0.795237\n",
      "[9900]\ttraining's binary_logloss: 0.126254\ttraining's amex_metric: 0.953242\tvalid_1's binary_logloss: 0.215641\tvalid_1's amex_metric: 0.79533\n",
      "Our fold 0 CV score is 0.7953296296357426\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.329978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329898\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2008\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.47823\ttraining's amex_metric: 0.7665\tvalid_1's binary_logloss: 0.478906\tvalid_1's amex_metric: 0.760943\n",
      "[200]\ttraining's binary_logloss: 0.404334\ttraining's amex_metric: 0.770391\tvalid_1's binary_logloss: 0.40554\tvalid_1's amex_metric: 0.764246\n",
      "[300]\ttraining's binary_logloss: 0.369492\ttraining's amex_metric: 0.773882\tvalid_1's binary_logloss: 0.371135\tvalid_1's amex_metric: 0.766317\n",
      "[400]\ttraining's binary_logloss: 0.325908\ttraining's amex_metric: 0.777666\tvalid_1's binary_logloss: 0.328171\tvalid_1's amex_metric: 0.76842\n",
      "[500]\ttraining's binary_logloss: 0.305772\ttraining's amex_metric: 0.780896\tvalid_1's binary_logloss: 0.30857\tvalid_1's amex_metric: 0.77117\n",
      "[600]\ttraining's binary_logloss: 0.27932\ttraining's amex_metric: 0.785242\tvalid_1's binary_logloss: 0.282884\tvalid_1's amex_metric: 0.773079\n",
      "[700]\ttraining's binary_logloss: 0.262864\ttraining's amex_metric: 0.789508\tvalid_1's binary_logloss: 0.267204\tvalid_1's amex_metric: 0.775236\n",
      "[800]\ttraining's binary_logloss: 0.250523\ttraining's amex_metric: 0.793063\tvalid_1's binary_logloss: 0.255702\tvalid_1's amex_metric: 0.777511\n",
      "[900]\ttraining's binary_logloss: 0.241301\ttraining's amex_metric: 0.796277\tvalid_1's binary_logloss: 0.247394\tvalid_1's amex_metric: 0.779477\n",
      "[1000]\ttraining's binary_logloss: 0.237307\ttraining's amex_metric: 0.799288\tvalid_1's binary_logloss: 0.244065\tvalid_1's amex_metric: 0.780717\n",
      "[1100]\ttraining's binary_logloss: 0.233977\ttraining's amex_metric: 0.801915\tvalid_1's binary_logloss: 0.241388\tvalid_1's amex_metric: 0.782045\n",
      "[1200]\ttraining's binary_logloss: 0.229528\ttraining's amex_metric: 0.804292\tvalid_1's binary_logloss: 0.237726\tvalid_1's amex_metric: 0.783178\n",
      "[1300]\ttraining's binary_logloss: 0.224364\ttraining's amex_metric: 0.806984\tvalid_1's binary_logloss: 0.233532\tvalid_1's amex_metric: 0.783689\n",
      "[1400]\ttraining's binary_logloss: 0.220628\ttraining's amex_metric: 0.809577\tvalid_1's binary_logloss: 0.230721\tvalid_1's amex_metric: 0.784504\n",
      "[1500]\ttraining's binary_logloss: 0.219092\ttraining's amex_metric: 0.811872\tvalid_1's binary_logloss: 0.229797\tvalid_1's amex_metric: 0.784605\n",
      "[1600]\ttraining's binary_logloss: 0.216698\ttraining's amex_metric: 0.813455\tvalid_1's binary_logloss: 0.228235\tvalid_1's amex_metric: 0.785437\n",
      "[1700]\ttraining's binary_logloss: 0.213805\ttraining's amex_metric: 0.81634\tvalid_1's binary_logloss: 0.226328\tvalid_1's amex_metric: 0.78688\n",
      "[1800]\ttraining's binary_logloss: 0.211524\ttraining's amex_metric: 0.818495\tvalid_1's binary_logloss: 0.224953\tvalid_1's amex_metric: 0.787593\n",
      "[1900]\ttraining's binary_logloss: 0.208881\ttraining's amex_metric: 0.821143\tvalid_1's binary_logloss: 0.223405\tvalid_1's amex_metric: 0.788307\n",
      "[2000]\ttraining's binary_logloss: 0.205835\ttraining's amex_metric: 0.824046\tvalid_1's binary_logloss: 0.221672\tvalid_1's amex_metric: 0.789462\n",
      "[2100]\ttraining's binary_logloss: 0.204461\ttraining's amex_metric: 0.826491\tvalid_1's binary_logloss: 0.221182\tvalid_1's amex_metric: 0.789753\n",
      "[2200]\ttraining's binary_logloss: 0.203499\ttraining's amex_metric: 0.828762\tvalid_1's binary_logloss: 0.220943\tvalid_1's amex_metric: 0.790246\n",
      "[2300]\ttraining's binary_logloss: 0.201986\ttraining's amex_metric: 0.831107\tvalid_1's binary_logloss: 0.220413\tvalid_1's amex_metric: 0.790481\n",
      "[2400]\ttraining's binary_logloss: 0.200513\ttraining's amex_metric: 0.833275\tvalid_1's binary_logloss: 0.21986\tvalid_1's amex_metric: 0.790725\n",
      "[2500]\ttraining's binary_logloss: 0.199074\ttraining's amex_metric: 0.835229\tvalid_1's binary_logloss: 0.219379\tvalid_1's amex_metric: 0.791379\n",
      "[2600]\ttraining's binary_logloss: 0.197108\ttraining's amex_metric: 0.837873\tvalid_1's binary_logloss: 0.218617\tvalid_1's amex_metric: 0.79179\n",
      "[2700]\ttraining's binary_logloss: 0.195555\ttraining's amex_metric: 0.840193\tvalid_1's binary_logloss: 0.218088\tvalid_1's amex_metric: 0.791774\n",
      "[2800]\ttraining's binary_logloss: 0.194157\ttraining's amex_metric: 0.842519\tvalid_1's binary_logloss: 0.217672\tvalid_1's amex_metric: 0.791394\n",
      "[2900]\ttraining's binary_logloss: 0.192684\ttraining's amex_metric: 0.844606\tvalid_1's binary_logloss: 0.217263\tvalid_1's amex_metric: 0.792091\n",
      "[3000]\ttraining's binary_logloss: 0.191361\ttraining's amex_metric: 0.846988\tvalid_1's binary_logloss: 0.216985\tvalid_1's amex_metric: 0.792353\n",
      "[3100]\ttraining's binary_logloss: 0.190312\ttraining's amex_metric: 0.849004\tvalid_1's binary_logloss: 0.216798\tvalid_1's amex_metric: 0.793051\n",
      "[3200]\ttraining's binary_logloss: 0.189001\ttraining's amex_metric: 0.850976\tvalid_1's binary_logloss: 0.216509\tvalid_1's amex_metric: 0.792801\n",
      "[3300]\ttraining's binary_logloss: 0.187691\ttraining's amex_metric: 0.852843\tvalid_1's binary_logloss: 0.216233\tvalid_1's amex_metric: 0.7933\n",
      "[3400]\ttraining's binary_logloss: 0.18619\ttraining's amex_metric: 0.855171\tvalid_1's binary_logloss: 0.215925\tvalid_1's amex_metric: 0.793536\n",
      "[3500]\ttraining's binary_logloss: 0.18491\ttraining's amex_metric: 0.857349\tvalid_1's binary_logloss: 0.21567\tvalid_1's amex_metric: 0.793968\n",
      "[3600]\ttraining's binary_logloss: 0.184013\ttraining's amex_metric: 0.859337\tvalid_1's binary_logloss: 0.215585\tvalid_1's amex_metric: 0.793955\n",
      "[3700]\ttraining's binary_logloss: 0.182946\ttraining's amex_metric: 0.861506\tvalid_1's binary_logloss: 0.215458\tvalid_1's amex_metric: 0.794202\n",
      "[3800]\ttraining's binary_logloss: 0.182243\ttraining's amex_metric: 0.862995\tvalid_1's binary_logloss: 0.215438\tvalid_1's amex_metric: 0.794317\n",
      "[3900]\ttraining's binary_logloss: 0.181126\ttraining's amex_metric: 0.865109\tvalid_1's binary_logloss: 0.215281\tvalid_1's amex_metric: 0.794533\n",
      "[4000]\ttraining's binary_logloss: 0.179851\ttraining's amex_metric: 0.867069\tvalid_1's binary_logloss: 0.215052\tvalid_1's amex_metric: 0.794459\n",
      "[4100]\ttraining's binary_logloss: 0.178454\ttraining's amex_metric: 0.868952\tvalid_1's binary_logloss: 0.214776\tvalid_1's amex_metric: 0.794573\n",
      "[4200]\ttraining's binary_logloss: 0.177055\ttraining's amex_metric: 0.871108\tvalid_1's binary_logloss: 0.214569\tvalid_1's amex_metric: 0.794731\n",
      "[4300]\ttraining's binary_logloss: 0.176312\ttraining's amex_metric: 0.872621\tvalid_1's binary_logloss: 0.214549\tvalid_1's amex_metric: 0.794675\n",
      "[4400]\ttraining's binary_logloss: 0.175018\ttraining's amex_metric: 0.87444\tvalid_1's binary_logloss: 0.214373\tvalid_1's amex_metric: 0.794624\n",
      "[4500]\ttraining's binary_logloss: 0.173917\ttraining's amex_metric: 0.87655\tvalid_1's binary_logloss: 0.214233\tvalid_1's amex_metric: 0.794822\n",
      "[4600]\ttraining's binary_logloss: 0.172522\ttraining's amex_metric: 0.878247\tvalid_1's binary_logloss: 0.214033\tvalid_1's amex_metric: 0.795487\n",
      "[4700]\ttraining's binary_logloss: 0.171481\ttraining's amex_metric: 0.880113\tvalid_1's binary_logloss: 0.213974\tvalid_1's amex_metric: 0.795419\n",
      "[4800]\ttraining's binary_logloss: 0.170442\ttraining's amex_metric: 0.881946\tvalid_1's binary_logloss: 0.213906\tvalid_1's amex_metric: 0.795709\n",
      "[4900]\ttraining's binary_logloss: 0.169591\ttraining's amex_metric: 0.883844\tvalid_1's binary_logloss: 0.213866\tvalid_1's amex_metric: 0.795771\n",
      "[5000]\ttraining's binary_logloss: 0.168728\ttraining's amex_metric: 0.885259\tvalid_1's binary_logloss: 0.213826\tvalid_1's amex_metric: 0.79526\n",
      "[5100]\ttraining's binary_logloss: 0.167826\ttraining's amex_metric: 0.887007\tvalid_1's binary_logloss: 0.2138\tvalid_1's amex_metric: 0.795468\n",
      "[5200]\ttraining's binary_logloss: 0.166667\ttraining's amex_metric: 0.888731\tvalid_1's binary_logloss: 0.213681\tvalid_1's amex_metric: 0.795676\n",
      "[5300]\ttraining's binary_logloss: 0.165812\ttraining's amex_metric: 0.890607\tvalid_1's binary_logloss: 0.213645\tvalid_1's amex_metric: 0.795396\n",
      "[5400]\ttraining's binary_logloss: 0.164624\ttraining's amex_metric: 0.892301\tvalid_1's binary_logloss: 0.21353\tvalid_1's amex_metric: 0.795727\n",
      "[5500]\ttraining's binary_logloss: 0.163739\ttraining's amex_metric: 0.894077\tvalid_1's binary_logloss: 0.213479\tvalid_1's amex_metric: 0.79547\n",
      "[5600]\ttraining's binary_logloss: 0.162876\ttraining's amex_metric: 0.895545\tvalid_1's binary_logloss: 0.213411\tvalid_1's amex_metric: 0.795424\n",
      "[5700]\ttraining's binary_logloss: 0.161829\ttraining's amex_metric: 0.897025\tvalid_1's binary_logloss: 0.213308\tvalid_1's amex_metric: 0.795785\n",
      "[5800]\ttraining's binary_logloss: 0.16092\ttraining's amex_metric: 0.898662\tvalid_1's binary_logloss: 0.213262\tvalid_1's amex_metric: 0.795652\n",
      "[5900]\ttraining's binary_logloss: 0.160121\ttraining's amex_metric: 0.900197\tvalid_1's binary_logloss: 0.213249\tvalid_1's amex_metric: 0.795448\n",
      "[6000]\ttraining's binary_logloss: 0.15905\ttraining's amex_metric: 0.901567\tvalid_1's binary_logloss: 0.213148\tvalid_1's amex_metric: 0.795445\n",
      "[6100]\ttraining's binary_logloss: 0.158221\ttraining's amex_metric: 0.902987\tvalid_1's binary_logloss: 0.213102\tvalid_1's amex_metric: 0.795313\n",
      "[6200]\ttraining's binary_logloss: 0.157462\ttraining's amex_metric: 0.904274\tvalid_1's binary_logloss: 0.213065\tvalid_1's amex_metric: 0.795666\n",
      "[6300]\ttraining's binary_logloss: 0.156472\ttraining's amex_metric: 0.905808\tvalid_1's binary_logloss: 0.213\tvalid_1's amex_metric: 0.795672\n",
      "[6400]\ttraining's binary_logloss: 0.155733\ttraining's amex_metric: 0.907405\tvalid_1's binary_logloss: 0.212989\tvalid_1's amex_metric: 0.795848\n",
      "[6500]\ttraining's binary_logloss: 0.154473\ttraining's amex_metric: 0.909219\tvalid_1's binary_logloss: 0.212875\tvalid_1's amex_metric: 0.795989\n",
      "[6600]\ttraining's binary_logloss: 0.153559\ttraining's amex_metric: 0.910882\tvalid_1's binary_logloss: 0.212825\tvalid_1's amex_metric: 0.795773\n",
      "[6700]\ttraining's binary_logloss: 0.152457\ttraining's amex_metric: 0.912306\tvalid_1's binary_logloss: 0.212763\tvalid_1's amex_metric: 0.795903\n",
      "[6800]\ttraining's binary_logloss: 0.151499\ttraining's amex_metric: 0.913933\tvalid_1's binary_logloss: 0.212729\tvalid_1's amex_metric: 0.79543\n",
      "[6900]\ttraining's binary_logloss: 0.150995\ttraining's amex_metric: 0.915231\tvalid_1's binary_logloss: 0.212756\tvalid_1's amex_metric: 0.795828\n",
      "[7000]\ttraining's binary_logloss: 0.150241\ttraining's amex_metric: 0.91648\tvalid_1's binary_logloss: 0.212731\tvalid_1's amex_metric: 0.79634\n",
      "[7100]\ttraining's binary_logloss: 0.149423\ttraining's amex_metric: 0.917636\tvalid_1's binary_logloss: 0.212694\tvalid_1's amex_metric: 0.796388\n",
      "[7200]\ttraining's binary_logloss: 0.148517\ttraining's amex_metric: 0.919056\tvalid_1's binary_logloss: 0.212635\tvalid_1's amex_metric: 0.796419\n",
      "[7300]\ttraining's binary_logloss: 0.147645\ttraining's amex_metric: 0.920209\tvalid_1's binary_logloss: 0.212587\tvalid_1's amex_metric: 0.797021\n",
      "[7400]\ttraining's binary_logloss: 0.146591\ttraining's amex_metric: 0.921525\tvalid_1's binary_logloss: 0.212495\tvalid_1's amex_metric: 0.796705\n",
      "[7500]\ttraining's binary_logloss: 0.145817\ttraining's amex_metric: 0.922948\tvalid_1's binary_logloss: 0.21248\tvalid_1's amex_metric: 0.796624\n",
      "[7600]\ttraining's binary_logloss: 0.145198\ttraining's amex_metric: 0.924262\tvalid_1's binary_logloss: 0.2125\tvalid_1's amex_metric: 0.796619\n",
      "[7700]\ttraining's binary_logloss: 0.144499\ttraining's amex_metric: 0.925774\tvalid_1's binary_logloss: 0.212485\tvalid_1's amex_metric: 0.796811\n",
      "[7800]\ttraining's binary_logloss: 0.143615\ttraining's amex_metric: 0.9272\tvalid_1's binary_logloss: 0.212471\tvalid_1's amex_metric: 0.797038\n",
      "[7900]\ttraining's binary_logloss: 0.142508\ttraining's amex_metric: 0.92856\tvalid_1's binary_logloss: 0.212422\tvalid_1's amex_metric: 0.796917\n",
      "[8000]\ttraining's binary_logloss: 0.14165\ttraining's amex_metric: 0.929934\tvalid_1's binary_logloss: 0.212389\tvalid_1's amex_metric: 0.797114\n",
      "[8100]\ttraining's binary_logloss: 0.140654\ttraining's amex_metric: 0.931249\tvalid_1's binary_logloss: 0.212351\tvalid_1's amex_metric: 0.797163\n",
      "[8200]\ttraining's binary_logloss: 0.139722\ttraining's amex_metric: 0.932738\tvalid_1's binary_logloss: 0.212314\tvalid_1's amex_metric: 0.797089\n",
      "[8300]\ttraining's binary_logloss: 0.138858\ttraining's amex_metric: 0.934272\tvalid_1's binary_logloss: 0.212291\tvalid_1's amex_metric: 0.797452\n",
      "[8400]\ttraining's binary_logloss: 0.138225\ttraining's amex_metric: 0.935437\tvalid_1's binary_logloss: 0.212289\tvalid_1's amex_metric: 0.797559\n",
      "[8500]\ttraining's binary_logloss: 0.137444\ttraining's amex_metric: 0.936629\tvalid_1's binary_logloss: 0.212286\tvalid_1's amex_metric: 0.796907\n",
      "[8600]\ttraining's binary_logloss: 0.136867\ttraining's amex_metric: 0.937725\tvalid_1's binary_logloss: 0.212306\tvalid_1's amex_metric: 0.7969\n",
      "[8700]\ttraining's binary_logloss: 0.136294\ttraining's amex_metric: 0.93875\tvalid_1's binary_logloss: 0.212319\tvalid_1's amex_metric: 0.797298\n",
      "[8800]\ttraining's binary_logloss: 0.13551\ttraining's amex_metric: 0.939921\tvalid_1's binary_logloss: 0.212293\tvalid_1's amex_metric: 0.797116\n",
      "[8900]\ttraining's binary_logloss: 0.134499\ttraining's amex_metric: 0.941207\tvalid_1's binary_logloss: 0.212217\tvalid_1's amex_metric: 0.797261\n",
      "[9000]\ttraining's binary_logloss: 0.133902\ttraining's amex_metric: 0.942322\tvalid_1's binary_logloss: 0.2122\tvalid_1's amex_metric: 0.797543\n",
      "[9100]\ttraining's binary_logloss: 0.133109\ttraining's amex_metric: 0.943364\tvalid_1's binary_logloss: 0.212156\tvalid_1's amex_metric: 0.797704\n",
      "[9200]\ttraining's binary_logloss: 0.132485\ttraining's amex_metric: 0.944428\tvalid_1's binary_logloss: 0.212153\tvalid_1's amex_metric: 0.797576\n",
      "[9300]\ttraining's binary_logloss: 0.131785\ttraining's amex_metric: 0.945501\tvalid_1's binary_logloss: 0.212146\tvalid_1's amex_metric: 0.797407\n",
      "[9400]\ttraining's binary_logloss: 0.13094\ttraining's amex_metric: 0.946585\tvalid_1's binary_logloss: 0.212133\tvalid_1's amex_metric: 0.797574\n",
      "[9500]\ttraining's binary_logloss: 0.130047\ttraining's amex_metric: 0.947827\tvalid_1's binary_logloss: 0.212127\tvalid_1's amex_metric: 0.797698\n",
      "[9600]\ttraining's binary_logloss: 0.129292\ttraining's amex_metric: 0.94878\tvalid_1's binary_logloss: 0.212109\tvalid_1's amex_metric: 0.797324\n",
      "[9700]\ttraining's binary_logloss: 0.128527\ttraining's amex_metric: 0.949702\tvalid_1's binary_logloss: 0.212097\tvalid_1's amex_metric: 0.79739\n",
      "[9800]\ttraining's binary_logloss: 0.127866\ttraining's amex_metric: 0.95082\tvalid_1's binary_logloss: 0.212097\tvalid_1's amex_metric: 0.797391\n",
      "[9900]\ttraining's binary_logloss: 0.126984\ttraining's amex_metric: 0.951985\tvalid_1's binary_logloss: 0.212117\tvalid_1's amex_metric: 0.797045\n",
      "[10000]\ttraining's binary_logloss: 0.126491\ttraining's amex_metric: 0.952855\tvalid_1's binary_logloss: 0.212107\tvalid_1's amex_metric: 0.797111\n",
      "[10100]\ttraining's binary_logloss: 0.125794\ttraining's amex_metric: 0.953772\tvalid_1's binary_logloss: 0.212075\tvalid_1's amex_metric: 0.79754\n",
      "[10200]\ttraining's binary_logloss: 0.12509\ttraining's amex_metric: 0.954741\tvalid_1's binary_logloss: 0.212064\tvalid_1's amex_metric: 0.798007\n",
      "[10300]\ttraining's binary_logloss: 0.124363\ttraining's amex_metric: 0.955776\tvalid_1's binary_logloss: 0.212047\tvalid_1's amex_metric: 0.798242\n",
      "[10400]\ttraining's binary_logloss: 0.123451\ttraining's amex_metric: 0.956679\tvalid_1's binary_logloss: 0.212048\tvalid_1's amex_metric: 0.798386\n",
      "Our fold 1 CV score is 0.7983861419113487\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.355797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330005\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2011\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.477802\ttraining's amex_metric: 0.767592\tvalid_1's binary_logloss: 0.479632\tvalid_1's amex_metric: 0.75421\n",
      "[200]\ttraining's binary_logloss: 0.404106\ttraining's amex_metric: 0.772747\tvalid_1's binary_logloss: 0.407142\tvalid_1's amex_metric: 0.758935\n",
      "[300]\ttraining's binary_logloss: 0.368911\ttraining's amex_metric: 0.775714\tvalid_1's binary_logloss: 0.372798\tvalid_1's amex_metric: 0.760603\n",
      "[400]\ttraining's binary_logloss: 0.325206\ttraining's amex_metric: 0.779245\tvalid_1's binary_logloss: 0.330216\tvalid_1's amex_metric: 0.763079\n",
      "[500]\ttraining's binary_logloss: 0.304801\ttraining's amex_metric: 0.783227\tvalid_1's binary_logloss: 0.310554\tvalid_1's amex_metric: 0.764724\n",
      "[600]\ttraining's binary_logloss: 0.278305\ttraining's amex_metric: 0.786121\tvalid_1's binary_logloss: 0.28525\tvalid_1's amex_metric: 0.767637\n",
      "[700]\ttraining's binary_logloss: 0.261878\ttraining's amex_metric: 0.79063\tvalid_1's binary_logloss: 0.26986\tvalid_1's amex_metric: 0.769692\n",
      "[800]\ttraining's binary_logloss: 0.249572\ttraining's amex_metric: 0.793893\tvalid_1's binary_logloss: 0.258604\tvalid_1's amex_metric: 0.772778\n",
      "[900]\ttraining's binary_logloss: 0.240284\ttraining's amex_metric: 0.79785\tvalid_1's binary_logloss: 0.250336\tvalid_1's amex_metric: 0.77408\n",
      "[1000]\ttraining's binary_logloss: 0.236296\ttraining's amex_metric: 0.800602\tvalid_1's binary_logloss: 0.247001\tvalid_1's amex_metric: 0.775579\n",
      "[1100]\ttraining's binary_logloss: 0.23293\ttraining's amex_metric: 0.803512\tvalid_1's binary_logloss: 0.244297\tvalid_1's amex_metric: 0.77597\n",
      "[1200]\ttraining's binary_logloss: 0.228563\ttraining's amex_metric: 0.805895\tvalid_1's binary_logloss: 0.240714\tvalid_1's amex_metric: 0.776936\n",
      "[1300]\ttraining's binary_logloss: 0.22341\ttraining's amex_metric: 0.808651\tvalid_1's binary_logloss: 0.236619\tvalid_1's amex_metric: 0.777911\n",
      "[1400]\ttraining's binary_logloss: 0.21966\ttraining's amex_metric: 0.811478\tvalid_1's binary_logloss: 0.233865\tvalid_1's amex_metric: 0.779084\n",
      "[1500]\ttraining's binary_logloss: 0.218109\ttraining's amex_metric: 0.813354\tvalid_1's binary_logloss: 0.232948\tvalid_1's amex_metric: 0.779521\n",
      "[1600]\ttraining's binary_logloss: 0.215709\ttraining's amex_metric: 0.815748\tvalid_1's binary_logloss: 0.231396\tvalid_1's amex_metric: 0.780556\n",
      "[1700]\ttraining's binary_logloss: 0.212767\ttraining's amex_metric: 0.817991\tvalid_1's binary_logloss: 0.229508\tvalid_1's amex_metric: 0.781199\n",
      "[1800]\ttraining's binary_logloss: 0.210513\ttraining's amex_metric: 0.820275\tvalid_1's binary_logloss: 0.228236\tvalid_1's amex_metric: 0.782409\n",
      "[1900]\ttraining's binary_logloss: 0.207874\ttraining's amex_metric: 0.822828\tvalid_1's binary_logloss: 0.226814\tvalid_1's amex_metric: 0.782924\n",
      "[2000]\ttraining's binary_logloss: 0.204849\ttraining's amex_metric: 0.825615\tvalid_1's binary_logloss: 0.225287\tvalid_1's amex_metric: 0.783407\n",
      "[2100]\ttraining's binary_logloss: 0.203465\ttraining's amex_metric: 0.828072\tvalid_1's binary_logloss: 0.224788\tvalid_1's amex_metric: 0.784222\n",
      "[2200]\ttraining's binary_logloss: 0.202491\ttraining's amex_metric: 0.830175\tvalid_1's binary_logloss: 0.224518\tvalid_1's amex_metric: 0.785394\n",
      "[2300]\ttraining's binary_logloss: 0.200987\ttraining's amex_metric: 0.832304\tvalid_1's binary_logloss: 0.223974\tvalid_1's amex_metric: 0.785662\n",
      "[2400]\ttraining's binary_logloss: 0.199522\ttraining's amex_metric: 0.834268\tvalid_1's binary_logloss: 0.223476\tvalid_1's amex_metric: 0.786295\n",
      "[2500]\ttraining's binary_logloss: 0.198044\ttraining's amex_metric: 0.836639\tvalid_1's binary_logloss: 0.222999\tvalid_1's amex_metric: 0.78629\n",
      "[2600]\ttraining's binary_logloss: 0.196066\ttraining's amex_metric: 0.838834\tvalid_1's binary_logloss: 0.222378\tvalid_1's amex_metric: 0.786455\n",
      "[2700]\ttraining's binary_logloss: 0.194545\ttraining's amex_metric: 0.841261\tvalid_1's binary_logloss: 0.221975\tvalid_1's amex_metric: 0.786605\n",
      "[2800]\ttraining's binary_logloss: 0.193143\ttraining's amex_metric: 0.843397\tvalid_1's binary_logloss: 0.221582\tvalid_1's amex_metric: 0.786594\n",
      "[2900]\ttraining's binary_logloss: 0.191643\ttraining's amex_metric: 0.845878\tvalid_1's binary_logloss: 0.221221\tvalid_1's amex_metric: 0.786987\n",
      "[3000]\ttraining's binary_logloss: 0.190291\ttraining's amex_metric: 0.848129\tvalid_1's binary_logloss: 0.2209\tvalid_1's amex_metric: 0.786908\n",
      "[3100]\ttraining's binary_logloss: 0.18924\ttraining's amex_metric: 0.850277\tvalid_1's binary_logloss: 0.22074\tvalid_1's amex_metric: 0.787536\n",
      "[3200]\ttraining's binary_logloss: 0.187918\ttraining's amex_metric: 0.852306\tvalid_1's binary_logloss: 0.220482\tvalid_1's amex_metric: 0.787682\n",
      "[3300]\ttraining's binary_logloss: 0.186618\ttraining's amex_metric: 0.854257\tvalid_1's binary_logloss: 0.220246\tvalid_1's amex_metric: 0.788202\n",
      "[3400]\ttraining's binary_logloss: 0.185078\ttraining's amex_metric: 0.856516\tvalid_1's binary_logloss: 0.21994\tvalid_1's amex_metric: 0.788099\n",
      "[3500]\ttraining's binary_logloss: 0.183812\ttraining's amex_metric: 0.858501\tvalid_1's binary_logloss: 0.219734\tvalid_1's amex_metric: 0.788115\n",
      "[3600]\ttraining's binary_logloss: 0.182896\ttraining's amex_metric: 0.860733\tvalid_1's binary_logloss: 0.219658\tvalid_1's amex_metric: 0.788223\n",
      "[3700]\ttraining's binary_logloss: 0.181847\ttraining's amex_metric: 0.862759\tvalid_1's binary_logloss: 0.219498\tvalid_1's amex_metric: 0.78801\n",
      "[3800]\ttraining's binary_logloss: 0.18115\ttraining's amex_metric: 0.864454\tvalid_1's binary_logloss: 0.219424\tvalid_1's amex_metric: 0.788833\n",
      "[3900]\ttraining's binary_logloss: 0.180065\ttraining's amex_metric: 0.866227\tvalid_1's binary_logloss: 0.219271\tvalid_1's amex_metric: 0.789266\n",
      "[4000]\ttraining's binary_logloss: 0.178812\ttraining's amex_metric: 0.867978\tvalid_1's binary_logloss: 0.219104\tvalid_1's amex_metric: 0.789435\n",
      "[4100]\ttraining's binary_logloss: 0.177423\ttraining's amex_metric: 0.870066\tvalid_1's binary_logloss: 0.218907\tvalid_1's amex_metric: 0.790164\n",
      "[4200]\ttraining's binary_logloss: 0.176018\ttraining's amex_metric: 0.871859\tvalid_1's binary_logloss: 0.218734\tvalid_1's amex_metric: 0.789922\n",
      "[4300]\ttraining's binary_logloss: 0.175285\ttraining's amex_metric: 0.873698\tvalid_1's binary_logloss: 0.218666\tvalid_1's amex_metric: 0.790134\n",
      "[4400]\ttraining's binary_logloss: 0.174\ttraining's amex_metric: 0.875526\tvalid_1's binary_logloss: 0.218493\tvalid_1's amex_metric: 0.790297\n",
      "[4500]\ttraining's binary_logloss: 0.172921\ttraining's amex_metric: 0.877456\tvalid_1's binary_logloss: 0.218404\tvalid_1's amex_metric: 0.790512\n",
      "[4600]\ttraining's binary_logloss: 0.171542\ttraining's amex_metric: 0.879348\tvalid_1's binary_logloss: 0.21826\tvalid_1's amex_metric: 0.790412\n",
      "[4700]\ttraining's binary_logloss: 0.170518\ttraining's amex_metric: 0.881223\tvalid_1's binary_logloss: 0.21815\tvalid_1's amex_metric: 0.791315\n",
      "[4800]\ttraining's binary_logloss: 0.16947\ttraining's amex_metric: 0.883447\tvalid_1's binary_logloss: 0.218071\tvalid_1's amex_metric: 0.791168\n",
      "[4900]\ttraining's binary_logloss: 0.168611\ttraining's amex_metric: 0.8853\tvalid_1's binary_logloss: 0.218031\tvalid_1's amex_metric: 0.791146\n",
      "[5000]\ttraining's binary_logloss: 0.167742\ttraining's amex_metric: 0.88715\tvalid_1's binary_logloss: 0.217977\tvalid_1's amex_metric: 0.790932\n",
      "[5100]\ttraining's binary_logloss: 0.166826\ttraining's amex_metric: 0.888783\tvalid_1's binary_logloss: 0.217901\tvalid_1's amex_metric: 0.791458\n",
      "[5200]\ttraining's binary_logloss: 0.165662\ttraining's amex_metric: 0.89046\tvalid_1's binary_logloss: 0.217795\tvalid_1's amex_metric: 0.791227\n",
      "[5300]\ttraining's binary_logloss: 0.164833\ttraining's amex_metric: 0.891917\tvalid_1's binary_logloss: 0.217758\tvalid_1's amex_metric: 0.7916\n",
      "[5400]\ttraining's binary_logloss: 0.163654\ttraining's amex_metric: 0.893532\tvalid_1's binary_logloss: 0.217649\tvalid_1's amex_metric: 0.791226\n",
      "[5500]\ttraining's binary_logloss: 0.162762\ttraining's amex_metric: 0.895053\tvalid_1's binary_logloss: 0.217598\tvalid_1's amex_metric: 0.791498\n",
      "[5600]\ttraining's binary_logloss: 0.161898\ttraining's amex_metric: 0.896519\tvalid_1's binary_logloss: 0.21757\tvalid_1's amex_metric: 0.791973\n",
      "[5700]\ttraining's binary_logloss: 0.160857\ttraining's amex_metric: 0.898188\tvalid_1's binary_logloss: 0.21751\tvalid_1's amex_metric: 0.792005\n",
      "[5800]\ttraining's binary_logloss: 0.15995\ttraining's amex_metric: 0.899662\tvalid_1's binary_logloss: 0.217472\tvalid_1's amex_metric: 0.791786\n",
      "[5900]\ttraining's binary_logloss: 0.15915\ttraining's amex_metric: 0.901163\tvalid_1's binary_logloss: 0.217439\tvalid_1's amex_metric: 0.792261\n",
      "[6000]\ttraining's binary_logloss: 0.158071\ttraining's amex_metric: 0.90274\tvalid_1's binary_logloss: 0.21737\tvalid_1's amex_metric: 0.791983\n",
      "[6100]\ttraining's binary_logloss: 0.157246\ttraining's amex_metric: 0.904388\tvalid_1's binary_logloss: 0.217345\tvalid_1's amex_metric: 0.791613\n",
      "[6200]\ttraining's binary_logloss: 0.156491\ttraining's amex_metric: 0.906151\tvalid_1's binary_logloss: 0.217281\tvalid_1's amex_metric: 0.792077\n",
      "[6300]\ttraining's binary_logloss: 0.155498\ttraining's amex_metric: 0.907396\tvalid_1's binary_logloss: 0.217207\tvalid_1's amex_metric: 0.792012\n",
      "[6400]\ttraining's binary_logloss: 0.154767\ttraining's amex_metric: 0.908914\tvalid_1's binary_logloss: 0.217177\tvalid_1's amex_metric: 0.791853\n",
      "[6500]\ttraining's binary_logloss: 0.153521\ttraining's amex_metric: 0.910766\tvalid_1's binary_logloss: 0.217081\tvalid_1's amex_metric: 0.792109\n",
      "[6600]\ttraining's binary_logloss: 0.152613\ttraining's amex_metric: 0.912446\tvalid_1's binary_logloss: 0.21705\tvalid_1's amex_metric: 0.792392\n",
      "[6700]\ttraining's binary_logloss: 0.15152\ttraining's amex_metric: 0.913984\tvalid_1's binary_logloss: 0.216999\tvalid_1's amex_metric: 0.792804\n",
      "[6800]\ttraining's binary_logloss: 0.150569\ttraining's amex_metric: 0.915757\tvalid_1's binary_logloss: 0.216963\tvalid_1's amex_metric: 0.792437\n",
      "[6900]\ttraining's binary_logloss: 0.150062\ttraining's amex_metric: 0.91693\tvalid_1's binary_logloss: 0.216966\tvalid_1's amex_metric: 0.791936\n",
      "[7000]\ttraining's binary_logloss: 0.149303\ttraining's amex_metric: 0.918155\tvalid_1's binary_logloss: 0.216938\tvalid_1's amex_metric: 0.792533\n",
      "[7100]\ttraining's binary_logloss: 0.14851\ttraining's amex_metric: 0.919395\tvalid_1's binary_logloss: 0.216908\tvalid_1's amex_metric: 0.792435\n",
      "[7200]\ttraining's binary_logloss: 0.147611\ttraining's amex_metric: 0.920681\tvalid_1's binary_logloss: 0.216856\tvalid_1's amex_metric: 0.792112\n",
      "[7300]\ttraining's binary_logloss: 0.14673\ttraining's amex_metric: 0.921955\tvalid_1's binary_logloss: 0.216858\tvalid_1's amex_metric: 0.792319\n",
      "[7400]\ttraining's binary_logloss: 0.145682\ttraining's amex_metric: 0.923432\tvalid_1's binary_logloss: 0.216821\tvalid_1's amex_metric: 0.792811\n",
      "Our fold 2 CV score is 0.7928113462119128\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.149063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329947\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.478082\ttraining's amex_metric: 0.76723\tvalid_1's binary_logloss: 0.479186\tvalid_1's amex_metric: 0.757463\n",
      "[200]\ttraining's binary_logloss: 0.404106\ttraining's amex_metric: 0.77173\tvalid_1's binary_logloss: 0.406046\tvalid_1's amex_metric: 0.760967\n",
      "[300]\ttraining's binary_logloss: 0.36906\ttraining's amex_metric: 0.77547\tvalid_1's binary_logloss: 0.37155\tvalid_1's amex_metric: 0.76342\n",
      "[400]\ttraining's binary_logloss: 0.325502\ttraining's amex_metric: 0.779132\tvalid_1's binary_logloss: 0.328903\tvalid_1's amex_metric: 0.765877\n",
      "[500]\ttraining's binary_logloss: 0.305215\ttraining's amex_metric: 0.782266\tvalid_1's binary_logloss: 0.309193\tvalid_1's amex_metric: 0.767745\n",
      "[600]\ttraining's binary_logloss: 0.27877\ttraining's amex_metric: 0.785874\tvalid_1's binary_logloss: 0.28365\tvalid_1's amex_metric: 0.769568\n",
      "[700]\ttraining's binary_logloss: 0.262324\ttraining's amex_metric: 0.789866\tvalid_1's binary_logloss: 0.26811\tvalid_1's amex_metric: 0.772831\n",
      "[800]\ttraining's binary_logloss: 0.250118\ttraining's amex_metric: 0.792828\tvalid_1's binary_logloss: 0.256785\tvalid_1's amex_metric: 0.775792\n",
      "[900]\ttraining's binary_logloss: 0.240853\ttraining's amex_metric: 0.796225\tvalid_1's binary_logloss: 0.248421\tvalid_1's amex_metric: 0.777663\n",
      "[1000]\ttraining's binary_logloss: 0.236863\ttraining's amex_metric: 0.799387\tvalid_1's binary_logloss: 0.245058\tvalid_1's amex_metric: 0.77913\n",
      "[1100]\ttraining's binary_logloss: 0.233506\ttraining's amex_metric: 0.801947\tvalid_1's binary_logloss: 0.242325\tvalid_1's amex_metric: 0.78046\n",
      "[1200]\ttraining's binary_logloss: 0.229116\ttraining's amex_metric: 0.804484\tvalid_1's binary_logloss: 0.238749\tvalid_1's amex_metric: 0.781933\n",
      "[1300]\ttraining's binary_logloss: 0.224009\ttraining's amex_metric: 0.807223\tvalid_1's binary_logloss: 0.234654\tvalid_1's amex_metric: 0.783534\n",
      "[1400]\ttraining's binary_logloss: 0.220221\ttraining's amex_metric: 0.810474\tvalid_1's binary_logloss: 0.231832\tvalid_1's amex_metric: 0.785123\n",
      "[1500]\ttraining's binary_logloss: 0.218671\ttraining's amex_metric: 0.812217\tvalid_1's binary_logloss: 0.230946\tvalid_1's amex_metric: 0.78558\n",
      "[1600]\ttraining's binary_logloss: 0.216307\ttraining's amex_metric: 0.814263\tvalid_1's binary_logloss: 0.229364\tvalid_1's amex_metric: 0.786221\n",
      "[1700]\ttraining's binary_logloss: 0.2134\ttraining's amex_metric: 0.816991\tvalid_1's binary_logloss: 0.227475\tvalid_1's amex_metric: 0.787837\n",
      "[1800]\ttraining's binary_logloss: 0.211121\ttraining's amex_metric: 0.819171\tvalid_1's binary_logloss: 0.226158\tvalid_1's amex_metric: 0.788449\n",
      "[1900]\ttraining's binary_logloss: 0.208466\ttraining's amex_metric: 0.821708\tvalid_1's binary_logloss: 0.224633\tvalid_1's amex_metric: 0.789423\n",
      "[2000]\ttraining's binary_logloss: 0.205406\ttraining's amex_metric: 0.824615\tvalid_1's binary_logloss: 0.223042\tvalid_1's amex_metric: 0.789668\n",
      "[2100]\ttraining's binary_logloss: 0.204017\ttraining's amex_metric: 0.826919\tvalid_1's binary_logloss: 0.222495\tvalid_1's amex_metric: 0.790531\n",
      "[2200]\ttraining's binary_logloss: 0.203033\ttraining's amex_metric: 0.82949\tvalid_1's binary_logloss: 0.222274\tvalid_1's amex_metric: 0.791036\n",
      "[2300]\ttraining's binary_logloss: 0.201517\ttraining's amex_metric: 0.831548\tvalid_1's binary_logloss: 0.221701\tvalid_1's amex_metric: 0.791363\n",
      "[2400]\ttraining's binary_logloss: 0.200034\ttraining's amex_metric: 0.833941\tvalid_1's binary_logloss: 0.221153\tvalid_1's amex_metric: 0.791555\n",
      "[2500]\ttraining's binary_logloss: 0.198585\ttraining's amex_metric: 0.836056\tvalid_1's binary_logloss: 0.22065\tvalid_1's amex_metric: 0.791965\n",
      "[2600]\ttraining's binary_logloss: 0.196588\ttraining's amex_metric: 0.838402\tvalid_1's binary_logloss: 0.219909\tvalid_1's amex_metric: 0.792688\n",
      "[2700]\ttraining's binary_logloss: 0.195048\ttraining's amex_metric: 0.840751\tvalid_1's binary_logloss: 0.219466\tvalid_1's amex_metric: 0.79327\n",
      "[2800]\ttraining's binary_logloss: 0.193659\ttraining's amex_metric: 0.842963\tvalid_1's binary_logloss: 0.219087\tvalid_1's amex_metric: 0.793448\n",
      "[2900]\ttraining's binary_logloss: 0.192167\ttraining's amex_metric: 0.84537\tvalid_1's binary_logloss: 0.218739\tvalid_1's amex_metric: 0.793305\n",
      "[3000]\ttraining's binary_logloss: 0.190805\ttraining's amex_metric: 0.847979\tvalid_1's binary_logloss: 0.218441\tvalid_1's amex_metric: 0.793948\n",
      "[3100]\ttraining's binary_logloss: 0.189748\ttraining's amex_metric: 0.84968\tvalid_1's binary_logloss: 0.218288\tvalid_1's amex_metric: 0.794285\n",
      "[3200]\ttraining's binary_logloss: 0.188433\ttraining's amex_metric: 0.851915\tvalid_1's binary_logloss: 0.21802\tvalid_1's amex_metric: 0.794758\n",
      "[3300]\ttraining's binary_logloss: 0.187129\ttraining's amex_metric: 0.85377\tvalid_1's binary_logloss: 0.217778\tvalid_1's amex_metric: 0.795195\n",
      "[3400]\ttraining's binary_logloss: 0.185618\ttraining's amex_metric: 0.855802\tvalid_1's binary_logloss: 0.217492\tvalid_1's amex_metric: 0.79468\n",
      "[3500]\ttraining's binary_logloss: 0.184333\ttraining's amex_metric: 0.858028\tvalid_1's binary_logloss: 0.217283\tvalid_1's amex_metric: 0.795264\n",
      "[3600]\ttraining's binary_logloss: 0.18342\ttraining's amex_metric: 0.859919\tvalid_1's binary_logloss: 0.21722\tvalid_1's amex_metric: 0.795194\n",
      "[3700]\ttraining's binary_logloss: 0.182388\ttraining's amex_metric: 0.861695\tvalid_1's binary_logloss: 0.217074\tvalid_1's amex_metric: 0.795769\n",
      "[3800]\ttraining's binary_logloss: 0.181689\ttraining's amex_metric: 0.863472\tvalid_1's binary_logloss: 0.217086\tvalid_1's amex_metric: 0.796099\n",
      "[3900]\ttraining's binary_logloss: 0.180595\ttraining's amex_metric: 0.865576\tvalid_1's binary_logloss: 0.216935\tvalid_1's amex_metric: 0.796575\n",
      "[4000]\ttraining's binary_logloss: 0.179338\ttraining's amex_metric: 0.867178\tvalid_1's binary_logloss: 0.216735\tvalid_1's amex_metric: 0.796558\n",
      "[4100]\ttraining's binary_logloss: 0.177939\ttraining's amex_metric: 0.869613\tvalid_1's binary_logloss: 0.216525\tvalid_1's amex_metric: 0.796781\n",
      "[4200]\ttraining's binary_logloss: 0.176543\ttraining's amex_metric: 0.871507\tvalid_1's binary_logloss: 0.216303\tvalid_1's amex_metric: 0.797112\n",
      "[4300]\ttraining's binary_logloss: 0.175785\ttraining's amex_metric: 0.873266\tvalid_1's binary_logloss: 0.216265\tvalid_1's amex_metric: 0.79711\n",
      "[4400]\ttraining's binary_logloss: 0.174486\ttraining's amex_metric: 0.875143\tvalid_1's binary_logloss: 0.216097\tvalid_1's amex_metric: 0.79706\n",
      "[4500]\ttraining's binary_logloss: 0.173373\ttraining's amex_metric: 0.876722\tvalid_1's binary_logloss: 0.215977\tvalid_1's amex_metric: 0.797401\n",
      "[4600]\ttraining's binary_logloss: 0.171987\ttraining's amex_metric: 0.878788\tvalid_1's binary_logloss: 0.215849\tvalid_1's amex_metric: 0.796993\n",
      "[4700]\ttraining's binary_logloss: 0.170969\ttraining's amex_metric: 0.880708\tvalid_1's binary_logloss: 0.215771\tvalid_1's amex_metric: 0.797361\n",
      "[4800]\ttraining's binary_logloss: 0.169926\ttraining's amex_metric: 0.882502\tvalid_1's binary_logloss: 0.215676\tvalid_1's amex_metric: 0.797198\n",
      "[4900]\ttraining's binary_logloss: 0.169078\ttraining's amex_metric: 0.884404\tvalid_1's binary_logloss: 0.215617\tvalid_1's amex_metric: 0.797792\n",
      "[5000]\ttraining's binary_logloss: 0.168206\ttraining's amex_metric: 0.885888\tvalid_1's binary_logloss: 0.215599\tvalid_1's amex_metric: 0.797654\n",
      "[5100]\ttraining's binary_logloss: 0.167298\ttraining's amex_metric: 0.887638\tvalid_1's binary_logloss: 0.215537\tvalid_1's amex_metric: 0.797685\n",
      "[5200]\ttraining's binary_logloss: 0.166136\ttraining's amex_metric: 0.889454\tvalid_1's binary_logloss: 0.215421\tvalid_1's amex_metric: 0.797603\n",
      "[5300]\ttraining's binary_logloss: 0.165281\ttraining's amex_metric: 0.891054\tvalid_1's binary_logloss: 0.215377\tvalid_1's amex_metric: 0.797869\n",
      "[5400]\ttraining's binary_logloss: 0.164099\ttraining's amex_metric: 0.892972\tvalid_1's binary_logloss: 0.215256\tvalid_1's amex_metric: 0.797852\n",
      "[5500]\ttraining's binary_logloss: 0.163202\ttraining's amex_metric: 0.894431\tvalid_1's binary_logloss: 0.215209\tvalid_1's amex_metric: 0.797782\n",
      "[5600]\ttraining's binary_logloss: 0.162333\ttraining's amex_metric: 0.89597\tvalid_1's binary_logloss: 0.215145\tvalid_1's amex_metric: 0.797614\n",
      "[5700]\ttraining's binary_logloss: 0.161282\ttraining's amex_metric: 0.897364\tvalid_1's binary_logloss: 0.215101\tvalid_1's amex_metric: 0.797254\n",
      "[5800]\ttraining's binary_logloss: 0.160363\ttraining's amex_metric: 0.899126\tvalid_1's binary_logloss: 0.215045\tvalid_1's amex_metric: 0.797753\n",
      "[5900]\ttraining's binary_logloss: 0.159563\ttraining's amex_metric: 0.900531\tvalid_1's binary_logloss: 0.215028\tvalid_1's amex_metric: 0.797754\n",
      "[6000]\ttraining's binary_logloss: 0.158489\ttraining's amex_metric: 0.902277\tvalid_1's binary_logloss: 0.214923\tvalid_1's amex_metric: 0.79803\n",
      "[6100]\ttraining's binary_logloss: 0.157667\ttraining's amex_metric: 0.903587\tvalid_1's binary_logloss: 0.214874\tvalid_1's amex_metric: 0.797628\n",
      "[6200]\ttraining's binary_logloss: 0.156905\ttraining's amex_metric: 0.904866\tvalid_1's binary_logloss: 0.214858\tvalid_1's amex_metric: 0.797677\n",
      "[6300]\ttraining's binary_logloss: 0.155917\ttraining's amex_metric: 0.90634\tvalid_1's binary_logloss: 0.214825\tvalid_1's amex_metric: 0.797991\n",
      "[6400]\ttraining's binary_logloss: 0.155177\ttraining's amex_metric: 0.907881\tvalid_1's binary_logloss: 0.214817\tvalid_1's amex_metric: 0.79806\n",
      "[6500]\ttraining's binary_logloss: 0.153933\ttraining's amex_metric: 0.909733\tvalid_1's binary_logloss: 0.214733\tvalid_1's amex_metric: 0.797883\n",
      "[6600]\ttraining's binary_logloss: 0.153012\ttraining's amex_metric: 0.911358\tvalid_1's binary_logloss: 0.214701\tvalid_1's amex_metric: 0.798544\n",
      "[6700]\ttraining's binary_logloss: 0.151908\ttraining's amex_metric: 0.912779\tvalid_1's binary_logloss: 0.21463\tvalid_1's amex_metric: 0.798431\n",
      "[6800]\ttraining's binary_logloss: 0.15095\ttraining's amex_metric: 0.914509\tvalid_1's binary_logloss: 0.214589\tvalid_1's amex_metric: 0.798317\n",
      "[6900]\ttraining's binary_logloss: 0.150443\ttraining's amex_metric: 0.915767\tvalid_1's binary_logloss: 0.214617\tvalid_1's amex_metric: 0.798169\n",
      "[7000]\ttraining's binary_logloss: 0.149688\ttraining's amex_metric: 0.917016\tvalid_1's binary_logloss: 0.214585\tvalid_1's amex_metric: 0.799021\n",
      "[7100]\ttraining's binary_logloss: 0.148892\ttraining's amex_metric: 0.918369\tvalid_1's binary_logloss: 0.214567\tvalid_1's amex_metric: 0.798981\n",
      "[7200]\ttraining's binary_logloss: 0.147985\ttraining's amex_metric: 0.919809\tvalid_1's binary_logloss: 0.214539\tvalid_1's amex_metric: 0.798774\n",
      "[7300]\ttraining's binary_logloss: 0.147093\ttraining's amex_metric: 0.921135\tvalid_1's binary_logloss: 0.214513\tvalid_1's amex_metric: 0.799346\n",
      "[7400]\ttraining's binary_logloss: 0.146057\ttraining's amex_metric: 0.922603\tvalid_1's binary_logloss: 0.214477\tvalid_1's amex_metric: 0.79889\n",
      "[7500]\ttraining's binary_logloss: 0.145294\ttraining's amex_metric: 0.924073\tvalid_1's binary_logloss: 0.214469\tvalid_1's amex_metric: 0.79931\n",
      "Our fold 3 CV score is 0.7993104058255349\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.201262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329908\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.478144\ttraining's amex_metric: 0.766775\tvalid_1's binary_logloss: 0.478865\tvalid_1's amex_metric: 0.758486\n",
      "[200]\ttraining's binary_logloss: 0.404153\ttraining's amex_metric: 0.771566\tvalid_1's binary_logloss: 0.40554\tvalid_1's amex_metric: 0.762783\n",
      "[300]\ttraining's binary_logloss: 0.369016\ttraining's amex_metric: 0.77491\tvalid_1's binary_logloss: 0.37097\tvalid_1's amex_metric: 0.764477\n",
      "[400]\ttraining's binary_logloss: 0.325444\ttraining's amex_metric: 0.779117\tvalid_1's binary_logloss: 0.328319\tvalid_1's amex_metric: 0.767721\n",
      "[500]\ttraining's binary_logloss: 0.305152\ttraining's amex_metric: 0.781741\tvalid_1's binary_logloss: 0.308659\tvalid_1's amex_metric: 0.769323\n",
      "[600]\ttraining's binary_logloss: 0.278661\ttraining's amex_metric: 0.785807\tvalid_1's binary_logloss: 0.283193\tvalid_1's amex_metric: 0.771013\n",
      "[700]\ttraining's binary_logloss: 0.262245\ttraining's amex_metric: 0.789651\tvalid_1's binary_logloss: 0.267727\tvalid_1's amex_metric: 0.774325\n",
      "[800]\ttraining's binary_logloss: 0.250055\ttraining's amex_metric: 0.792988\tvalid_1's binary_logloss: 0.256557\tvalid_1's amex_metric: 0.775872\n",
      "[900]\ttraining's binary_logloss: 0.240812\ttraining's amex_metric: 0.796915\tvalid_1's binary_logloss: 0.248328\tvalid_1's amex_metric: 0.7779\n",
      "[1000]\ttraining's binary_logloss: 0.236793\ttraining's amex_metric: 0.799639\tvalid_1's binary_logloss: 0.245001\tvalid_1's amex_metric: 0.779064\n",
      "[1100]\ttraining's binary_logloss: 0.233458\ttraining's amex_metric: 0.802496\tvalid_1's binary_logloss: 0.242318\tvalid_1's amex_metric: 0.779331\n",
      "[1200]\ttraining's binary_logloss: 0.229084\ttraining's amex_metric: 0.804647\tvalid_1's binary_logloss: 0.238738\tvalid_1's amex_metric: 0.780965\n",
      "[1300]\ttraining's binary_logloss: 0.223947\ttraining's amex_metric: 0.807616\tvalid_1's binary_logloss: 0.234671\tvalid_1's amex_metric: 0.782797\n",
      "[1400]\ttraining's binary_logloss: 0.220172\ttraining's amex_metric: 0.81038\tvalid_1's binary_logloss: 0.23193\tvalid_1's amex_metric: 0.784021\n",
      "[1500]\ttraining's binary_logloss: 0.218623\ttraining's amex_metric: 0.812488\tvalid_1's binary_logloss: 0.231004\tvalid_1's amex_metric: 0.785568\n",
      "[1600]\ttraining's binary_logloss: 0.216273\ttraining's amex_metric: 0.814396\tvalid_1's binary_logloss: 0.229511\tvalid_1's amex_metric: 0.785925\n",
      "[1700]\ttraining's binary_logloss: 0.21338\ttraining's amex_metric: 0.81738\tvalid_1's binary_logloss: 0.227676\tvalid_1's amex_metric: 0.787242\n",
      "[1800]\ttraining's binary_logloss: 0.211116\ttraining's amex_metric: 0.819595\tvalid_1's binary_logloss: 0.226417\tvalid_1's amex_metric: 0.788095\n",
      "[1900]\ttraining's binary_logloss: 0.208483\ttraining's amex_metric: 0.822168\tvalid_1's binary_logloss: 0.224933\tvalid_1's amex_metric: 0.789171\n",
      "[2000]\ttraining's binary_logloss: 0.205421\ttraining's amex_metric: 0.825094\tvalid_1's binary_logloss: 0.223351\tvalid_1's amex_metric: 0.789537\n",
      "[2100]\ttraining's binary_logloss: 0.204028\ttraining's amex_metric: 0.827663\tvalid_1's binary_logloss: 0.222806\tvalid_1's amex_metric: 0.789973\n",
      "[2200]\ttraining's binary_logloss: 0.203045\ttraining's amex_metric: 0.829922\tvalid_1's binary_logloss: 0.222548\tvalid_1's amex_metric: 0.789942\n",
      "[2300]\ttraining's binary_logloss: 0.201521\ttraining's amex_metric: 0.831842\tvalid_1's binary_logloss: 0.221983\tvalid_1's amex_metric: 0.790826\n",
      "[2400]\ttraining's binary_logloss: 0.200058\ttraining's amex_metric: 0.833926\tvalid_1's binary_logloss: 0.221478\tvalid_1's amex_metric: 0.791776\n",
      "[2500]\ttraining's binary_logloss: 0.19861\ttraining's amex_metric: 0.836187\tvalid_1's binary_logloss: 0.22102\tvalid_1's amex_metric: 0.79228\n",
      "[2600]\ttraining's binary_logloss: 0.196623\ttraining's amex_metric: 0.838729\tvalid_1's binary_logloss: 0.220311\tvalid_1's amex_metric: 0.793286\n",
      "[2700]\ttraining's binary_logloss: 0.195065\ttraining's amex_metric: 0.840843\tvalid_1's binary_logloss: 0.219854\tvalid_1's amex_metric: 0.793728\n",
      "[2800]\ttraining's binary_logloss: 0.193669\ttraining's amex_metric: 0.842745\tvalid_1's binary_logloss: 0.219502\tvalid_1's amex_metric: 0.793897\n",
      "[2900]\ttraining's binary_logloss: 0.192164\ttraining's amex_metric: 0.845275\tvalid_1's binary_logloss: 0.219086\tvalid_1's amex_metric: 0.795052\n",
      "[3000]\ttraining's binary_logloss: 0.190807\ttraining's amex_metric: 0.847635\tvalid_1's binary_logloss: 0.218758\tvalid_1's amex_metric: 0.795415\n",
      "[3100]\ttraining's binary_logloss: 0.189753\ttraining's amex_metric: 0.849448\tvalid_1's binary_logloss: 0.218578\tvalid_1's amex_metric: 0.795821\n",
      "[3200]\ttraining's binary_logloss: 0.188423\ttraining's amex_metric: 0.85164\tvalid_1's binary_logloss: 0.218296\tvalid_1's amex_metric: 0.795992\n",
      "[3300]\ttraining's binary_logloss: 0.187133\ttraining's amex_metric: 0.853761\tvalid_1's binary_logloss: 0.218049\tvalid_1's amex_metric: 0.796067\n",
      "[3400]\ttraining's binary_logloss: 0.18561\ttraining's amex_metric: 0.856163\tvalid_1's binary_logloss: 0.217718\tvalid_1's amex_metric: 0.796251\n",
      "[3500]\ttraining's binary_logloss: 0.184343\ttraining's amex_metric: 0.858541\tvalid_1's binary_logloss: 0.217487\tvalid_1's amex_metric: 0.796278\n",
      "[3600]\ttraining's binary_logloss: 0.183433\ttraining's amex_metric: 0.860293\tvalid_1's binary_logloss: 0.21736\tvalid_1's amex_metric: 0.7965\n",
      "[3700]\ttraining's binary_logloss: 0.18238\ttraining's amex_metric: 0.862254\tvalid_1's binary_logloss: 0.21722\tvalid_1's amex_metric: 0.796272\n",
      "[3800]\ttraining's binary_logloss: 0.181684\ttraining's amex_metric: 0.86396\tvalid_1's binary_logloss: 0.217161\tvalid_1's amex_metric: 0.796332\n",
      "[3900]\ttraining's binary_logloss: 0.180586\ttraining's amex_metric: 0.865899\tvalid_1's binary_logloss: 0.217025\tvalid_1's amex_metric: 0.796254\n",
      "[4000]\ttraining's binary_logloss: 0.179322\ttraining's amex_metric: 0.867875\tvalid_1's binary_logloss: 0.216838\tvalid_1's amex_metric: 0.796096\n",
      "[4100]\ttraining's binary_logloss: 0.177922\ttraining's amex_metric: 0.869843\tvalid_1's binary_logloss: 0.216627\tvalid_1's amex_metric: 0.796385\n",
      "[4200]\ttraining's binary_logloss: 0.176525\ttraining's amex_metric: 0.87165\tvalid_1's binary_logloss: 0.216405\tvalid_1's amex_metric: 0.796769\n",
      "[4300]\ttraining's binary_logloss: 0.175775\ttraining's amex_metric: 0.873059\tvalid_1's binary_logloss: 0.21637\tvalid_1's amex_metric: 0.796925\n",
      "[4400]\ttraining's binary_logloss: 0.174466\ttraining's amex_metric: 0.875273\tvalid_1's binary_logloss: 0.216203\tvalid_1's amex_metric: 0.797381\n",
      "[4500]\ttraining's binary_logloss: 0.173365\ttraining's amex_metric: 0.877273\tvalid_1's binary_logloss: 0.21609\tvalid_1's amex_metric: 0.797409\n",
      "[4600]\ttraining's binary_logloss: 0.171974\ttraining's amex_metric: 0.879001\tvalid_1's binary_logloss: 0.215887\tvalid_1's amex_metric: 0.797745\n",
      "[4700]\ttraining's binary_logloss: 0.17095\ttraining's amex_metric: 0.881176\tvalid_1's binary_logloss: 0.21582\tvalid_1's amex_metric: 0.79724\n",
      "[4800]\ttraining's binary_logloss: 0.169918\ttraining's amex_metric: 0.882639\tvalid_1's binary_logloss: 0.215736\tvalid_1's amex_metric: 0.797476\n",
      "[4900]\ttraining's binary_logloss: 0.169064\ttraining's amex_metric: 0.884162\tvalid_1's binary_logloss: 0.215671\tvalid_1's amex_metric: 0.797799\n",
      "[5000]\ttraining's binary_logloss: 0.168197\ttraining's amex_metric: 0.886051\tvalid_1's binary_logloss: 0.215637\tvalid_1's amex_metric: 0.798149\n",
      "[5100]\ttraining's binary_logloss: 0.16729\ttraining's amex_metric: 0.887709\tvalid_1's binary_logloss: 0.215608\tvalid_1's amex_metric: 0.797711\n",
      "[5200]\ttraining's binary_logloss: 0.166139\ttraining's amex_metric: 0.889481\tvalid_1's binary_logloss: 0.215552\tvalid_1's amex_metric: 0.797504\n",
      "[5300]\ttraining's binary_logloss: 0.165314\ttraining's amex_metric: 0.890989\tvalid_1's binary_logloss: 0.215489\tvalid_1's amex_metric: 0.797482\n",
      "[5400]\ttraining's binary_logloss: 0.164133\ttraining's amex_metric: 0.892656\tvalid_1's binary_logloss: 0.215403\tvalid_1's amex_metric: 0.797599\n",
      "[5500]\ttraining's binary_logloss: 0.163236\ttraining's amex_metric: 0.894371\tvalid_1's binary_logloss: 0.215368\tvalid_1's amex_metric: 0.797166\n",
      "[5600]\ttraining's binary_logloss: 0.162367\ttraining's amex_metric: 0.895773\tvalid_1's binary_logloss: 0.21531\tvalid_1's amex_metric: 0.79727\n",
      "[5700]\ttraining's binary_logloss: 0.161321\ttraining's amex_metric: 0.897395\tvalid_1's binary_logloss: 0.215245\tvalid_1's amex_metric: 0.797198\n",
      "[5800]\ttraining's binary_logloss: 0.160417\ttraining's amex_metric: 0.898987\tvalid_1's binary_logloss: 0.215202\tvalid_1's amex_metric: 0.797022\n",
      "[5900]\ttraining's binary_logloss: 0.159616\ttraining's amex_metric: 0.900445\tvalid_1's binary_logloss: 0.21514\tvalid_1's amex_metric: 0.796706\n",
      "[6000]\ttraining's binary_logloss: 0.158554\ttraining's amex_metric: 0.902038\tvalid_1's binary_logloss: 0.215066\tvalid_1's amex_metric: 0.796555\n",
      "[6100]\ttraining's binary_logloss: 0.15773\ttraining's amex_metric: 0.903673\tvalid_1's binary_logloss: 0.21502\tvalid_1's amex_metric: 0.796887\n",
      "[6200]\ttraining's binary_logloss: 0.156969\ttraining's amex_metric: 0.90541\tvalid_1's binary_logloss: 0.214989\tvalid_1's amex_metric: 0.797342\n",
      "[6300]\ttraining's binary_logloss: 0.155978\ttraining's amex_metric: 0.90686\tvalid_1's binary_logloss: 0.214931\tvalid_1's amex_metric: 0.797079\n",
      "[6400]\ttraining's binary_logloss: 0.155244\ttraining's amex_metric: 0.908294\tvalid_1's binary_logloss: 0.214928\tvalid_1's amex_metric: 0.796911\n",
      "[6500]\ttraining's binary_logloss: 0.153983\ttraining's amex_metric: 0.909748\tvalid_1's binary_logloss: 0.214869\tvalid_1's amex_metric: 0.79677\n",
      "[6600]\ttraining's binary_logloss: 0.153068\ttraining's amex_metric: 0.911471\tvalid_1's binary_logloss: 0.214831\tvalid_1's amex_metric: 0.797203\n",
      "[6700]\ttraining's binary_logloss: 0.151965\ttraining's amex_metric: 0.912943\tvalid_1's binary_logloss: 0.214802\tvalid_1's amex_metric: 0.79672\n",
      "[6800]\ttraining's binary_logloss: 0.151001\ttraining's amex_metric: 0.914786\tvalid_1's binary_logloss: 0.214762\tvalid_1's amex_metric: 0.797236\n",
      "[6900]\ttraining's binary_logloss: 0.150488\ttraining's amex_metric: 0.91619\tvalid_1's binary_logloss: 0.214765\tvalid_1's amex_metric: 0.797451\n",
      "[7000]\ttraining's binary_logloss: 0.149737\ttraining's amex_metric: 0.917555\tvalid_1's binary_logloss: 0.214767\tvalid_1's amex_metric: 0.796986\n",
      "[7100]\ttraining's binary_logloss: 0.148938\ttraining's amex_metric: 0.919004\tvalid_1's binary_logloss: 0.214721\tvalid_1's amex_metric: 0.797252\n",
      "[7200]\ttraining's binary_logloss: 0.148038\ttraining's amex_metric: 0.920194\tvalid_1's binary_logloss: 0.214675\tvalid_1's amex_metric: 0.797242\n",
      "[7300]\ttraining's binary_logloss: 0.147169\ttraining's amex_metric: 0.921514\tvalid_1's binary_logloss: 0.214659\tvalid_1's amex_metric: 0.797538\n",
      "[7400]\ttraining's binary_logloss: 0.146118\ttraining's amex_metric: 0.923075\tvalid_1's binary_logloss: 0.214631\tvalid_1's amex_metric: 0.797289\n",
      "[7500]\ttraining's binary_logloss: 0.145359\ttraining's amex_metric: 0.924514\tvalid_1's binary_logloss: 0.214594\tvalid_1's amex_metric: 0.79726\n",
      "[7600]\ttraining's binary_logloss: 0.144717\ttraining's amex_metric: 0.926008\tvalid_1's binary_logloss: 0.214567\tvalid_1's amex_metric: 0.797334\n",
      "[7700]\ttraining's binary_logloss: 0.144013\ttraining's amex_metric: 0.927254\tvalid_1's binary_logloss: 0.214587\tvalid_1's amex_metric: 0.797115\n",
      "[7800]\ttraining's binary_logloss: 0.143142\ttraining's amex_metric: 0.928331\tvalid_1's binary_logloss: 0.214564\tvalid_1's amex_metric: 0.797497\n",
      "[7900]\ttraining's binary_logloss: 0.14203\ttraining's amex_metric: 0.929672\tvalid_1's binary_logloss: 0.214532\tvalid_1's amex_metric: 0.797332\n",
      "[8000]\ttraining's binary_logloss: 0.141164\ttraining's amex_metric: 0.931124\tvalid_1's binary_logloss: 0.214505\tvalid_1's amex_metric: 0.797805\n",
      "[8100]\ttraining's binary_logloss: 0.140157\ttraining's amex_metric: 0.932334\tvalid_1's binary_logloss: 0.214488\tvalid_1's amex_metric: 0.797515\n",
      "[8200]\ttraining's binary_logloss: 0.139237\ttraining's amex_metric: 0.933721\tvalid_1's binary_logloss: 0.214464\tvalid_1's amex_metric: 0.797334\n",
      "[8300]\ttraining's binary_logloss: 0.138367\ttraining's amex_metric: 0.9349\tvalid_1's binary_logloss: 0.214435\tvalid_1's amex_metric: 0.797282\n",
      "[8400]\ttraining's binary_logloss: 0.13774\ttraining's amex_metric: 0.93615\tvalid_1's binary_logloss: 0.214429\tvalid_1's amex_metric: 0.798\n",
      "[8500]\ttraining's binary_logloss: 0.136965\ttraining's amex_metric: 0.937483\tvalid_1's binary_logloss: 0.21441\tvalid_1's amex_metric: 0.7979\n",
      "[8600]\ttraining's binary_logloss: 0.136385\ttraining's amex_metric: 0.938442\tvalid_1's binary_logloss: 0.214405\tvalid_1's amex_metric: 0.798156\n",
      "[8700]\ttraining's binary_logloss: 0.135797\ttraining's amex_metric: 0.939471\tvalid_1's binary_logloss: 0.214382\tvalid_1's amex_metric: 0.797364\n",
      "[8800]\ttraining's binary_logloss: 0.135018\ttraining's amex_metric: 0.94058\tvalid_1's binary_logloss: 0.21439\tvalid_1's amex_metric: 0.797992\n",
      "[8900]\ttraining's binary_logloss: 0.134022\ttraining's amex_metric: 0.941964\tvalid_1's binary_logloss: 0.214385\tvalid_1's amex_metric: 0.798329\n",
      "Our fold 4 CV score is 0.7983291865668821\n",
      "Our out of folds CV score is 0.7965816551786491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = []\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "epoch = [9900,10400,7400,7500,8900]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = epoch[fold],#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1000,\n",
    "#         eval_metric=[lgb_amex_metric],\n",
    "        verbose_eval = 100,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    \n",
    "    # Add to out of folds array\n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(tr_target, oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f08ac4-d86f-4dcd-b281-4788f52bcce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.7965816551786491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute out of folds metric\n",
    "score = amex_metric(tr_target, oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeefba0-9b46-4227-b634-0a45793eb9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
