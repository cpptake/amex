{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diff„ÅÆXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29760\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29760\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp40_lgb_lagdiff_c3/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"xgb\"\n",
    "    ver = \"exp40\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 2013)\n",
      "(924621, 2012)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "train = pd.read_parquet('../feature/exp38_lagdiff_c3/train_lagdiff_c3.parquet')\n",
    "test = pd.read_parquet('../feature/exp38_lagdiff_c3/test_lagdiff_c3.parquet')\n",
    "\n",
    "# train, test = read_data()\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "909b5439-4527-4bb5-be49-2769343a79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../output/exp40_lgb_lagdiff_c3/lgbm_dart_fold0_seed42.pkl', 'wb') as f:\n",
    "#     pickle.dump(foo, f)    \n",
    "\n",
    "\n",
    "\n",
    "aa = joblib.load(filename)\n",
    "# aa = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5f272d-2129-4b46-a8de-80d78b84a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv(\"../output/exp41_cat_lagdiff_c3/oof_exp41_cat_0.7948423257935038_5fold_seed42.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a7377b-81dd-43d5-b1b5-cb91b2e3d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp40_lgb_df = pd.read_csv(\"../output/exp40_lgb_lagdiff_c3/oof_exp40_lgb_0.7969750332783205_5fold_seed42.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d26e58e0-97a6-4301-9f12-3440c9b7d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp43_xgb_df = pd.read_csv(\"../output/exp43_xgb_lagdiff_c3_hypar/oof_exp43_xgb_0.7956869117243499_5fold_seed42.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0163debb-f94b-40ba-bbfc-12ed2547f2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>tabnet_oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001812036f1558332e5c0880ecbad70b13a6f28ab04a8...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00031e8be98bc3411f6037cbd4d3eeaf24b3ae221682b7...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004b8596c4946866d1b27a8b40488ecf49c6eae9c7bf4...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  target  tabnet_oot\n",
       "0  0001812036f1558332e5c0880ecbad70b13a6f28ab04a8...       1    0.000385\n",
       "1  00031e8be98bc3411f6037cbd4d3eeaf24b3ae221682b7...       0    0.000808\n",
       "2  0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...       1    0.001188\n",
       "3  0004b8596c4946866d1b27a8b40488ecf49c6eae9c7bf4...       0    0.006864\n",
       "4  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0    0.001674"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp43_xgb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5dca2f4-9549-49e1-8dc7-9e5a4c3f23aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>tabnet_oot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001812036f1558332e5c0880ecbad70b13a6f28ab04a8...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00031e8be98bc3411f6037cbd4d3eeaf24b3ae221682b7...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004b8596c4946866d1b27a8b40488ecf49c6eae9c7bf4...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  target  tabnet_oot\n",
       "0  0001812036f1558332e5c0880ecbad70b13a6f28ab04a8...       1    0.000565\n",
       "1  00031e8be98bc3411f6037cbd4d3eeaf24b3ae221682b7...       0    0.000954\n",
       "2  0004860c260168fcaad0734a1dfedb7ceb1a83aaac54e2...       1    0.000964\n",
       "3  0004b8596c4946866d1b27a8b40488ecf49c6eae9c7bf4...       0    0.004840\n",
       "4  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0    0.002225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp40_lgb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58c3f47-7d00-4a1e-b1f0-c2211cf3bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp40_lgb_df[\"cids\"] = cat_df[\"customer_ID\"]\n",
    "exp40_lgb_df[\"cat_pred\"] = cat_df[\"exp41_cat_oof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf18e81-b47d-48f6-859a-ca675efed17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a610d-ece3-484e-bea0-0444d9ed8a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de934a-2bdd-4323-b1d6-10d9d5ecaa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.050850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327785\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1998\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.337432\ttraining's amex_metric: 0.776902\tvalid_1's binary_logloss: 0.339288\tvalid_1's amex_metric: 0.769485\n",
      "[1000]\ttraining's binary_logloss: 0.246361\ttraining's amex_metric: 0.794334\tvalid_1's binary_logloss: 0.251836\tvalid_1's amex_metric: 0.780897\n",
      "[1500]\ttraining's binary_logloss: 0.222081\ttraining's amex_metric: 0.808593\tvalid_1's binary_logloss: 0.231518\tvalid_1's amex_metric: 0.787854\n",
      "[2000]\ttraining's binary_logloss: 0.208161\ttraining's amex_metric: 0.821775\tvalid_1's binary_logloss: 0.222658\tvalid_1's amex_metric: 0.792067\n",
      "[2500]\ttraining's binary_logloss: 0.200975\ttraining's amex_metric: 0.831891\tvalid_1's binary_logloss: 0.219763\tvalid_1's amex_metric: 0.794849\n",
      "[3000]\ttraining's binary_logloss: 0.19386\ttraining's amex_metric: 0.841813\tvalid_1's binary_logloss: 0.217472\tvalid_1's amex_metric: 0.796113\n",
      "[3500]\ttraining's binary_logloss: 0.187228\ttraining's amex_metric: 0.852669\tvalid_1's binary_logloss: 0.215895\tvalid_1's amex_metric: 0.798793\n",
      "[4000]\ttraining's binary_logloss: 0.181376\ttraining's amex_metric: 0.863167\tvalid_1's binary_logloss: 0.214917\tvalid_1's amex_metric: 0.7995\n",
      "[4500]\ttraining's binary_logloss: 0.175701\ttraining's amex_metric: 0.872332\tvalid_1's binary_logloss: 0.214164\tvalid_1's amex_metric: 0.800472\n",
      "[5000]\ttraining's binary_logloss: 0.170077\ttraining's amex_metric: 0.882163\tvalid_1's binary_logloss: 0.2136\tvalid_1's amex_metric: 0.80068\n",
      "[5500]\ttraining's binary_logloss: 0.165051\ttraining's amex_metric: 0.890847\tvalid_1's binary_logloss: 0.213228\tvalid_1's amex_metric: 0.801717\n",
      "[6000]\ttraining's binary_logloss: 0.160765\ttraining's amex_metric: 0.898544\tvalid_1's binary_logloss: 0.213012\tvalid_1's amex_metric: 0.801598\n",
      "[6500]\ttraining's binary_logloss: 0.156241\ttraining's amex_metric: 0.906166\tvalid_1's binary_logloss: 0.212765\tvalid_1's amex_metric: 0.801732\n",
      "[7000]\ttraining's binary_logloss: 0.151048\ttraining's amex_metric: 0.914427\tvalid_1's binary_logloss: 0.212561\tvalid_1's amex_metric: 0.80151\n",
      "[7500]\ttraining's binary_logloss: 0.146127\ttraining's amex_metric: 0.922536\tvalid_1's binary_logloss: 0.212404\tvalid_1's amex_metric: 0.801802\n",
      "[8000]\ttraining's binary_logloss: 0.141729\ttraining's amex_metric: 0.92974\tvalid_1's binary_logloss: 0.212263\tvalid_1's amex_metric: 0.802299\n",
      "[8500]\ttraining's binary_logloss: 0.137971\ttraining's amex_metric: 0.936454\tvalid_1's binary_logloss: 0.212173\tvalid_1's amex_metric: 0.802071\n",
      "[9000]\ttraining's binary_logloss: 0.133723\ttraining's amex_metric: 0.942647\tvalid_1's binary_logloss: 0.212029\tvalid_1's amex_metric: 0.802826\n",
      "[9500]\ttraining's binary_logloss: 0.129948\ttraining's amex_metric: 0.948368\tvalid_1's binary_logloss: 0.211952\tvalid_1's amex_metric: 0.802993\n",
      "[10000]\ttraining's binary_logloss: 0.126237\ttraining's amex_metric: 0.953753\tvalid_1's binary_logloss: 0.211965\tvalid_1's amex_metric: 0.803179\n",
      "Our fold 0 CV score is 0.8031790940837239\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.164052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327863\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.337104\ttraining's amex_metric: 0.779473\tvalid_1's binary_logloss: 0.340486\tvalid_1's amex_metric: 0.762317\n",
      "[1000]\ttraining's binary_logloss: 0.245686\ttraining's amex_metric: 0.797251\tvalid_1's binary_logloss: 0.253695\tvalid_1's amex_metric: 0.772509\n",
      "[1500]\ttraining's binary_logloss: 0.221223\ttraining's amex_metric: 0.810615\tvalid_1's binary_logloss: 0.233994\tvalid_1's amex_metric: 0.779905\n",
      "[2000]\ttraining's binary_logloss: 0.207198\ttraining's amex_metric: 0.823624\tvalid_1's binary_logloss: 0.225449\tvalid_1's amex_metric: 0.784425\n",
      "[2500]\ttraining's binary_logloss: 0.199989\ttraining's amex_metric: 0.834387\tvalid_1's binary_logloss: 0.222675\tvalid_1's amex_metric: 0.787737\n",
      "[3000]\ttraining's binary_logloss: 0.192838\ttraining's amex_metric: 0.844553\tvalid_1's binary_logloss: 0.22054\tvalid_1's amex_metric: 0.789621\n",
      "[3500]\ttraining's binary_logloss: 0.186169\ttraining's amex_metric: 0.854796\tvalid_1's binary_logloss: 0.219095\tvalid_1's amex_metric: 0.79084\n",
      "[4000]\ttraining's binary_logloss: 0.180324\ttraining's amex_metric: 0.864458\tvalid_1's binary_logloss: 0.218273\tvalid_1's amex_metric: 0.791064\n",
      "[4500]\ttraining's binary_logloss: 0.174631\ttraining's amex_metric: 0.87436\tvalid_1's binary_logloss: 0.217666\tvalid_1's amex_metric: 0.79103\n",
      "[5000]\ttraining's binary_logloss: 0.169056\ttraining's amex_metric: 0.883872\tvalid_1's binary_logloss: 0.217275\tvalid_1's amex_metric: 0.791901\n",
      "[5500]\ttraining's binary_logloss: 0.164071\ttraining's amex_metric: 0.892476\tvalid_1's binary_logloss: 0.217006\tvalid_1's amex_metric: 0.79207\n",
      "[6000]\ttraining's binary_logloss: 0.159784\ttraining's amex_metric: 0.900195\tvalid_1's binary_logloss: 0.216889\tvalid_1's amex_metric: 0.792418\n",
      "[6500]\ttraining's binary_logloss: 0.155278\ttraining's amex_metric: 0.907497\tvalid_1's binary_logloss: 0.216676\tvalid_1's amex_metric: 0.792825\n",
      "[7000]\ttraining's binary_logloss: 0.150065\ttraining's amex_metric: 0.915715\tvalid_1's binary_logloss: 0.216412\tvalid_1's amex_metric: 0.792459\n",
      "[7500]\ttraining's binary_logloss: 0.145152\ttraining's amex_metric: 0.923822\tvalid_1's binary_logloss: 0.216253\tvalid_1's amex_metric: 0.792111\n",
      "Our fold 1 CV score is 0.7921106238296655\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.104061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327828\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.336849\ttraining's amex_metric: 0.778682\tvalid_1's binary_logloss: 0.340677\tvalid_1's amex_metric: 0.766681\n",
      "[1000]\ttraining's binary_logloss: 0.245608\ttraining's amex_metric: 0.795826\tvalid_1's binary_logloss: 0.253947\tvalid_1's amex_metric: 0.777433\n",
      "[1500]\ttraining's binary_logloss: 0.221301\ttraining's amex_metric: 0.809571\tvalid_1's binary_logloss: 0.233753\tvalid_1's amex_metric: 0.784753\n",
      "[2000]\ttraining's binary_logloss: 0.20742\ttraining's amex_metric: 0.822869\tvalid_1's binary_logloss: 0.224988\tvalid_1's amex_metric: 0.789507\n",
      "[2500]\ttraining's binary_logloss: 0.200224\ttraining's amex_metric: 0.833439\tvalid_1's binary_logloss: 0.222135\tvalid_1's amex_metric: 0.792288\n",
      "[3000]\ttraining's binary_logloss: 0.193053\ttraining's amex_metric: 0.844131\tvalid_1's binary_logloss: 0.219986\tvalid_1's amex_metric: 0.794052\n",
      "[3500]\ttraining's binary_logloss: 0.186459\ttraining's amex_metric: 0.85454\tvalid_1's binary_logloss: 0.21861\tvalid_1's amex_metric: 0.794686\n",
      "[4000]\ttraining's binary_logloss: 0.180589\ttraining's amex_metric: 0.865238\tvalid_1's binary_logloss: 0.217686\tvalid_1's amex_metric: 0.795951\n",
      "[4500]\ttraining's binary_logloss: 0.174904\ttraining's amex_metric: 0.875066\tvalid_1's binary_logloss: 0.21694\tvalid_1's amex_metric: 0.79636\n",
      "[5000]\ttraining's binary_logloss: 0.169282\ttraining's amex_metric: 0.884196\tvalid_1's binary_logloss: 0.216399\tvalid_1's amex_metric: 0.796931\n",
      "[5500]\ttraining's binary_logloss: 0.164272\ttraining's amex_metric: 0.892452\tvalid_1's binary_logloss: 0.21596\tvalid_1's amex_metric: 0.798186\n",
      "[6000]\ttraining's binary_logloss: 0.159987\ttraining's amex_metric: 0.900292\tvalid_1's binary_logloss: 0.215732\tvalid_1's amex_metric: 0.79824\n",
      "[6500]\ttraining's binary_logloss: 0.155487\ttraining's amex_metric: 0.907439\tvalid_1's binary_logloss: 0.215607\tvalid_1's amex_metric: 0.798221\n",
      "[7000]\ttraining's binary_logloss: 0.150288\ttraining's amex_metric: 0.915656\tvalid_1's binary_logloss: 0.215381\tvalid_1's amex_metric: 0.797513\n",
      "[7500]\ttraining's binary_logloss: 0.145354\ttraining's amex_metric: 0.923433\tvalid_1's binary_logloss: 0.215234\tvalid_1's amex_metric: 0.798347\n",
      "Our fold 2 CV score is 0.7983472272504591\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.326957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327802\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.336589\ttraining's amex_metric: 0.779523\tvalid_1's binary_logloss: 0.341467\tvalid_1's amex_metric: 0.76119\n",
      "[1000]\ttraining's binary_logloss: 0.245399\ttraining's amex_metric: 0.796762\tvalid_1's binary_logloss: 0.255002\tvalid_1's amex_metric: 0.77148\n",
      "[1500]\ttraining's binary_logloss: 0.221047\ttraining's amex_metric: 0.810731\tvalid_1's binary_logloss: 0.234995\tvalid_1's amex_metric: 0.778969\n",
      "[2000]\ttraining's binary_logloss: 0.207154\ttraining's amex_metric: 0.824751\tvalid_1's binary_logloss: 0.226471\tvalid_1's amex_metric: 0.782195\n",
      "[2500]\ttraining's binary_logloss: 0.199953\ttraining's amex_metric: 0.834951\tvalid_1's binary_logloss: 0.223557\tvalid_1's amex_metric: 0.785479\n",
      "[3000]\ttraining's binary_logloss: 0.192787\ttraining's amex_metric: 0.844749\tvalid_1's binary_logloss: 0.22136\tvalid_1's amex_metric: 0.78684\n",
      "[3500]\ttraining's binary_logloss: 0.186217\ttraining's amex_metric: 0.855411\tvalid_1's binary_logloss: 0.219913\tvalid_1's amex_metric: 0.787788\n",
      "[4000]\ttraining's binary_logloss: 0.180424\ttraining's amex_metric: 0.865084\tvalid_1's binary_logloss: 0.219083\tvalid_1's amex_metric: 0.788728\n",
      "[4500]\ttraining's binary_logloss: 0.174755\ttraining's amex_metric: 0.874253\tvalid_1's binary_logloss: 0.218352\tvalid_1's amex_metric: 0.789351\n",
      "[5000]\ttraining's binary_logloss: 0.169148\ttraining's amex_metric: 0.883758\tvalid_1's binary_logloss: 0.217728\tvalid_1's amex_metric: 0.789984\n",
      "[5500]\ttraining's binary_logloss: 0.164172\ttraining's amex_metric: 0.892189\tvalid_1's binary_logloss: 0.217287\tvalid_1's amex_metric: 0.789927\n",
      "[6000]\ttraining's binary_logloss: 0.159864\ttraining's amex_metric: 0.900056\tvalid_1's binary_logloss: 0.217056\tvalid_1's amex_metric: 0.790675\n",
      "[6500]\ttraining's binary_logloss: 0.155371\ttraining's amex_metric: 0.907571\tvalid_1's binary_logloss: 0.216802\tvalid_1's amex_metric: 0.791027\n",
      "[7000]\ttraining's binary_logloss: 0.150153\ttraining's amex_metric: 0.91549\tvalid_1's binary_logloss: 0.216523\tvalid_1's amex_metric: 0.790672\n",
      "[7500]\ttraining's binary_logloss: 0.145265\ttraining's amex_metric: 0.923464\tvalid_1's binary_logloss: 0.216368\tvalid_1's amex_metric: 0.790783\n",
      "[8000]\ttraining's binary_logloss: 0.140892\ttraining's amex_metric: 0.930545\tvalid_1's binary_logloss: 0.216286\tvalid_1's amex_metric: 0.791268\n",
      "[8500]\ttraining's binary_logloss: 0.137148\ttraining's amex_metric: 0.937308\tvalid_1's binary_logloss: 0.216164\tvalid_1's amex_metric: 0.792217\n",
      "Our fold 3 CV score is 0.7922166423404453\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.046618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327753\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.337171\ttraining's amex_metric: 0.777804\tvalid_1's binary_logloss: 0.340175\tvalid_1's amex_metric: 0.767707\n",
      "[1000]\ttraining's binary_logloss: 0.246033\ttraining's amex_metric: 0.795306\tvalid_1's binary_logloss: 0.25289\tvalid_1's amex_metric: 0.778261\n",
      "[1500]\ttraining's binary_logloss: 0.221735\ttraining's amex_metric: 0.809498\tvalid_1's binary_logloss: 0.232613\tvalid_1's amex_metric: 0.783958\n",
      "[2000]\ttraining's binary_logloss: 0.207851\ttraining's amex_metric: 0.822985\tvalid_1's binary_logloss: 0.223742\tvalid_1's amex_metric: 0.788254\n",
      "[2500]\ttraining's binary_logloss: 0.200726\ttraining's amex_metric: 0.833388\tvalid_1's binary_logloss: 0.22084\tvalid_1's amex_metric: 0.791733\n",
      "[3000]\ttraining's binary_logloss: 0.193552\ttraining's amex_metric: 0.843556\tvalid_1's binary_logloss: 0.218495\tvalid_1's amex_metric: 0.792871\n",
      "[3500]\ttraining's binary_logloss: 0.186915\ttraining's amex_metric: 0.853798\tvalid_1's binary_logloss: 0.21704\tvalid_1's amex_metric: 0.794092\n",
      "[4000]\ttraining's binary_logloss: 0.181097\ttraining's amex_metric: 0.863766\tvalid_1's binary_logloss: 0.216017\tvalid_1's amex_metric: 0.795417\n",
      "[4500]\ttraining's binary_logloss: 0.175392\ttraining's amex_metric: 0.872844\tvalid_1's binary_logloss: 0.21533\tvalid_1's amex_metric: 0.796656\n",
      "[5000]\ttraining's binary_logloss: 0.169796\ttraining's amex_metric: 0.882577\tvalid_1's binary_logloss: 0.214835\tvalid_1's amex_metric: 0.797953\n",
      "[5500]\ttraining's binary_logloss: 0.164798\ttraining's amex_metric: 0.891196\tvalid_1's binary_logloss: 0.214523\tvalid_1's amex_metric: 0.797462\n",
      "[6000]\ttraining's binary_logloss: 0.160489\ttraining's amex_metric: 0.899216\tvalid_1's binary_logloss: 0.214345\tvalid_1's amex_metric: 0.798626\n",
      "[6500]\ttraining's binary_logloss: 0.155931\ttraining's amex_metric: 0.90658\tvalid_1's binary_logloss: 0.214038\tvalid_1's amex_metric: 0.798396\n",
      "[7000]\ttraining's binary_logloss: 0.150749\ttraining's amex_metric: 0.914745\tvalid_1's binary_logloss: 0.213787\tvalid_1's amex_metric: 0.798379\n",
      "[7500]\ttraining's binary_logloss: 0.145822\ttraining's amex_metric: 0.922582\tvalid_1's binary_logloss: 0.213622\tvalid_1's amex_metric: 0.79879\n",
      "[8000]\ttraining's binary_logloss: 0.141445\ttraining's amex_metric: 0.929843\tvalid_1's binary_logloss: 0.213508\tvalid_1's amex_metric: 0.798965\n",
      "[8500]\ttraining's binary_logloss: 0.137694\ttraining's amex_metric: 0.936247\tvalid_1's binary_logloss: 0.21343\tvalid_1's amex_metric: 0.798948\n",
      "[9000]\ttraining's binary_logloss: 0.133456\ttraining's amex_metric: 0.942349\tvalid_1's binary_logloss: 0.213349\tvalid_1's amex_metric: 0.798626\n",
      "[9500]\ttraining's binary_logloss: 0.129657\ttraining's amex_metric: 0.948293\tvalid_1's binary_logloss: 0.21327\tvalid_1's amex_metric: 0.799383\n",
      "[10000]\ttraining's binary_logloss: 0.125941\ttraining's amex_metric: 0.95357\tvalid_1's binary_logloss: 0.213247\tvalid_1's amex_metric: 0.799076\n",
      "[10500]\ttraining's binary_logloss: 0.122683\ttraining's amex_metric: 0.957894\tvalid_1's binary_logloss: 0.213265\tvalid_1's amex_metric: 0.79879\n",
      "Our fold 4 CV score is 0.7987896154603351\n",
      "Our out of folds CV score is 0.7969750332783205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "# oof_predictions = np.zeros(len(train))\n",
    "oof_predictions = []\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "epoch = [10000,7500,7500,8500,10500]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.LGBMClassifier()\n",
    "    \n",
    "    # Save best model\n",
    "    filename = f\"../output/exp40_lgb_lagdiff_c3/lgbm_dart_fold{fold}_seed42.pkl\"\n",
    "    model = joblib.load(filename)\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(y_val)\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f08ac4-d86f-4dcd-b281-4788f52bcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeefba0-9b46-4227-b634-0a45793eb9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f92108-afb7-45f5-b3e1-706857cb9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed9500-72c1-48b4-893d-bc1fa5d7a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762416b-52f6-4464-94ff-e76c6e7da87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1dca5-1edb-4513-82d0-b66ee1b4df81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
