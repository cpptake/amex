{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp31\n",
    "\n",
    "\n",
    "exp16にc2特徴量を追加\n",
    "\n",
    "\n",
    "exp16のスコア\n",
    "\n",
    "1Fold：0.7959229889532946  　　0.7969883338523035\n",
    "2Fold：0.796323672863839       0.7954764983395108\n",
    "3Fold：0.7959284796989159      0.7941681892822336\n",
    "4Fold：0.7961656161300223      0.7946013318261542\n",
    "5Fold：0.7957363246647742      0.79518510573899\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22700\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22700\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             handles = get_handle(\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m             )\n\u001b[0;32m    342\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp35_lgb_lag2/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp35\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 1639)\n",
      "(924621, 1638)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "\n",
    "train_diff = pd.read_parquet('../feature/exp35_lagdiff/train_diff.parquet')\n",
    "test_diff = pd.read_parquet('../feature/exp35_lagdiff/test_diff.parquet')\n",
    "\n",
    "train = train.merge(train_diff,on = \"customer_ID\",how = \"left\")\n",
    "test = test.merge(test_diff,on = \"customer_ID\",how = \"left\")\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81e96cc-3556-4978-9d8b-c7148c48d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_everything(CFG.seed)\n",
    "# train, test = read_data()\n",
    "\n",
    "\n",
    "# date_train = pd.read_pickle('../feature/exp18_4_tsfresh/train_c3.pkl')\n",
    "# date_test = pd.read_pickle('../feature/exp18_4_tsfresh/test_c3.pkl')\n",
    "\n",
    "# train = train.merge(date_train,on = \"customer_ID\",how = \"left\")\n",
    "# test = test.merge(date_test,on = \"customer_ID\",how = \"left\")\n",
    "\n",
    "# del date_train,date_test\n",
    "# gc.collect\n",
    "\n",
    "# # train = pd.read_feather('../feature/exp30_statementdate/train_statedate.ftr')\n",
    "# # test0 = pd.read_feather('../feature/exp30_statementdate/test_statedate0.ftr')\n",
    "# # test1 = pd.read_feather('../feature/exp30_statementdate/test_statedate1.ftr')\n",
    "# # test2 = pd.read_feather('../feature/exp30_statementdate/test_statedate2.ftr')\n",
    "# # test3 = pd.read_feather('../feature/exp30_statementdate/test_statedate3.ftr')\n",
    "\n",
    "# # test = pd.cocat([test0,test1,test2,test3],axis = 0)\n",
    "\n",
    "\n",
    "# # del test0,test1,test2,test3\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date_train = pd.read_pickle('../feature/Statement Dates/train_SDist.pkl')\n",
    "# date_test = pd.read_pickle('../feature/Statement Dates/test_SDist.pkl')\n",
    "\n",
    "# train = train.merge(date_train,on = \"customer_ID\",how = \"left\")\n",
    "# test = test.merge(date_test,on = \"customer_ID\",how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.776629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255440\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.337604\ttraining's amex_metric: 0.777876\tvalid_1's binary_logloss: 0.339534\tvalid_1's amex_metric: 0.770204\n",
      "[1000]\ttraining's binary_logloss: 0.246434\ttraining's amex_metric: 0.793891\tvalid_1's binary_logloss: 0.251809\tvalid_1's amex_metric: 0.781686\n",
      "[1500]\ttraining's binary_logloss: 0.222175\ttraining's amex_metric: 0.808425\tvalid_1's binary_logloss: 0.231496\tvalid_1's amex_metric: 0.788199\n",
      "[2000]\ttraining's binary_logloss: 0.208421\ttraining's amex_metric: 0.82129\tvalid_1's binary_logloss: 0.222723\tvalid_1's amex_metric: 0.792704\n",
      "[2500]\ttraining's binary_logloss: 0.201314\ttraining's amex_metric: 0.83198\tvalid_1's binary_logloss: 0.219827\tvalid_1's amex_metric: 0.795344\n",
      "[3000]\ttraining's binary_logloss: 0.194256\ttraining's amex_metric: 0.841831\tvalid_1's binary_logloss: 0.21763\tvalid_1's amex_metric: 0.797224\n",
      "[3500]\ttraining's binary_logloss: 0.187754\ttraining's amex_metric: 0.85237\tvalid_1's binary_logloss: 0.216009\tvalid_1's amex_metric: 0.798929\n",
      "[4000]\ttraining's binary_logloss: 0.181967\ttraining's amex_metric: 0.862539\tvalid_1's binary_logloss: 0.215036\tvalid_1's amex_metric: 0.799881\n",
      "[4500]\ttraining's binary_logloss: 0.176385\ttraining's amex_metric: 0.871587\tvalid_1's binary_logloss: 0.214277\tvalid_1's amex_metric: 0.800039\n",
      "[5000]\ttraining's binary_logloss: 0.170897\ttraining's amex_metric: 0.881091\tvalid_1's binary_logloss: 0.213725\tvalid_1's amex_metric: 0.801058\n",
      "[5500]\ttraining's binary_logloss: 0.165972\ttraining's amex_metric: 0.8897\tvalid_1's binary_logloss: 0.21331\tvalid_1's amex_metric: 0.80186\n",
      "[6000]\ttraining's binary_logloss: 0.16175\ttraining's amex_metric: 0.897136\tvalid_1's binary_logloss: 0.213076\tvalid_1's amex_metric: 0.802712\n",
      "[6500]\ttraining's binary_logloss: 0.157305\ttraining's amex_metric: 0.904208\tvalid_1's binary_logloss: 0.212764\tvalid_1's amex_metric: 0.802529\n",
      "[7000]\ttraining's binary_logloss: 0.152179\ttraining's amex_metric: 0.9126\tvalid_1's binary_logloss: 0.212518\tvalid_1's amex_metric: 0.803065\n",
      "[7500]\ttraining's binary_logloss: 0.147279\ttraining's amex_metric: 0.920462\tvalid_1's binary_logloss: 0.212257\tvalid_1's amex_metric: 0.803537\n",
      "[8000]\ttraining's binary_logloss: 0.142944\ttraining's amex_metric: 0.927558\tvalid_1's binary_logloss: 0.212139\tvalid_1's amex_metric: 0.802616\n",
      "[8500]\ttraining's binary_logloss: 0.139215\ttraining's amex_metric: 0.934124\tvalid_1's binary_logloss: 0.212034\tvalid_1's amex_metric: 0.803427\n",
      "[9000]\ttraining's binary_logloss: 0.135017\ttraining's amex_metric: 0.940215\tvalid_1's binary_logloss: 0.211905\tvalid_1's amex_metric: 0.80279\n",
      "[9500]\ttraining's binary_logloss: 0.131242\ttraining's amex_metric: 0.94562\tvalid_1's binary_logloss: 0.21186\tvalid_1's amex_metric: 0.803915\n",
      "[10000]\ttraining's binary_logloss: 0.127563\ttraining's amex_metric: 0.951053\tvalid_1's binary_logloss: 0.211833\tvalid_1's amex_metric: 0.804027\n",
      "[10500]\ttraining's binary_logloss: 0.12433\ttraining's amex_metric: 0.955875\tvalid_1's binary_logloss: 0.211874\tvalid_1's amex_metric: 0.803722\n",
      "Our fold 0 CV score is 0.8037215965037502\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.588659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255571\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.337222\ttraining's amex_metric: 0.779618\tvalid_1's binary_logloss: 0.340572\tvalid_1's amex_metric: 0.762957\n",
      "[1000]\ttraining's binary_logloss: 0.245754\ttraining's amex_metric: 0.796926\tvalid_1's binary_logloss: 0.253585\tvalid_1's amex_metric: 0.773358\n",
      "[1500]\ttraining's binary_logloss: 0.221438\ttraining's amex_metric: 0.810387\tvalid_1's binary_logloss: 0.233831\tvalid_1's amex_metric: 0.780723\n",
      "[2000]\ttraining's binary_logloss: 0.207538\ttraining's amex_metric: 0.82343\tvalid_1's binary_logloss: 0.225488\tvalid_1's amex_metric: 0.78435\n",
      "[2500]\ttraining's binary_logloss: 0.200477\ttraining's amex_metric: 0.833296\tvalid_1's binary_logloss: 0.222864\tvalid_1's amex_metric: 0.786358\n",
      "[3000]\ttraining's binary_logloss: 0.193396\ttraining's amex_metric: 0.84356\tvalid_1's binary_logloss: 0.220735\tvalid_1's amex_metric: 0.788783\n",
      "[3500]\ttraining's binary_logloss: 0.186812\ttraining's amex_metric: 0.854078\tvalid_1's binary_logloss: 0.219364\tvalid_1's amex_metric: 0.789831\n",
      "[4000]\ttraining's binary_logloss: 0.18103\ttraining's amex_metric: 0.863246\tvalid_1's binary_logloss: 0.218453\tvalid_1's amex_metric: 0.791222\n",
      "[4500]\ttraining's binary_logloss: 0.17543\ttraining's amex_metric: 0.872797\tvalid_1's binary_logloss: 0.217823\tvalid_1's amex_metric: 0.791879\n",
      "[5000]\ttraining's binary_logloss: 0.169876\ttraining's amex_metric: 0.88219\tvalid_1's binary_logloss: 0.21727\tvalid_1's amex_metric: 0.793058\n",
      "[5500]\ttraining's binary_logloss: 0.164934\ttraining's amex_metric: 0.890521\tvalid_1's binary_logloss: 0.216836\tvalid_1's amex_metric: 0.794068\n",
      "[6000]\ttraining's binary_logloss: 0.160698\ttraining's amex_metric: 0.898597\tvalid_1's binary_logloss: 0.216632\tvalid_1's amex_metric: 0.794225\n",
      "[6500]\ttraining's binary_logloss: 0.156215\ttraining's amex_metric: 0.906115\tvalid_1's binary_logloss: 0.216449\tvalid_1's amex_metric: 0.794005\n",
      "[7000]\ttraining's binary_logloss: 0.151089\ttraining's amex_metric: 0.913959\tvalid_1's binary_logloss: 0.21619\tvalid_1's amex_metric: 0.79397\n",
      "[7500]\ttraining's binary_logloss: 0.14623\ttraining's amex_metric: 0.921834\tvalid_1's binary_logloss: 0.216062\tvalid_1's amex_metric: 0.794638\n",
      "[8000]\ttraining's binary_logloss: 0.141918\ttraining's amex_metric: 0.928837\tvalid_1's binary_logloss: 0.216047\tvalid_1's amex_metric: 0.794276\n",
      "[8500]\ttraining's binary_logloss: 0.138228\ttraining's amex_metric: 0.934938\tvalid_1's binary_logloss: 0.216074\tvalid_1's amex_metric: 0.794314\n",
      "[9000]\ttraining's binary_logloss: 0.134026\ttraining's amex_metric: 0.941496\tvalid_1's binary_logloss: 0.216018\tvalid_1's amex_metric: 0.793535\n",
      "[9500]\ttraining's binary_logloss: 0.130288\ttraining's amex_metric: 0.94721\tvalid_1's binary_logloss: 0.215972\tvalid_1's amex_metric: 0.793532\n",
      "[10000]\ttraining's binary_logloss: 0.126591\ttraining's amex_metric: 0.952496\tvalid_1's binary_logloss: 0.215949\tvalid_1's amex_metric: 0.792959\n",
      "[10500]\ttraining's binary_logloss: 0.123367\ttraining's amex_metric: 0.956883\tvalid_1's binary_logloss: 0.215946\tvalid_1's amex_metric: 0.793158\n",
      "Our fold 1 CV score is 0.7931576891230516\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.757508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255484\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.337222\ttraining's amex_metric: 0.778141\tvalid_1's binary_logloss: 0.341033\tvalid_1's amex_metric: 0.767613\n",
      "[1000]\ttraining's binary_logloss: 0.245788\ttraining's amex_metric: 0.795556\tvalid_1's binary_logloss: 0.25391\tvalid_1's amex_metric: 0.777843\n",
      "[1500]\ttraining's binary_logloss: 0.221514\ttraining's amex_metric: 0.809551\tvalid_1's binary_logloss: 0.233819\tvalid_1's amex_metric: 0.784641\n",
      "[2000]\ttraining's binary_logloss: 0.207758\ttraining's amex_metric: 0.822336\tvalid_1's binary_logloss: 0.225047\tvalid_1's amex_metric: 0.789831\n",
      "[2500]\ttraining's binary_logloss: 0.200683\ttraining's amex_metric: 0.832448\tvalid_1's binary_logloss: 0.222226\tvalid_1's amex_metric: 0.792238\n",
      "[3000]\ttraining's binary_logloss: 0.193625\ttraining's amex_metric: 0.842786\tvalid_1's binary_logloss: 0.220058\tvalid_1's amex_metric: 0.793292\n",
      "[3500]\ttraining's binary_logloss: 0.187115\ttraining's amex_metric: 0.852828\tvalid_1's binary_logloss: 0.21864\tvalid_1's amex_metric: 0.794443\n",
      "[4000]\ttraining's binary_logloss: 0.18135\ttraining's amex_metric: 0.862647\tvalid_1's binary_logloss: 0.217769\tvalid_1's amex_metric: 0.794456\n",
      "[4500]\ttraining's binary_logloss: 0.175742\ttraining's amex_metric: 0.872334\tvalid_1's binary_logloss: 0.21705\tvalid_1's amex_metric: 0.795123\n",
      "[5000]\ttraining's binary_logloss: 0.170223\ttraining's amex_metric: 0.881628\tvalid_1's binary_logloss: 0.216537\tvalid_1's amex_metric: 0.795386\n",
      "[5500]\ttraining's binary_logloss: 0.165279\ttraining's amex_metric: 0.890455\tvalid_1's binary_logloss: 0.216207\tvalid_1's amex_metric: 0.795844\n",
      "[6000]\ttraining's binary_logloss: 0.16104\ttraining's amex_metric: 0.898161\tvalid_1's binary_logloss: 0.215963\tvalid_1's amex_metric: 0.795964\n",
      "[6500]\ttraining's binary_logloss: 0.156589\ttraining's amex_metric: 0.905463\tvalid_1's binary_logloss: 0.215722\tvalid_1's amex_metric: 0.796664\n",
      "[7000]\ttraining's binary_logloss: 0.151439\ttraining's amex_metric: 0.913889\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex_metric: 0.797577\n",
      "[7500]\ttraining's binary_logloss: 0.146571\ttraining's amex_metric: 0.92176\tvalid_1's binary_logloss: 0.215314\tvalid_1's amex_metric: 0.798019\n",
      "[8000]\ttraining's binary_logloss: 0.142219\ttraining's amex_metric: 0.928801\tvalid_1's binary_logloss: 0.215257\tvalid_1's amex_metric: 0.797692\n",
      "[8500]\ttraining's binary_logloss: 0.13853\ttraining's amex_metric: 0.935185\tvalid_1's binary_logloss: 0.215239\tvalid_1's amex_metric: 0.797337\n",
      "[9000]\ttraining's binary_logloss: 0.134324\ttraining's amex_metric: 0.941187\tvalid_1's binary_logloss: 0.215121\tvalid_1's amex_metric: 0.797118\n",
      "[9500]\ttraining's binary_logloss: 0.130566\ttraining's amex_metric: 0.946692\tvalid_1's binary_logloss: 0.21506\tvalid_1's amex_metric: 0.796799\n",
      "[10000]\ttraining's binary_logloss: 0.126888\ttraining's amex_metric: 0.952113\tvalid_1's binary_logloss: 0.215056\tvalid_1's amex_metric: 0.797872\n",
      "[10500]\ttraining's binary_logloss: 0.123661\ttraining's amex_metric: 0.956648\tvalid_1's binary_logloss: 0.215052\tvalid_1's amex_metric: 0.797519\n",
      "Our fold 2 CV score is 0.7975192047907957\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.747277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255495\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.336951\ttraining's amex_metric: 0.780114\tvalid_1's binary_logloss: 0.34175\tvalid_1's amex_metric: 0.761252\n",
      "[1000]\ttraining's binary_logloss: 0.245504\ttraining's amex_metric: 0.796369\tvalid_1's binary_logloss: 0.25496\tvalid_1's amex_metric: 0.771968\n",
      "[1500]\ttraining's binary_logloss: 0.221257\ttraining's amex_metric: 0.811133\tvalid_1's binary_logloss: 0.235021\tvalid_1's amex_metric: 0.777377\n",
      "[2000]\ttraining's binary_logloss: 0.207425\ttraining's amex_metric: 0.824026\tvalid_1's binary_logloss: 0.226369\tvalid_1's amex_metric: 0.782216\n",
      "[2500]\ttraining's binary_logloss: 0.200333\ttraining's amex_metric: 0.834407\tvalid_1's binary_logloss: 0.223568\tvalid_1's amex_metric: 0.784493\n",
      "[3000]\ttraining's binary_logloss: 0.193324\ttraining's amex_metric: 0.844227\tvalid_1's binary_logloss: 0.22127\tvalid_1's amex_metric: 0.787596\n",
      "[3500]\ttraining's binary_logloss: 0.18682\ttraining's amex_metric: 0.854029\tvalid_1's binary_logloss: 0.219767\tvalid_1's amex_metric: 0.788646\n",
      "[4000]\ttraining's binary_logloss: 0.181072\ttraining's amex_metric: 0.863802\tvalid_1's binary_logloss: 0.218916\tvalid_1's amex_metric: 0.789689\n",
      "[4500]\ttraining's binary_logloss: 0.175468\ttraining's amex_metric: 0.873146\tvalid_1's binary_logloss: 0.218209\tvalid_1's amex_metric: 0.789857\n",
      "[5000]\ttraining's binary_logloss: 0.169958\ttraining's amex_metric: 0.882357\tvalid_1's binary_logloss: 0.217633\tvalid_1's amex_metric: 0.790601\n",
      "[5500]\ttraining's binary_logloss: 0.165067\ttraining's amex_metric: 0.890976\tvalid_1's binary_logloss: 0.217296\tvalid_1's amex_metric: 0.791073\n",
      "[6000]\ttraining's binary_logloss: 0.160827\ttraining's amex_metric: 0.898652\tvalid_1's binary_logloss: 0.217098\tvalid_1's amex_metric: 0.790926\n",
      "[6500]\ttraining's binary_logloss: 0.156341\ttraining's amex_metric: 0.906037\tvalid_1's binary_logloss: 0.216821\tvalid_1's amex_metric: 0.791437\n",
      "[7000]\ttraining's binary_logloss: 0.151181\ttraining's amex_metric: 0.913632\tvalid_1's binary_logloss: 0.216515\tvalid_1's amex_metric: 0.791995\n",
      "[7500]\ttraining's binary_logloss: 0.146334\ttraining's amex_metric: 0.921583\tvalid_1's binary_logloss: 0.216354\tvalid_1's amex_metric: 0.792273\n",
      "[8000]\ttraining's binary_logloss: 0.142018\ttraining's amex_metric: 0.928541\tvalid_1's binary_logloss: 0.216299\tvalid_1's amex_metric: 0.792828\n",
      "[8500]\ttraining's binary_logloss: 0.138292\ttraining's amex_metric: 0.935082\tvalid_1's binary_logloss: 0.216249\tvalid_1's amex_metric: 0.793099\n",
      "[9000]\ttraining's binary_logloss: 0.134121\ttraining's amex_metric: 0.941277\tvalid_1's binary_logloss: 0.216246\tvalid_1's amex_metric: 0.792566\n",
      "[9500]\ttraining's binary_logloss: 0.13038\ttraining's amex_metric: 0.946895\tvalid_1's binary_logloss: 0.216226\tvalid_1's amex_metric: 0.792468\n",
      "[10000]\ttraining's binary_logloss: 0.126695\ttraining's amex_metric: 0.952093\tvalid_1's binary_logloss: 0.216218\tvalid_1's amex_metric: 0.792428\n",
      "[10500]\ttraining's binary_logloss: 0.123475\ttraining's amex_metric: 0.956986\tvalid_1's binary_logloss: 0.216238\tvalid_1's amex_metric: 0.792004\n",
      "Our fold 3 CV score is 0.7920039054485284\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.719400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255458\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.337523\ttraining's amex_metric: 0.778192\tvalid_1's binary_logloss: 0.340471\tvalid_1's amex_metric: 0.76844\n",
      "[1000]\ttraining's binary_logloss: 0.246191\ttraining's amex_metric: 0.794879\tvalid_1's binary_logloss: 0.252959\tvalid_1's amex_metric: 0.777738\n",
      "[1500]\ttraining's binary_logloss: 0.221984\ttraining's amex_metric: 0.809399\tvalid_1's binary_logloss: 0.232587\tvalid_1's amex_metric: 0.783785\n",
      "[2000]\ttraining's binary_logloss: 0.208185\ttraining's amex_metric: 0.822359\tvalid_1's binary_logloss: 0.223775\tvalid_1's amex_metric: 0.789314\n",
      "[2500]\ttraining's binary_logloss: 0.201081\ttraining's amex_metric: 0.832712\tvalid_1's binary_logloss: 0.220872\tvalid_1's amex_metric: 0.791742\n",
      "[3000]\ttraining's binary_logloss: 0.193982\ttraining's amex_metric: 0.842498\tvalid_1's binary_logloss: 0.218531\tvalid_1's amex_metric: 0.793528\n",
      "[3500]\ttraining's binary_logloss: 0.187434\ttraining's amex_metric: 0.852814\tvalid_1's binary_logloss: 0.217044\tvalid_1's amex_metric: 0.795081\n",
      "[4000]\ttraining's binary_logloss: 0.181703\ttraining's amex_metric: 0.862225\tvalid_1's binary_logloss: 0.216108\tvalid_1's amex_metric: 0.797478\n",
      "[4500]\ttraining's binary_logloss: 0.176089\ttraining's amex_metric: 0.871831\tvalid_1's binary_logloss: 0.215428\tvalid_1's amex_metric: 0.797578\n",
      "[5000]\ttraining's binary_logloss: 0.170553\ttraining's amex_metric: 0.880552\tvalid_1's binary_logloss: 0.214836\tvalid_1's amex_metric: 0.798386\n",
      "[5500]\ttraining's binary_logloss: 0.16562\ttraining's amex_metric: 0.889406\tvalid_1's binary_logloss: 0.214398\tvalid_1's amex_metric: 0.798719\n",
      "[6000]\ttraining's binary_logloss: 0.16143\ttraining's amex_metric: 0.897129\tvalid_1's binary_logloss: 0.214239\tvalid_1's amex_metric: 0.798635\n",
      "[6500]\ttraining's binary_logloss: 0.156959\ttraining's amex_metric: 0.904493\tvalid_1's binary_logloss: 0.214023\tvalid_1's amex_metric: 0.798994\n",
      "[7000]\ttraining's binary_logloss: 0.151838\ttraining's amex_metric: 0.912507\tvalid_1's binary_logloss: 0.213802\tvalid_1's amex_metric: 0.798861\n",
      "[7500]\ttraining's binary_logloss: 0.146967\ttraining's amex_metric: 0.920769\tvalid_1's binary_logloss: 0.213693\tvalid_1's amex_metric: 0.799103\n",
      "[8000]\ttraining's binary_logloss: 0.14265\ttraining's amex_metric: 0.927922\tvalid_1's binary_logloss: 0.213585\tvalid_1's amex_metric: 0.797869\n",
      "[8500]\ttraining's binary_logloss: 0.138918\ttraining's amex_metric: 0.934474\tvalid_1's binary_logloss: 0.213467\tvalid_1's amex_metric: 0.798371\n",
      "[9000]\ttraining's binary_logloss: 0.134756\ttraining's amex_metric: 0.940514\tvalid_1's binary_logloss: 0.213416\tvalid_1's amex_metric: 0.798897\n",
      "[9500]\ttraining's binary_logloss: 0.131\ttraining's amex_metric: 0.946435\tvalid_1's binary_logloss: 0.21339\tvalid_1's amex_metric: 0.799044\n",
      "[10000]\ttraining's binary_logloss: 0.127324\ttraining's amex_metric: 0.951374\tvalid_1's binary_logloss: 0.213382\tvalid_1's amex_metric: 0.798221\n",
      "[10500]\ttraining's binary_logloss: 0.124103\ttraining's amex_metric: 0.955923\tvalid_1's binary_logloss: 0.213365\tvalid_1's amex_metric: 0.799295\n",
      "Our fold 4 CV score is 0.7992953046113944\n",
      "Our out of folds CV score is 0.7972664239808107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 10500,#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1500,\n",
    "#         eval_metric=[lgb_amex_metric],\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1dca5-1edb-4513-82d0-b66ee1b4df81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
