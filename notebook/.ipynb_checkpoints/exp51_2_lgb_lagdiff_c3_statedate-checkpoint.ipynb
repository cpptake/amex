{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diff„ÅÆXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2804\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2804\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp51_lgb_lagdiff_c3_statedate/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp51\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 2025)\n",
      "(924621, 2024)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "train = pd.read_pickle('../feature/exp50_lagdiff_c3_statedate/train_lagdiff_c3_statedate.pkl')\n",
    "test = pd.read_pickle('../feature/exp50_lagdiff_c3_statedate/test_lagdiff_c3_statedate.pkl')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f272d-2129-4b46-a8de-80d78b84a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.142854 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329961\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.468324\ttraining's amex_metric: 0.764875\tvalid_1's binary_logloss: 0.468854\tvalid_1's amex_metric: 0.759038\n",
      "[200]\ttraining's binary_logloss: 0.448268\ttraining's amex_metric: 0.768456\tvalid_1's binary_logloss: 0.448989\tvalid_1's amex_metric: 0.762844\n",
      "[300]\ttraining's binary_logloss: 0.394179\ttraining's amex_metric: 0.771245\tvalid_1's binary_logloss: 0.395321\tvalid_1's amex_metric: 0.76672\n",
      "[400]\ttraining's binary_logloss: 0.368783\ttraining's amex_metric: 0.774461\tvalid_1's binary_logloss: 0.370217\tvalid_1's amex_metric: 0.767902\n",
      "[500]\ttraining's binary_logloss: 0.337533\ttraining's amex_metric: 0.777377\tvalid_1's binary_logloss: 0.339368\tvalid_1's amex_metric: 0.769957\n",
      "[600]\ttraining's binary_logloss: 0.318893\ttraining's amex_metric: 0.779937\tvalid_1's binary_logloss: 0.321152\tvalid_1's amex_metric: 0.772169\n",
      "[700]\ttraining's binary_logloss: 0.288963\ttraining's amex_metric: 0.783361\tvalid_1's binary_logloss: 0.291999\tvalid_1's amex_metric: 0.774212\n",
      "[800]\ttraining's binary_logloss: 0.268274\ttraining's amex_metric: 0.787311\tvalid_1's binary_logloss: 0.272143\tvalid_1's amex_metric: 0.776292\n",
      "[900]\ttraining's binary_logloss: 0.252954\ttraining's amex_metric: 0.791372\tvalid_1's binary_logloss: 0.257723\tvalid_1's amex_metric: 0.779614\n",
      "[1000]\ttraining's binary_logloss: 0.246396\ttraining's amex_metric: 0.794858\tvalid_1's binary_logloss: 0.251794\tvalid_1's amex_metric: 0.781336\n",
      "[1100]\ttraining's binary_logloss: 0.236404\ttraining's amex_metric: 0.798284\tvalid_1's binary_logloss: 0.242797\tvalid_1's amex_metric: 0.783189\n",
      "[1200]\ttraining's binary_logloss: 0.233089\ttraining's amex_metric: 0.801017\tvalid_1's binary_logloss: 0.240085\tvalid_1's amex_metric: 0.784736\n",
      "[1300]\ttraining's binary_logloss: 0.229063\ttraining's amex_metric: 0.803632\tvalid_1's binary_logloss: 0.236812\tvalid_1's amex_metric: 0.785441\n",
      "[1400]\ttraining's binary_logloss: 0.224357\ttraining's amex_metric: 0.80636\tvalid_1's binary_logloss: 0.233057\tvalid_1's amex_metric: 0.787665\n",
      "[1500]\ttraining's binary_logloss: 0.222083\ttraining's amex_metric: 0.808481\tvalid_1's binary_logloss: 0.231446\tvalid_1's amex_metric: 0.787961\n",
      "[1600]\ttraining's binary_logloss: 0.216881\ttraining's amex_metric: 0.811484\tvalid_1's binary_logloss: 0.22757\tvalid_1's amex_metric: 0.789285\n",
      "[1700]\ttraining's binary_logloss: 0.214404\ttraining's amex_metric: 0.814172\tvalid_1's binary_logloss: 0.226016\tvalid_1's amex_metric: 0.791131\n",
      "[1800]\ttraining's binary_logloss: 0.213003\ttraining's amex_metric: 0.816709\tvalid_1's binary_logloss: 0.225326\tvalid_1's amex_metric: 0.791503\n",
      "[1900]\ttraining's binary_logloss: 0.21069\ttraining's amex_metric: 0.819293\tvalid_1's binary_logloss: 0.22402\tvalid_1's amex_metric: 0.792263\n",
      "[2000]\ttraining's binary_logloss: 0.208166\ttraining's amex_metric: 0.821685\tvalid_1's binary_logloss: 0.222643\tvalid_1's amex_metric: 0.792987\n",
      "[2100]\ttraining's binary_logloss: 0.206624\ttraining's amex_metric: 0.824252\tvalid_1's binary_logloss: 0.221931\tvalid_1's amex_metric: 0.793086\n",
      "[2200]\ttraining's binary_logloss: 0.205347\ttraining's amex_metric: 0.82636\tvalid_1's binary_logloss: 0.221417\tvalid_1's amex_metric: 0.793953\n",
      "[2300]\ttraining's binary_logloss: 0.20338\ttraining's amex_metric: 0.828462\tvalid_1's binary_logloss: 0.220483\tvalid_1's amex_metric: 0.794832\n",
      "[2400]\ttraining's binary_logloss: 0.202202\ttraining's amex_metric: 0.830835\tvalid_1's binary_logloss: 0.220091\tvalid_1's amex_metric: 0.795566\n",
      "[2500]\ttraining's binary_logloss: 0.200924\ttraining's amex_metric: 0.832766\tvalid_1's binary_logloss: 0.219655\tvalid_1's amex_metric: 0.795995\n",
      "[2600]\ttraining's binary_logloss: 0.199501\ttraining's amex_metric: 0.834658\tvalid_1's binary_logloss: 0.219146\tvalid_1's amex_metric: 0.795995\n",
      "[2700]\ttraining's binary_logloss: 0.197885\ttraining's amex_metric: 0.836612\tvalid_1's binary_logloss: 0.218527\tvalid_1's amex_metric: 0.796043\n",
      "[2800]\ttraining's binary_logloss: 0.197016\ttraining's amex_metric: 0.838226\tvalid_1's binary_logloss: 0.218343\tvalid_1's amex_metric: 0.796808\n",
      "[2900]\ttraining's binary_logloss: 0.195356\ttraining's amex_metric: 0.840328\tvalid_1's binary_logloss: 0.217772\tvalid_1's amex_metric: 0.797305\n",
      "[3000]\ttraining's binary_logloss: 0.19372\ttraining's amex_metric: 0.842365\tvalid_1's binary_logloss: 0.217275\tvalid_1's amex_metric: 0.797244\n",
      "[3100]\ttraining's binary_logloss: 0.192494\ttraining's amex_metric: 0.844733\tvalid_1's binary_logloss: 0.216982\tvalid_1's amex_metric: 0.798124\n",
      "[3200]\ttraining's binary_logloss: 0.19118\ttraining's amex_metric: 0.846719\tvalid_1's binary_logloss: 0.216642\tvalid_1's amex_metric: 0.798434\n",
      "[3300]\ttraining's binary_logloss: 0.189961\ttraining's amex_metric: 0.848761\tvalid_1's binary_logloss: 0.216379\tvalid_1's amex_metric: 0.799134\n",
      "[3400]\ttraining's binary_logloss: 0.188336\ttraining's amex_metric: 0.851186\tvalid_1's binary_logloss: 0.21595\tvalid_1's amex_metric: 0.79896\n",
      "[3500]\ttraining's binary_logloss: 0.187147\ttraining's amex_metric: 0.853059\tvalid_1's binary_logloss: 0.215752\tvalid_1's amex_metric: 0.798868\n",
      "[3600]\ttraining's binary_logloss: 0.185974\ttraining's amex_metric: 0.8552\tvalid_1's binary_logloss: 0.215533\tvalid_1's amex_metric: 0.799165\n",
      "[3700]\ttraining's binary_logloss: 0.184305\ttraining's amex_metric: 0.857109\tvalid_1's binary_logloss: 0.21519\tvalid_1's amex_metric: 0.799402\n",
      "[3800]\ttraining's binary_logloss: 0.183227\ttraining's amex_metric: 0.859413\tvalid_1's binary_logloss: 0.215046\tvalid_1's amex_metric: 0.799585\n",
      "[3900]\ttraining's binary_logloss: 0.182273\ttraining's amex_metric: 0.861428\tvalid_1's binary_logloss: 0.21492\tvalid_1's amex_metric: 0.800012\n",
      "[4000]\ttraining's binary_logloss: 0.181327\ttraining's amex_metric: 0.863038\tvalid_1's binary_logloss: 0.214807\tvalid_1's amex_metric: 0.800391\n",
      "[4100]\ttraining's binary_logloss: 0.180216\ttraining's amex_metric: 0.86519\tvalid_1's binary_logloss: 0.214651\tvalid_1's amex_metric: 0.800327\n",
      "[4200]\ttraining's binary_logloss: 0.178834\ttraining's amex_metric: 0.867169\tvalid_1's binary_logloss: 0.214426\tvalid_1's amex_metric: 0.800352\n",
      "[4300]\ttraining's binary_logloss: 0.177427\ttraining's amex_metric: 0.869195\tvalid_1's binary_logloss: 0.214186\tvalid_1's amex_metric: 0.80004\n",
      "[4400]\ttraining's binary_logloss: 0.176754\ttraining's amex_metric: 0.871224\tvalid_1's binary_logloss: 0.214157\tvalid_1's amex_metric: 0.80035\n",
      "[4500]\ttraining's binary_logloss: 0.175645\ttraining's amex_metric: 0.872992\tvalid_1's binary_logloss: 0.214037\tvalid_1's amex_metric: 0.800888\n",
      "[4600]\ttraining's binary_logloss: 0.174122\ttraining's amex_metric: 0.875172\tvalid_1's binary_logloss: 0.213828\tvalid_1's amex_metric: 0.80063\n",
      "[4700]\ttraining's binary_logloss: 0.173263\ttraining's amex_metric: 0.87689\tvalid_1's binary_logloss: 0.213779\tvalid_1's amex_metric: 0.801099\n",
      "[4800]\ttraining's binary_logloss: 0.17218\ttraining's amex_metric: 0.878887\tvalid_1's binary_logloss: 0.213675\tvalid_1's amex_metric: 0.800686\n",
      "[4900]\ttraining's binary_logloss: 0.170941\ttraining's amex_metric: 0.880687\tvalid_1's binary_logloss: 0.213553\tvalid_1's amex_metric: 0.800203\n",
      "[5000]\ttraining's binary_logloss: 0.170034\ttraining's amex_metric: 0.882511\tvalid_1's binary_logloss: 0.213493\tvalid_1's amex_metric: 0.800772\n",
      "[5100]\ttraining's binary_logloss: 0.169165\ttraining's amex_metric: 0.884215\tvalid_1's binary_logloss: 0.213434\tvalid_1's amex_metric: 0.801277\n",
      "[5200]\ttraining's binary_logloss: 0.167946\ttraining's amex_metric: 0.886297\tvalid_1's binary_logloss: 0.213288\tvalid_1's amex_metric: 0.801667\n",
      "[5300]\ttraining's binary_logloss: 0.167269\ttraining's amex_metric: 0.887686\tvalid_1's binary_logloss: 0.213246\tvalid_1's amex_metric: 0.801797\n",
      "[5400]\ttraining's binary_logloss: 0.166178\ttraining's amex_metric: 0.889502\tvalid_1's binary_logloss: 0.213163\tvalid_1's amex_metric: 0.801391\n",
      "[5500]\ttraining's binary_logloss: 0.165017\ttraining's amex_metric: 0.891078\tvalid_1's binary_logloss: 0.213068\tvalid_1's amex_metric: 0.801682\n",
      "[5600]\ttraining's binary_logloss: 0.16414\ttraining's amex_metric: 0.892933\tvalid_1's binary_logloss: 0.213048\tvalid_1's amex_metric: 0.801771\n",
      "[5700]\ttraining's binary_logloss: 0.163137\ttraining's amex_metric: 0.894629\tvalid_1's binary_logloss: 0.212948\tvalid_1's amex_metric: 0.801798\n",
      "[5800]\ttraining's binary_logloss: 0.162398\ttraining's amex_metric: 0.895984\tvalid_1's binary_logloss: 0.212939\tvalid_1's amex_metric: 0.80208\n",
      "[5900]\ttraining's binary_logloss: 0.161342\ttraining's amex_metric: 0.897577\tvalid_1's binary_logloss: 0.212872\tvalid_1's amex_metric: 0.801755\n",
      "[6000]\ttraining's binary_logloss: 0.160703\ttraining's amex_metric: 0.899179\tvalid_1's binary_logloss: 0.212867\tvalid_1's amex_metric: 0.801471\n",
      "[6100]\ttraining's binary_logloss: 0.159859\ttraining's amex_metric: 0.900587\tvalid_1's binary_logloss: 0.212825\tvalid_1's amex_metric: 0.801813\n",
      "[6200]\ttraining's binary_logloss: 0.159008\ttraining's amex_metric: 0.901855\tvalid_1's binary_logloss: 0.212768\tvalid_1's amex_metric: 0.802184\n",
      "[6300]\ttraining's binary_logloss: 0.15803\ttraining's amex_metric: 0.903226\tvalid_1's binary_logloss: 0.212697\tvalid_1's amex_metric: 0.801902\n",
      "[6400]\ttraining's binary_logloss: 0.157237\ttraining's amex_metric: 0.904599\tvalid_1's binary_logloss: 0.212655\tvalid_1's amex_metric: 0.80175\n",
      "[6500]\ttraining's binary_logloss: 0.156204\ttraining's amex_metric: 0.906197\tvalid_1's binary_logloss: 0.212601\tvalid_1's amex_metric: 0.801923\n",
      "[6600]\ttraining's binary_logloss: 0.155053\ttraining's amex_metric: 0.907929\tvalid_1's binary_logloss: 0.21255\tvalid_1's amex_metric: 0.801736\n",
      "[6700]\ttraining's binary_logloss: 0.154021\ttraining's amex_metric: 0.909568\tvalid_1's binary_logloss: 0.212493\tvalid_1's amex_metric: 0.801666\n",
      "[6800]\ttraining's binary_logloss: 0.152895\ttraining's amex_metric: 0.911206\tvalid_1's binary_logloss: 0.212444\tvalid_1's amex_metric: 0.801525\n",
      "[6900]\ttraining's binary_logloss: 0.151971\ttraining's amex_metric: 0.912989\tvalid_1's binary_logloss: 0.212429\tvalid_1's amex_metric: 0.801151\n",
      "[7000]\ttraining's binary_logloss: 0.150999\ttraining's amex_metric: 0.91468\tvalid_1's binary_logloss: 0.21239\tvalid_1's amex_metric: 0.801163\n",
      "[7100]\ttraining's binary_logloss: 0.149865\ttraining's amex_metric: 0.916501\tvalid_1's binary_logloss: 0.21232\tvalid_1's amex_metric: 0.801832\n",
      "[7200]\ttraining's binary_logloss: 0.148852\ttraining's amex_metric: 0.918092\tvalid_1's binary_logloss: 0.212277\tvalid_1's amex_metric: 0.802158\n",
      "[7300]\ttraining's binary_logloss: 0.147942\ttraining's amex_metric: 0.919742\tvalid_1's binary_logloss: 0.212262\tvalid_1's amex_metric: 0.802058\n",
      "[7400]\ttraining's binary_logloss: 0.147038\ttraining's amex_metric: 0.921453\tvalid_1's binary_logloss: 0.212273\tvalid_1's amex_metric: 0.802157\n",
      "[7500]\ttraining's binary_logloss: 0.146072\ttraining's amex_metric: 0.923199\tvalid_1's binary_logloss: 0.21223\tvalid_1's amex_metric: 0.802233\n",
      "[7600]\ttraining's binary_logloss: 0.145227\ttraining's amex_metric: 0.924779\tvalid_1's binary_logloss: 0.212217\tvalid_1's amex_metric: 0.801983\n",
      "[7700]\ttraining's binary_logloss: 0.144514\ttraining's amex_metric: 0.926298\tvalid_1's binary_logloss: 0.212215\tvalid_1's amex_metric: 0.802343\n",
      "[7800]\ttraining's binary_logloss: 0.143691\ttraining's amex_metric: 0.927679\tvalid_1's binary_logloss: 0.212211\tvalid_1's amex_metric: 0.802404\n",
      "[7900]\ttraining's binary_logloss: 0.142764\ttraining's amex_metric: 0.928923\tvalid_1's binary_logloss: 0.212201\tvalid_1's amex_metric: 0.801874\n",
      "[8000]\ttraining's binary_logloss: 0.141685\ttraining's amex_metric: 0.930184\tvalid_1's binary_logloss: 0.212148\tvalid_1's amex_metric: 0.802242\n",
      "[8100]\ttraining's binary_logloss: 0.140928\ttraining's amex_metric: 0.931679\tvalid_1's binary_logloss: 0.212147\tvalid_1's amex_metric: 0.802601\n",
      "[8200]\ttraining's binary_logloss: 0.139955\ttraining's amex_metric: 0.933054\tvalid_1's binary_logloss: 0.212131\tvalid_1's amex_metric: 0.802014\n",
      "[8300]\ttraining's binary_logloss: 0.139263\ttraining's amex_metric: 0.934087\tvalid_1's binary_logloss: 0.21212\tvalid_1's amex_metric: 0.802292\n",
      "[8400]\ttraining's binary_logloss: 0.138612\ttraining's amex_metric: 0.935457\tvalid_1's binary_logloss: 0.212098\tvalid_1's amex_metric: 0.802172\n",
      "[8500]\ttraining's binary_logloss: 0.137933\ttraining's amex_metric: 0.936567\tvalid_1's binary_logloss: 0.212075\tvalid_1's amex_metric: 0.80218\n",
      "[8600]\ttraining's binary_logloss: 0.137128\ttraining's amex_metric: 0.93766\tvalid_1's binary_logloss: 0.212095\tvalid_1's amex_metric: 0.802317\n",
      "[8700]\ttraining's binary_logloss: 0.136014\ttraining's amex_metric: 0.938949\tvalid_1's binary_logloss: 0.212065\tvalid_1's amex_metric: 0.802571\n",
      "Our fold 0 CV score is 0.8025712120750821\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.285180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330050\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2007\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.468185\ttraining's amex_metric: 0.767646\tvalid_1's binary_logloss: 0.469242\tvalid_1's amex_metric: 0.755043\n",
      "[200]\ttraining's binary_logloss: 0.447786\ttraining's amex_metric: 0.770708\tvalid_1's binary_logloss: 0.449234\tvalid_1's amex_metric: 0.756994\n",
      "[300]\ttraining's binary_logloss: 0.393731\ttraining's amex_metric: 0.773982\tvalid_1's binary_logloss: 0.395915\tvalid_1's amex_metric: 0.759192\n",
      "[400]\ttraining's binary_logloss: 0.368384\ttraining's amex_metric: 0.776482\tvalid_1's binary_logloss: 0.371056\tvalid_1's amex_metric: 0.760694\n",
      "[500]\ttraining's binary_logloss: 0.336935\ttraining's amex_metric: 0.779324\tvalid_1's binary_logloss: 0.34031\tvalid_1's amex_metric: 0.762032\n",
      "[600]\ttraining's binary_logloss: 0.318429\ttraining's amex_metric: 0.781879\tvalid_1's binary_logloss: 0.32234\tvalid_1's amex_metric: 0.764357\n",
      "[700]\ttraining's binary_logloss: 0.288458\ttraining's amex_metric: 0.785599\tvalid_1's binary_logloss: 0.293421\tvalid_1's amex_metric: 0.766339\n",
      "[800]\ttraining's binary_logloss: 0.26763\ttraining's amex_metric: 0.789168\tvalid_1's binary_logloss: 0.273708\tvalid_1's amex_metric: 0.768637\n",
      "[900]\ttraining's binary_logloss: 0.252277\ttraining's amex_metric: 0.793544\tvalid_1's binary_logloss: 0.259497\tvalid_1's amex_metric: 0.770874\n",
      "[1000]\ttraining's binary_logloss: 0.245714\ttraining's amex_metric: 0.79681\tvalid_1's binary_logloss: 0.25369\tvalid_1's amex_metric: 0.772617\n",
      "[1100]\ttraining's binary_logloss: 0.235713\ttraining's amex_metric: 0.80038\tvalid_1's binary_logloss: 0.244965\tvalid_1's amex_metric: 0.774109\n",
      "[1200]\ttraining's binary_logloss: 0.232284\ttraining's amex_metric: 0.802962\tvalid_1's binary_logloss: 0.242277\tvalid_1's amex_metric: 0.775677\n",
      "[1300]\ttraining's binary_logloss: 0.228254\ttraining's amex_metric: 0.805464\tvalid_1's binary_logloss: 0.239127\tvalid_1's amex_metric: 0.776981\n",
      "[1400]\ttraining's binary_logloss: 0.223516\ttraining's amex_metric: 0.808197\tvalid_1's binary_logloss: 0.235418\tvalid_1's amex_metric: 0.778477\n",
      "[1500]\ttraining's binary_logloss: 0.221226\ttraining's amex_metric: 0.810611\tvalid_1's binary_logloss: 0.233918\tvalid_1's amex_metric: 0.779417\n",
      "[1600]\ttraining's binary_logloss: 0.215985\ttraining's amex_metric: 0.813524\tvalid_1's binary_logloss: 0.230206\tvalid_1's amex_metric: 0.780613\n",
      "[1700]\ttraining's binary_logloss: 0.213473\ttraining's amex_metric: 0.81608\tvalid_1's binary_logloss: 0.228689\tvalid_1's amex_metric: 0.781295\n",
      "[1800]\ttraining's binary_logloss: 0.212088\ttraining's amex_metric: 0.818462\tvalid_1's binary_logloss: 0.22806\tvalid_1's amex_metric: 0.782057\n",
      "[1900]\ttraining's binary_logloss: 0.209762\ttraining's amex_metric: 0.821091\tvalid_1's binary_logloss: 0.226788\tvalid_1's amex_metric: 0.782997\n",
      "[2000]\ttraining's binary_logloss: 0.207225\ttraining's amex_metric: 0.823794\tvalid_1's binary_logloss: 0.22546\tvalid_1's amex_metric: 0.78401\n",
      "[2100]\ttraining's binary_logloss: 0.205694\ttraining's amex_metric: 0.825992\tvalid_1's binary_logloss: 0.224822\tvalid_1's amex_metric: 0.78478\n",
      "[2200]\ttraining's binary_logloss: 0.204425\ttraining's amex_metric: 0.827926\tvalid_1's binary_logloss: 0.224405\tvalid_1's amex_metric: 0.785074\n",
      "[2300]\ttraining's binary_logloss: 0.202488\ttraining's amex_metric: 0.829955\tvalid_1's binary_logloss: 0.22359\tvalid_1's amex_metric: 0.785771\n",
      "[2400]\ttraining's binary_logloss: 0.201291\ttraining's amex_metric: 0.831899\tvalid_1's binary_logloss: 0.223173\tvalid_1's amex_metric: 0.786364\n",
      "[2500]\ttraining's binary_logloss: 0.200013\ttraining's amex_metric: 0.834168\tvalid_1's binary_logloss: 0.222808\tvalid_1's amex_metric: 0.786993\n",
      "[2600]\ttraining's binary_logloss: 0.198598\ttraining's amex_metric: 0.836555\tvalid_1's binary_logloss: 0.222351\tvalid_1's amex_metric: 0.787394\n",
      "[2700]\ttraining's binary_logloss: 0.196995\ttraining's amex_metric: 0.83878\tvalid_1's binary_logloss: 0.221861\tvalid_1's amex_metric: 0.787433\n",
      "[2800]\ttraining's binary_logloss: 0.196145\ttraining's amex_metric: 0.84052\tvalid_1's binary_logloss: 0.22171\tvalid_1's amex_metric: 0.788224\n",
      "[2900]\ttraining's binary_logloss: 0.194477\ttraining's amex_metric: 0.842936\tvalid_1's binary_logloss: 0.221214\tvalid_1's amex_metric: 0.788626\n",
      "[3000]\ttraining's binary_logloss: 0.192823\ttraining's amex_metric: 0.844993\tvalid_1's binary_logloss: 0.220724\tvalid_1's amex_metric: 0.789222\n",
      "[3100]\ttraining's binary_logloss: 0.191579\ttraining's amex_metric: 0.846704\tvalid_1's binary_logloss: 0.220395\tvalid_1's amex_metric: 0.78934\n",
      "[3200]\ttraining's binary_logloss: 0.19025\ttraining's amex_metric: 0.848742\tvalid_1's binary_logloss: 0.220103\tvalid_1's amex_metric: 0.789681\n",
      "[3300]\ttraining's binary_logloss: 0.189019\ttraining's amex_metric: 0.850843\tvalid_1's binary_logloss: 0.219837\tvalid_1's amex_metric: 0.789673\n",
      "[3400]\ttraining's binary_logloss: 0.187374\ttraining's amex_metric: 0.85324\tvalid_1's binary_logloss: 0.219457\tvalid_1's amex_metric: 0.790149\n",
      "[3500]\ttraining's binary_logloss: 0.18617\ttraining's amex_metric: 0.855157\tvalid_1's binary_logloss: 0.219247\tvalid_1's amex_metric: 0.790549\n",
      "[3600]\ttraining's binary_logloss: 0.185013\ttraining's amex_metric: 0.857234\tvalid_1's binary_logloss: 0.219092\tvalid_1's amex_metric: 0.790315\n",
      "[3700]\ttraining's binary_logloss: 0.183347\ttraining's amex_metric: 0.859411\tvalid_1's binary_logloss: 0.218761\tvalid_1's amex_metric: 0.790613\n",
      "[3800]\ttraining's binary_logloss: 0.182267\ttraining's amex_metric: 0.861695\tvalid_1's binary_logloss: 0.218635\tvalid_1's amex_metric: 0.790768\n",
      "[3900]\ttraining's binary_logloss: 0.181295\ttraining's amex_metric: 0.863583\tvalid_1's binary_logloss: 0.218544\tvalid_1's amex_metric: 0.791517\n",
      "[4000]\ttraining's binary_logloss: 0.180345\ttraining's amex_metric: 0.865045\tvalid_1's binary_logloss: 0.218424\tvalid_1's amex_metric: 0.791283\n",
      "[4100]\ttraining's binary_logloss: 0.179238\ttraining's amex_metric: 0.866951\tvalid_1's binary_logloss: 0.218284\tvalid_1's amex_metric: 0.791105\n",
      "[4200]\ttraining's binary_logloss: 0.177841\ttraining's amex_metric: 0.868747\tvalid_1's binary_logloss: 0.218104\tvalid_1's amex_metric: 0.792252\n",
      "[4300]\ttraining's binary_logloss: 0.176455\ttraining's amex_metric: 0.870795\tvalid_1's binary_logloss: 0.217925\tvalid_1's amex_metric: 0.792169\n",
      "[4400]\ttraining's binary_logloss: 0.175774\ttraining's amex_metric: 0.872529\tvalid_1's binary_logloss: 0.217906\tvalid_1's amex_metric: 0.792536\n",
      "[4500]\ttraining's binary_logloss: 0.17465\ttraining's amex_metric: 0.874485\tvalid_1's binary_logloss: 0.217783\tvalid_1's amex_metric: 0.792464\n",
      "[4600]\ttraining's binary_logloss: 0.173116\ttraining's amex_metric: 0.87645\tvalid_1's binary_logloss: 0.217608\tvalid_1's amex_metric: 0.792698\n",
      "[4700]\ttraining's binary_logloss: 0.172249\ttraining's amex_metric: 0.878323\tvalid_1's binary_logloss: 0.217564\tvalid_1's amex_metric: 0.792848\n",
      "[4800]\ttraining's binary_logloss: 0.171174\ttraining's amex_metric: 0.880077\tvalid_1's binary_logloss: 0.217487\tvalid_1's amex_metric: 0.792694\n",
      "[4900]\ttraining's binary_logloss: 0.169946\ttraining's amex_metric: 0.881847\tvalid_1's binary_logloss: 0.217401\tvalid_1's amex_metric: 0.793026\n",
      "[5000]\ttraining's binary_logloss: 0.169054\ttraining's amex_metric: 0.883657\tvalid_1's binary_logloss: 0.217347\tvalid_1's amex_metric: 0.792966\n",
      "[5100]\ttraining's binary_logloss: 0.168187\ttraining's amex_metric: 0.885541\tvalid_1's binary_logloss: 0.217297\tvalid_1's amex_metric: 0.79339\n",
      "[5200]\ttraining's binary_logloss: 0.166967\ttraining's amex_metric: 0.88749\tvalid_1's binary_logloss: 0.217179\tvalid_1's amex_metric: 0.793479\n",
      "[5300]\ttraining's binary_logloss: 0.166285\ttraining's amex_metric: 0.889185\tvalid_1's binary_logloss: 0.217153\tvalid_1's amex_metric: 0.793792\n",
      "[5400]\ttraining's binary_logloss: 0.165196\ttraining's amex_metric: 0.890813\tvalid_1's binary_logloss: 0.21708\tvalid_1's amex_metric: 0.793596\n",
      "[5500]\ttraining's binary_logloss: 0.164028\ttraining's amex_metric: 0.892755\tvalid_1's binary_logloss: 0.217015\tvalid_1's amex_metric: 0.793923\n",
      "[5600]\ttraining's binary_logloss: 0.163167\ttraining's amex_metric: 0.894466\tvalid_1's binary_logloss: 0.21696\tvalid_1's amex_metric: 0.793502\n",
      "[5700]\ttraining's binary_logloss: 0.162163\ttraining's amex_metric: 0.896031\tvalid_1's binary_logloss: 0.21691\tvalid_1's amex_metric: 0.793979\n",
      "[5800]\ttraining's binary_logloss: 0.161426\ttraining's amex_metric: 0.897456\tvalid_1's binary_logloss: 0.216879\tvalid_1's amex_metric: 0.793592\n",
      "[5900]\ttraining's binary_logloss: 0.160368\ttraining's amex_metric: 0.899178\tvalid_1's binary_logloss: 0.216824\tvalid_1's amex_metric: 0.793434\n",
      "[6000]\ttraining's binary_logloss: 0.159737\ttraining's amex_metric: 0.900573\tvalid_1's binary_logloss: 0.216828\tvalid_1's amex_metric: 0.79306\n",
      "[6100]\ttraining's binary_logloss: 0.158879\ttraining's amex_metric: 0.901982\tvalid_1's binary_logloss: 0.216822\tvalid_1's amex_metric: 0.792994\n",
      "[6200]\ttraining's binary_logloss: 0.158017\ttraining's amex_metric: 0.903303\tvalid_1's binary_logloss: 0.216788\tvalid_1's amex_metric: 0.79319\n",
      "[6300]\ttraining's binary_logloss: 0.157043\ttraining's amex_metric: 0.904563\tvalid_1's binary_logloss: 0.216732\tvalid_1's amex_metric: 0.793096\n",
      "[6400]\ttraining's binary_logloss: 0.156246\ttraining's amex_metric: 0.906125\tvalid_1's binary_logloss: 0.216727\tvalid_1's amex_metric: 0.793118\n",
      "[6500]\ttraining's binary_logloss: 0.155208\ttraining's amex_metric: 0.907813\tvalid_1's binary_logloss: 0.216653\tvalid_1's amex_metric: 0.79301\n",
      "[6600]\ttraining's binary_logloss: 0.154079\ttraining's amex_metric: 0.90939\tvalid_1's binary_logloss: 0.216603\tvalid_1's amex_metric: 0.793483\n",
      "[6700]\ttraining's binary_logloss: 0.153034\ttraining's amex_metric: 0.911182\tvalid_1's binary_logloss: 0.216522\tvalid_1's amex_metric: 0.793719\n",
      "[6800]\ttraining's binary_logloss: 0.15191\ttraining's amex_metric: 0.912656\tvalid_1's binary_logloss: 0.216463\tvalid_1's amex_metric: 0.793735\n",
      "[6900]\ttraining's binary_logloss: 0.150989\ttraining's amex_metric: 0.914493\tvalid_1's binary_logloss: 0.21647\tvalid_1's amex_metric: 0.793818\n",
      "[7000]\ttraining's binary_logloss: 0.150021\ttraining's amex_metric: 0.9161\tvalid_1's binary_logloss: 0.216422\tvalid_1's amex_metric: 0.79398\n",
      "[7100]\ttraining's binary_logloss: 0.148916\ttraining's amex_metric: 0.91803\tvalid_1's binary_logloss: 0.216351\tvalid_1's amex_metric: 0.794612\n",
      "[7200]\ttraining's binary_logloss: 0.147904\ttraining's amex_metric: 0.919545\tvalid_1's binary_logloss: 0.216277\tvalid_1's amex_metric: 0.794973\n",
      "[7300]\ttraining's binary_logloss: 0.147011\ttraining's amex_metric: 0.921053\tvalid_1's binary_logloss: 0.216235\tvalid_1's amex_metric: 0.795011\n",
      "Our fold 1 CV score is 0.7950114886955777\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.327282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 330007\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.467682\ttraining's amex_metric: 0.765324\tvalid_1's binary_logloss: 0.469117\tvalid_1's amex_metric: 0.757468\n",
      "[200]\ttraining's binary_logloss: 0.447709\ttraining's amex_metric: 0.769466\tvalid_1's binary_logloss: 0.449592\tvalid_1's amex_metric: 0.759879\n",
      "[300]\ttraining's binary_logloss: 0.393563\ttraining's amex_metric: 0.772805\tvalid_1's binary_logloss: 0.396213\tvalid_1's amex_metric: 0.763113\n",
      "[400]\ttraining's binary_logloss: 0.368135\ttraining's amex_metric: 0.775118\tvalid_1's binary_logloss: 0.371255\tvalid_1's amex_metric: 0.765396\n",
      "[500]\ttraining's binary_logloss: 0.336916\ttraining's amex_metric: 0.777926\tvalid_1's binary_logloss: 0.340707\tvalid_1's amex_metric: 0.766473\n",
      "[600]\ttraining's binary_logloss: 0.318279\ttraining's amex_metric: 0.780885\tvalid_1's binary_logloss: 0.322612\tvalid_1's amex_metric: 0.769043\n",
      "[700]\ttraining's binary_logloss: 0.288396\ttraining's amex_metric: 0.784456\tvalid_1's binary_logloss: 0.293789\tvalid_1's amex_metric: 0.770607\n",
      "[800]\ttraining's binary_logloss: 0.26756\ttraining's amex_metric: 0.788232\tvalid_1's binary_logloss: 0.273999\tvalid_1's amex_metric: 0.773769\n",
      "[900]\ttraining's binary_logloss: 0.252135\ttraining's amex_metric: 0.792634\tvalid_1's binary_logloss: 0.259684\tvalid_1's amex_metric: 0.776318\n",
      "[1000]\ttraining's binary_logloss: 0.245581\ttraining's amex_metric: 0.795775\tvalid_1's binary_logloss: 0.25382\tvalid_1's amex_metric: 0.777741\n",
      "[1100]\ttraining's binary_logloss: 0.235581\ttraining's amex_metric: 0.798995\tvalid_1's binary_logloss: 0.245003\tvalid_1's amex_metric: 0.779904\n",
      "[1200]\ttraining's binary_logloss: 0.232249\ttraining's amex_metric: 0.80158\tvalid_1's binary_logloss: 0.242301\tvalid_1's amex_metric: 0.781302\n",
      "[1300]\ttraining's binary_logloss: 0.228267\ttraining's amex_metric: 0.804417\tvalid_1's binary_logloss: 0.239085\tvalid_1's amex_metric: 0.783217\n",
      "[1400]\ttraining's binary_logloss: 0.223547\ttraining's amex_metric: 0.806984\tvalid_1's binary_logloss: 0.235339\tvalid_1's amex_metric: 0.784492\n",
      "[1500]\ttraining's binary_logloss: 0.221247\ttraining's amex_metric: 0.809407\tvalid_1's binary_logloss: 0.233702\tvalid_1's amex_metric: 0.785293\n",
      "[1600]\ttraining's binary_logloss: 0.215997\ttraining's amex_metric: 0.812627\tvalid_1's binary_logloss: 0.229861\tvalid_1's amex_metric: 0.786789\n",
      "[1700]\ttraining's binary_logloss: 0.213514\ttraining's amex_metric: 0.815608\tvalid_1's binary_logloss: 0.228314\tvalid_1's amex_metric: 0.787745\n",
      "[1800]\ttraining's binary_logloss: 0.212112\ttraining's amex_metric: 0.817995\tvalid_1's binary_logloss: 0.227612\tvalid_1's amex_metric: 0.788963\n",
      "[1900]\ttraining's binary_logloss: 0.209818\ttraining's amex_metric: 0.820626\tvalid_1's binary_logloss: 0.226268\tvalid_1's amex_metric: 0.789803\n",
      "[2000]\ttraining's binary_logloss: 0.207332\ttraining's amex_metric: 0.823019\tvalid_1's binary_logloss: 0.224937\tvalid_1's amex_metric: 0.790429\n",
      "[2100]\ttraining's binary_logloss: 0.205804\ttraining's amex_metric: 0.825167\tvalid_1's binary_logloss: 0.224248\tvalid_1's amex_metric: 0.790861\n",
      "[2200]\ttraining's binary_logloss: 0.204556\ttraining's amex_metric: 0.82732\tvalid_1's binary_logloss: 0.223763\tvalid_1's amex_metric: 0.791666\n",
      "[2300]\ttraining's binary_logloss: 0.202631\ttraining's amex_metric: 0.829199\tvalid_1's binary_logloss: 0.222884\tvalid_1's amex_metric: 0.791944\n",
      "[2400]\ttraining's binary_logloss: 0.201436\ttraining's amex_metric: 0.831598\tvalid_1's binary_logloss: 0.222488\tvalid_1's amex_metric: 0.792304\n",
      "[2500]\ttraining's binary_logloss: 0.20017\ttraining's amex_metric: 0.833616\tvalid_1's binary_logloss: 0.222056\tvalid_1's amex_metric: 0.792458\n",
      "[2600]\ttraining's binary_logloss: 0.198773\ttraining's amex_metric: 0.835825\tvalid_1's binary_logloss: 0.221569\tvalid_1's amex_metric: 0.792452\n",
      "[2700]\ttraining's binary_logloss: 0.197166\ttraining's amex_metric: 0.838118\tvalid_1's binary_logloss: 0.221016\tvalid_1's amex_metric: 0.792649\n",
      "[2800]\ttraining's binary_logloss: 0.196322\ttraining's amex_metric: 0.839745\tvalid_1's binary_logloss: 0.220854\tvalid_1's amex_metric: 0.792835\n",
      "[2900]\ttraining's binary_logloss: 0.194699\ttraining's amex_metric: 0.842032\tvalid_1's binary_logloss: 0.220299\tvalid_1's amex_metric: 0.793015\n",
      "[3000]\ttraining's binary_logloss: 0.193051\ttraining's amex_metric: 0.844246\tvalid_1's binary_logloss: 0.219839\tvalid_1's amex_metric: 0.794227\n",
      "[3100]\ttraining's binary_logloss: 0.191821\ttraining's amex_metric: 0.846328\tvalid_1's binary_logloss: 0.219567\tvalid_1's amex_metric: 0.79471\n",
      "[3200]\ttraining's binary_logloss: 0.190509\ttraining's amex_metric: 0.848331\tvalid_1's binary_logloss: 0.21927\tvalid_1's amex_metric: 0.794692\n",
      "[3300]\ttraining's binary_logloss: 0.189293\ttraining's amex_metric: 0.850325\tvalid_1's binary_logloss: 0.219035\tvalid_1's amex_metric: 0.794901\n",
      "[3400]\ttraining's binary_logloss: 0.187663\ttraining's amex_metric: 0.85234\tvalid_1's binary_logloss: 0.218608\tvalid_1's amex_metric: 0.795957\n",
      "[3500]\ttraining's binary_logloss: 0.186478\ttraining's amex_metric: 0.85465\tvalid_1's binary_logloss: 0.21841\tvalid_1's amex_metric: 0.795428\n",
      "[3600]\ttraining's binary_logloss: 0.185286\ttraining's amex_metric: 0.856385\tvalid_1's binary_logloss: 0.218215\tvalid_1's amex_metric: 0.795501\n",
      "[3700]\ttraining's binary_logloss: 0.183641\ttraining's amex_metric: 0.858649\tvalid_1's binary_logloss: 0.217899\tvalid_1's amex_metric: 0.795553\n",
      "[3800]\ttraining's binary_logloss: 0.182547\ttraining's amex_metric: 0.86061\tvalid_1's binary_logloss: 0.217718\tvalid_1's amex_metric: 0.795868\n",
      "[3900]\ttraining's binary_logloss: 0.181572\ttraining's amex_metric: 0.862787\tvalid_1's binary_logloss: 0.217631\tvalid_1's amex_metric: 0.795923\n",
      "[4000]\ttraining's binary_logloss: 0.180631\ttraining's amex_metric: 0.864408\tvalid_1's binary_logloss: 0.217545\tvalid_1's amex_metric: 0.796351\n",
      "[4100]\ttraining's binary_logloss: 0.179534\ttraining's amex_metric: 0.866485\tvalid_1's binary_logloss: 0.217436\tvalid_1's amex_metric: 0.796329\n",
      "[4200]\ttraining's binary_logloss: 0.17814\ttraining's amex_metric: 0.868492\tvalid_1's binary_logloss: 0.217224\tvalid_1's amex_metric: 0.796049\n",
      "[4300]\ttraining's binary_logloss: 0.176749\ttraining's amex_metric: 0.870766\tvalid_1's binary_logloss: 0.217036\tvalid_1's amex_metric: 0.796167\n",
      "[4400]\ttraining's binary_logloss: 0.176078\ttraining's amex_metric: 0.872461\tvalid_1's binary_logloss: 0.21699\tvalid_1's amex_metric: 0.795746\n",
      "[4500]\ttraining's binary_logloss: 0.174958\ttraining's amex_metric: 0.874192\tvalid_1's binary_logloss: 0.216856\tvalid_1's amex_metric: 0.796142\n",
      "[4600]\ttraining's binary_logloss: 0.173426\ttraining's amex_metric: 0.87646\tvalid_1's binary_logloss: 0.216678\tvalid_1's amex_metric: 0.796541\n",
      "[4700]\ttraining's binary_logloss: 0.172544\ttraining's amex_metric: 0.878192\tvalid_1's binary_logloss: 0.216633\tvalid_1's amex_metric: 0.796729\n",
      "[4800]\ttraining's binary_logloss: 0.171459\ttraining's amex_metric: 0.879987\tvalid_1's binary_logloss: 0.216528\tvalid_1's amex_metric: 0.796459\n",
      "[4900]\ttraining's binary_logloss: 0.170208\ttraining's amex_metric: 0.881856\tvalid_1's binary_logloss: 0.216399\tvalid_1's amex_metric: 0.796428\n",
      "[5000]\ttraining's binary_logloss: 0.169305\ttraining's amex_metric: 0.883877\tvalid_1's binary_logloss: 0.216362\tvalid_1's amex_metric: 0.797033\n",
      "[5100]\ttraining's binary_logloss: 0.168441\ttraining's amex_metric: 0.885483\tvalid_1's binary_logloss: 0.216333\tvalid_1's amex_metric: 0.797046\n",
      "[5200]\ttraining's binary_logloss: 0.167212\ttraining's amex_metric: 0.887274\tvalid_1's binary_logloss: 0.216216\tvalid_1's amex_metric: 0.79711\n",
      "[5300]\ttraining's binary_logloss: 0.166548\ttraining's amex_metric: 0.889059\tvalid_1's binary_logloss: 0.216226\tvalid_1's amex_metric: 0.796884\n",
      "[5400]\ttraining's binary_logloss: 0.165453\ttraining's amex_metric: 0.890828\tvalid_1's binary_logloss: 0.216131\tvalid_1's amex_metric: 0.796905\n",
      "[5500]\ttraining's binary_logloss: 0.164289\ttraining's amex_metric: 0.892346\tvalid_1's binary_logloss: 0.216024\tvalid_1's amex_metric: 0.797074\n",
      "[5600]\ttraining's binary_logloss: 0.163425\ttraining's amex_metric: 0.893928\tvalid_1's binary_logloss: 0.215997\tvalid_1's amex_metric: 0.797588\n",
      "[5700]\ttraining's binary_logloss: 0.16244\ttraining's amex_metric: 0.895749\tvalid_1's binary_logloss: 0.215923\tvalid_1's amex_metric: 0.79725\n",
      "[5800]\ttraining's binary_logloss: 0.161708\ttraining's amex_metric: 0.897294\tvalid_1's binary_logloss: 0.215902\tvalid_1's amex_metric: 0.797349\n",
      "[5900]\ttraining's binary_logloss: 0.160651\ttraining's amex_metric: 0.898651\tvalid_1's binary_logloss: 0.215794\tvalid_1's amex_metric: 0.79712\n",
      "[6000]\ttraining's binary_logloss: 0.16003\ttraining's amex_metric: 0.900053\tvalid_1's binary_logloss: 0.215802\tvalid_1's amex_metric: 0.797564\n",
      "[6100]\ttraining's binary_logloss: 0.159173\ttraining's amex_metric: 0.901483\tvalid_1's binary_logloss: 0.215769\tvalid_1's amex_metric: 0.797776\n",
      "[6200]\ttraining's binary_logloss: 0.158312\ttraining's amex_metric: 0.90281\tvalid_1's binary_logloss: 0.215721\tvalid_1's amex_metric: 0.797303\n",
      "[6300]\ttraining's binary_logloss: 0.157338\ttraining's amex_metric: 0.904205\tvalid_1's binary_logloss: 0.215661\tvalid_1's amex_metric: 0.797691\n",
      "[6400]\ttraining's binary_logloss: 0.15656\ttraining's amex_metric: 0.905939\tvalid_1's binary_logloss: 0.215635\tvalid_1's amex_metric: 0.797995\n",
      "[6500]\ttraining's binary_logloss: 0.155514\ttraining's amex_metric: 0.907318\tvalid_1's binary_logloss: 0.215582\tvalid_1's amex_metric: 0.79842\n",
      "[6600]\ttraining's binary_logloss: 0.154382\ttraining's amex_metric: 0.908669\tvalid_1's binary_logloss: 0.215517\tvalid_1's amex_metric: 0.798156\n",
      "[6700]\ttraining's binary_logloss: 0.153348\ttraining's amex_metric: 0.910384\tvalid_1's binary_logloss: 0.215469\tvalid_1's amex_metric: 0.798273\n",
      "[6800]\ttraining's binary_logloss: 0.152217\ttraining's amex_metric: 0.912232\tvalid_1's binary_logloss: 0.215426\tvalid_1's amex_metric: 0.798151\n",
      "[6900]\ttraining's binary_logloss: 0.151298\ttraining's amex_metric: 0.91406\tvalid_1's binary_logloss: 0.215403\tvalid_1's amex_metric: 0.798329\n",
      "[7000]\ttraining's binary_logloss: 0.150324\ttraining's amex_metric: 0.915559\tvalid_1's binary_logloss: 0.215388\tvalid_1's amex_metric: 0.798014\n",
      "[7100]\ttraining's binary_logloss: 0.149209\ttraining's amex_metric: 0.91744\tvalid_1's binary_logloss: 0.215366\tvalid_1's amex_metric: 0.79854\n",
      "Our fold 2 CV score is 0.7985404108111394\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.142393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329988\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.467586\ttraining's amex_metric: 0.766974\tvalid_1's binary_logloss: 0.46952\tvalid_1's amex_metric: 0.752457\n",
      "[200]\ttraining's binary_logloss: 0.447582\ttraining's amex_metric: 0.771006\tvalid_1's binary_logloss: 0.450054\tvalid_1's amex_metric: 0.7545\n",
      "[300]\ttraining's binary_logloss: 0.393274\ttraining's amex_metric: 0.774336\tvalid_1's binary_logloss: 0.396703\tvalid_1's amex_metric: 0.757934\n",
      "[400]\ttraining's binary_logloss: 0.367886\ttraining's amex_metric: 0.776664\tvalid_1's binary_logloss: 0.371888\tvalid_1's amex_metric: 0.759873\n",
      "[500]\ttraining's binary_logloss: 0.33658\ttraining's amex_metric: 0.779048\tvalid_1's binary_logloss: 0.3414\tvalid_1's amex_metric: 0.762089\n",
      "[600]\ttraining's binary_logloss: 0.317957\ttraining's amex_metric: 0.781944\tvalid_1's binary_logloss: 0.323418\tvalid_1's amex_metric: 0.763741\n",
      "[700]\ttraining's binary_logloss: 0.288079\ttraining's amex_metric: 0.785374\tvalid_1's binary_logloss: 0.294654\tvalid_1's amex_metric: 0.7653\n",
      "[800]\ttraining's binary_logloss: 0.267303\ttraining's amex_metric: 0.789451\tvalid_1's binary_logloss: 0.274961\tvalid_1's amex_metric: 0.767778\n",
      "[900]\ttraining's binary_logloss: 0.25187\ttraining's amex_metric: 0.793545\tvalid_1's binary_logloss: 0.260737\tvalid_1's amex_metric: 0.770119\n",
      "[1000]\ttraining's binary_logloss: 0.24526\ttraining's amex_metric: 0.796685\tvalid_1's binary_logloss: 0.25489\tvalid_1's amex_metric: 0.772624\n",
      "[1100]\ttraining's binary_logloss: 0.235267\ttraining's amex_metric: 0.800865\tvalid_1's binary_logloss: 0.246134\tvalid_1's amex_metric: 0.774011\n",
      "[1200]\ttraining's binary_logloss: 0.231913\ttraining's amex_metric: 0.80389\tvalid_1's binary_logloss: 0.243421\tvalid_1's amex_metric: 0.775132\n",
      "[1300]\ttraining's binary_logloss: 0.22794\ttraining's amex_metric: 0.806916\tvalid_1's binary_logloss: 0.240271\tvalid_1's amex_metric: 0.7759\n",
      "[1400]\ttraining's binary_logloss: 0.223233\ttraining's amex_metric: 0.809547\tvalid_1's binary_logloss: 0.236565\tvalid_1's amex_metric: 0.777159\n",
      "[1500]\ttraining's binary_logloss: 0.220981\ttraining's amex_metric: 0.812074\tvalid_1's binary_logloss: 0.235005\tvalid_1's amex_metric: 0.778344\n",
      "[1600]\ttraining's binary_logloss: 0.215701\ttraining's amex_metric: 0.81539\tvalid_1's binary_logloss: 0.231173\tvalid_1's amex_metric: 0.779923\n",
      "[1700]\ttraining's binary_logloss: 0.213208\ttraining's amex_metric: 0.818167\tvalid_1's binary_logloss: 0.229663\tvalid_1's amex_metric: 0.781121\n",
      "[1800]\ttraining's binary_logloss: 0.211835\ttraining's amex_metric: 0.820077\tvalid_1's binary_logloss: 0.229023\tvalid_1's amex_metric: 0.782308\n",
      "[1900]\ttraining's binary_logloss: 0.209566\ttraining's amex_metric: 0.822422\tvalid_1's binary_logloss: 0.227751\tvalid_1's amex_metric: 0.783671\n",
      "[2000]\ttraining's binary_logloss: 0.207062\ttraining's amex_metric: 0.824764\tvalid_1's binary_logloss: 0.226365\tvalid_1's amex_metric: 0.784435\n",
      "[2100]\ttraining's binary_logloss: 0.205524\ttraining's amex_metric: 0.826957\tvalid_1's binary_logloss: 0.225702\tvalid_1's amex_metric: 0.784393\n",
      "[2200]\ttraining's binary_logloss: 0.204266\ttraining's amex_metric: 0.828771\tvalid_1's binary_logloss: 0.225218\tvalid_1's amex_metric: 0.784739\n",
      "[2300]\ttraining's binary_logloss: 0.202315\ttraining's amex_metric: 0.831234\tvalid_1's binary_logloss: 0.22435\tvalid_1's amex_metric: 0.784994\n",
      "[2400]\ttraining's binary_logloss: 0.201125\ttraining's amex_metric: 0.832998\tvalid_1's binary_logloss: 0.223953\tvalid_1's amex_metric: 0.78525\n",
      "[2500]\ttraining's binary_logloss: 0.199846\ttraining's amex_metric: 0.834919\tvalid_1's binary_logloss: 0.223535\tvalid_1's amex_metric: 0.786267\n",
      "[2600]\ttraining's binary_logloss: 0.198444\ttraining's amex_metric: 0.837258\tvalid_1's binary_logloss: 0.22305\tvalid_1's amex_metric: 0.786924\n",
      "[2700]\ttraining's binary_logloss: 0.196823\ttraining's amex_metric: 0.839413\tvalid_1's binary_logloss: 0.222461\tvalid_1's amex_metric: 0.786735\n",
      "[2800]\ttraining's binary_logloss: 0.195939\ttraining's amex_metric: 0.841152\tvalid_1's binary_logloss: 0.222261\tvalid_1's amex_metric: 0.786825\n",
      "[2900]\ttraining's binary_logloss: 0.194311\ttraining's amex_metric: 0.843334\tvalid_1's binary_logloss: 0.221751\tvalid_1's amex_metric: 0.786998\n",
      "[3000]\ttraining's binary_logloss: 0.192691\ttraining's amex_metric: 0.845558\tvalid_1's binary_logloss: 0.221219\tvalid_1's amex_metric: 0.787071\n",
      "[3100]\ttraining's binary_logloss: 0.191451\ttraining's amex_metric: 0.847533\tvalid_1's binary_logloss: 0.220946\tvalid_1's amex_metric: 0.787359\n",
      "[3200]\ttraining's binary_logloss: 0.190143\ttraining's amex_metric: 0.849757\tvalid_1's binary_logloss: 0.220632\tvalid_1's amex_metric: 0.787813\n",
      "[3300]\ttraining's binary_logloss: 0.188928\ttraining's amex_metric: 0.851663\tvalid_1's binary_logloss: 0.220391\tvalid_1's amex_metric: 0.788201\n",
      "[3400]\ttraining's binary_logloss: 0.187311\ttraining's amex_metric: 0.853598\tvalid_1's binary_logloss: 0.219992\tvalid_1's amex_metric: 0.788487\n",
      "[3500]\ttraining's binary_logloss: 0.186128\ttraining's amex_metric: 0.855591\tvalid_1's binary_logloss: 0.219802\tvalid_1's amex_metric: 0.788668\n",
      "[3600]\ttraining's binary_logloss: 0.184968\ttraining's amex_metric: 0.857843\tvalid_1's binary_logloss: 0.219603\tvalid_1's amex_metric: 0.789001\n",
      "[3700]\ttraining's binary_logloss: 0.183305\ttraining's amex_metric: 0.859845\tvalid_1's binary_logloss: 0.219237\tvalid_1's amex_metric: 0.789139\n",
      "[3800]\ttraining's binary_logloss: 0.182234\ttraining's amex_metric: 0.861602\tvalid_1's binary_logloss: 0.219087\tvalid_1's amex_metric: 0.789283\n",
      "[3900]\ttraining's binary_logloss: 0.181279\ttraining's amex_metric: 0.86339\tvalid_1's binary_logloss: 0.218981\tvalid_1's amex_metric: 0.789618\n",
      "[4000]\ttraining's binary_logloss: 0.180351\ttraining's amex_metric: 0.865319\tvalid_1's binary_logloss: 0.21889\tvalid_1's amex_metric: 0.789652\n",
      "[4100]\ttraining's binary_logloss: 0.17923\ttraining's amex_metric: 0.867\tvalid_1's binary_logloss: 0.21872\tvalid_1's amex_metric: 0.790163\n",
      "[4200]\ttraining's binary_logloss: 0.177847\ttraining's amex_metric: 0.868998\tvalid_1's binary_logloss: 0.218506\tvalid_1's amex_metric: 0.789929\n",
      "[4300]\ttraining's binary_logloss: 0.176448\ttraining's amex_metric: 0.870893\tvalid_1's binary_logloss: 0.218318\tvalid_1's amex_metric: 0.790245\n",
      "[4400]\ttraining's binary_logloss: 0.175779\ttraining's amex_metric: 0.872688\tvalid_1's binary_logloss: 0.218298\tvalid_1's amex_metric: 0.791011\n",
      "[4500]\ttraining's binary_logloss: 0.174657\ttraining's amex_metric: 0.874651\tvalid_1's binary_logloss: 0.21818\tvalid_1's amex_metric: 0.790766\n",
      "[4600]\ttraining's binary_logloss: 0.173146\ttraining's amex_metric: 0.87647\tvalid_1's binary_logloss: 0.217971\tvalid_1's amex_metric: 0.7907\n",
      "[4700]\ttraining's binary_logloss: 0.172273\ttraining's amex_metric: 0.878648\tvalid_1's binary_logloss: 0.217905\tvalid_1's amex_metric: 0.790709\n",
      "[4800]\ttraining's binary_logloss: 0.171194\ttraining's amex_metric: 0.880263\tvalid_1's binary_logloss: 0.217773\tvalid_1's amex_metric: 0.790833\n",
      "[4900]\ttraining's binary_logloss: 0.169964\ttraining's amex_metric: 0.88192\tvalid_1's binary_logloss: 0.21765\tvalid_1's amex_metric: 0.79124\n",
      "[5000]\ttraining's binary_logloss: 0.169076\ttraining's amex_metric: 0.88372\tvalid_1's binary_logloss: 0.217602\tvalid_1's amex_metric: 0.790984\n",
      "[5100]\ttraining's binary_logloss: 0.168205\ttraining's amex_metric: 0.88548\tvalid_1's binary_logloss: 0.217558\tvalid_1's amex_metric: 0.791361\n",
      "[5200]\ttraining's binary_logloss: 0.166983\ttraining's amex_metric: 0.887258\tvalid_1's binary_logloss: 0.217481\tvalid_1's amex_metric: 0.791285\n",
      "[5300]\ttraining's binary_logloss: 0.166329\ttraining's amex_metric: 0.88901\tvalid_1's binary_logloss: 0.217461\tvalid_1's amex_metric: 0.791262\n",
      "[5400]\ttraining's binary_logloss: 0.165248\ttraining's amex_metric: 0.890688\tvalid_1's binary_logloss: 0.217386\tvalid_1's amex_metric: 0.791317\n",
      "[5500]\ttraining's binary_logloss: 0.164082\ttraining's amex_metric: 0.892402\tvalid_1's binary_logloss: 0.217261\tvalid_1's amex_metric: 0.791478\n",
      "[5600]\ttraining's binary_logloss: 0.16321\ttraining's amex_metric: 0.893842\tvalid_1's binary_logloss: 0.217196\tvalid_1's amex_metric: 0.791795\n",
      "[5700]\ttraining's binary_logloss: 0.162214\ttraining's amex_metric: 0.89546\tvalid_1's binary_logloss: 0.21712\tvalid_1's amex_metric: 0.791458\n",
      "[5800]\ttraining's binary_logloss: 0.161469\ttraining's amex_metric: 0.896927\tvalid_1's binary_logloss: 0.217107\tvalid_1's amex_metric: 0.791801\n",
      "[5900]\ttraining's binary_logloss: 0.160432\ttraining's amex_metric: 0.898642\tvalid_1's binary_logloss: 0.217063\tvalid_1's amex_metric: 0.792223\n",
      "[6000]\ttraining's binary_logloss: 0.159799\ttraining's amex_metric: 0.899967\tvalid_1's binary_logloss: 0.217052\tvalid_1's amex_metric: 0.792006\n",
      "[6100]\ttraining's binary_logloss: 0.158957\ttraining's amex_metric: 0.901434\tvalid_1's binary_logloss: 0.217061\tvalid_1's amex_metric: 0.792139\n",
      "[6200]\ttraining's binary_logloss: 0.158111\ttraining's amex_metric: 0.902991\tvalid_1's binary_logloss: 0.217008\tvalid_1's amex_metric: 0.791798\n",
      "[6300]\ttraining's binary_logloss: 0.157147\ttraining's amex_metric: 0.904534\tvalid_1's binary_logloss: 0.216955\tvalid_1's amex_metric: 0.791765\n",
      "[6400]\ttraining's binary_logloss: 0.156353\ttraining's amex_metric: 0.905827\tvalid_1's binary_logloss: 0.216915\tvalid_1's amex_metric: 0.791993\n",
      "[6500]\ttraining's binary_logloss: 0.15532\ttraining's amex_metric: 0.907475\tvalid_1's binary_logloss: 0.216819\tvalid_1's amex_metric: 0.791912\n",
      "[6600]\ttraining's binary_logloss: 0.154171\ttraining's amex_metric: 0.908973\tvalid_1's binary_logloss: 0.216745\tvalid_1's amex_metric: 0.791945\n",
      "[6700]\ttraining's binary_logloss: 0.153142\ttraining's amex_metric: 0.91079\tvalid_1's binary_logloss: 0.216678\tvalid_1's amex_metric: 0.79249\n",
      "[6800]\ttraining's binary_logloss: 0.152023\ttraining's amex_metric: 0.912417\tvalid_1's binary_logloss: 0.216614\tvalid_1's amex_metric: 0.792611\n",
      "[6900]\ttraining's binary_logloss: 0.151094\ttraining's amex_metric: 0.91403\tvalid_1's binary_logloss: 0.216561\tvalid_1's amex_metric: 0.792818\n",
      "[7000]\ttraining's binary_logloss: 0.150128\ttraining's amex_metric: 0.915729\tvalid_1's binary_logloss: 0.216507\tvalid_1's amex_metric: 0.792625\n",
      "[7100]\ttraining's binary_logloss: 0.149009\ttraining's amex_metric: 0.917686\tvalid_1's binary_logloss: 0.216439\tvalid_1's amex_metric: 0.792539\n",
      "[7200]\ttraining's binary_logloss: 0.148007\ttraining's amex_metric: 0.919305\tvalid_1's binary_logloss: 0.216416\tvalid_1's amex_metric: 0.792394\n",
      "[7300]\ttraining's binary_logloss: 0.147093\ttraining's amex_metric: 0.920733\tvalid_1's binary_logloss: 0.216398\tvalid_1's amex_metric: 0.792104\n",
      "[7400]\ttraining's binary_logloss: 0.146183\ttraining's amex_metric: 0.922143\tvalid_1's binary_logloss: 0.21636\tvalid_1's amex_metric: 0.792179\n",
      "[7500]\ttraining's binary_logloss: 0.145205\ttraining's amex_metric: 0.923768\tvalid_1's binary_logloss: 0.216301\tvalid_1's amex_metric: 0.79243\n",
      "[7600]\ttraining's binary_logloss: 0.144372\ttraining's amex_metric: 0.925175\tvalid_1's binary_logloss: 0.216255\tvalid_1's amex_metric: 0.792448\n",
      "[7700]\ttraining's binary_logloss: 0.143661\ttraining's amex_metric: 0.926574\tvalid_1's binary_logloss: 0.216226\tvalid_1's amex_metric: 0.792459\n",
      "[7800]\ttraining's binary_logloss: 0.142843\ttraining's amex_metric: 0.927953\tvalid_1's binary_logloss: 0.216217\tvalid_1's amex_metric: 0.792587\n",
      "[7900]\ttraining's binary_logloss: 0.141929\ttraining's amex_metric: 0.929344\tvalid_1's binary_logloss: 0.216162\tvalid_1's amex_metric: 0.792476\n",
      "[8000]\ttraining's binary_logloss: 0.140856\ttraining's amex_metric: 0.930751\tvalid_1's binary_logloss: 0.216141\tvalid_1's amex_metric: 0.792563\n",
      "[8100]\ttraining's binary_logloss: 0.140099\ttraining's amex_metric: 0.932212\tvalid_1's binary_logloss: 0.216104\tvalid_1's amex_metric: 0.792662\n",
      "[8200]\ttraining's binary_logloss: 0.139128\ttraining's amex_metric: 0.93362\tvalid_1's binary_logloss: 0.216097\tvalid_1's amex_metric: 0.792473\n",
      "[8300]\ttraining's binary_logloss: 0.138444\ttraining's amex_metric: 0.935009\tvalid_1's binary_logloss: 0.216068\tvalid_1's amex_metric: 0.792507\n",
      "[8400]\ttraining's binary_logloss: 0.137793\ttraining's amex_metric: 0.936403\tvalid_1's binary_logloss: 0.216059\tvalid_1's amex_metric: 0.792425\n",
      "[8500]\ttraining's binary_logloss: 0.137108\ttraining's amex_metric: 0.937301\tvalid_1's binary_logloss: 0.216061\tvalid_1's amex_metric: 0.792551\n",
      "[8600]\ttraining's binary_logloss: 0.136295\ttraining's amex_metric: 0.938325\tvalid_1's binary_logloss: 0.216057\tvalid_1's amex_metric: 0.792445\n",
      "[8700]\ttraining's binary_logloss: 0.135198\ttraining's amex_metric: 0.939722\tvalid_1's binary_logloss: 0.216056\tvalid_1's amex_metric: 0.792842\n",
      "[8800]\ttraining's binary_logloss: 0.134481\ttraining's amex_metric: 0.941065\tvalid_1's binary_logloss: 0.216028\tvalid_1's amex_metric: 0.792873\n",
      "[8900]\ttraining's binary_logloss: 0.133734\ttraining's amex_metric: 0.942374\tvalid_1's binary_logloss: 0.216013\tvalid_1's amex_metric: 0.793131\n",
      "[9000]\ttraining's binary_logloss: 0.132894\ttraining's amex_metric: 0.943459\tvalid_1's binary_logloss: 0.215989\tvalid_1's amex_metric: 0.793371\n",
      "Our fold 3 CV score is 0.7933713117256143\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2023 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.613092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 329930\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 2010\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.467929\ttraining's amex_metric: 0.765955\tvalid_1's binary_logloss: 0.468978\tvalid_1's amex_metric: 0.757693\n",
      "[200]\ttraining's binary_logloss: 0.44794\ttraining's amex_metric: 0.769446\tvalid_1's binary_logloss: 0.449377\tvalid_1's amex_metric: 0.760607\n",
      "[300]\ttraining's binary_logloss: 0.393828\ttraining's amex_metric: 0.772789\tvalid_1's binary_logloss: 0.395916\tvalid_1's amex_metric: 0.763429\n",
      "[400]\ttraining's binary_logloss: 0.368398\ttraining's amex_metric: 0.775084\tvalid_1's binary_logloss: 0.370931\tvalid_1's amex_metric: 0.764878\n",
      "[500]\ttraining's binary_logloss: 0.337154\ttraining's amex_metric: 0.777594\tvalid_1's binary_logloss: 0.340241\tvalid_1's amex_metric: 0.766666\n",
      "[600]\ttraining's binary_logloss: 0.318579\ttraining's amex_metric: 0.780289\tvalid_1's binary_logloss: 0.322091\tvalid_1's amex_metric: 0.768763\n",
      "[700]\ttraining's binary_logloss: 0.288678\ttraining's amex_metric: 0.784411\tvalid_1's binary_logloss: 0.293112\tvalid_1's amex_metric: 0.770768\n",
      "[800]\ttraining's binary_logloss: 0.267956\ttraining's amex_metric: 0.788024\tvalid_1's binary_logloss: 0.273316\tvalid_1's amex_metric: 0.773442\n",
      "[900]\ttraining's binary_logloss: 0.252581\ttraining's amex_metric: 0.792118\tvalid_1's binary_logloss: 0.258911\tvalid_1's amex_metric: 0.775952\n",
      "[1000]\ttraining's binary_logloss: 0.24602\ttraining's amex_metric: 0.795017\tvalid_1's binary_logloss: 0.252936\tvalid_1's amex_metric: 0.777997\n",
      "[1100]\ttraining's binary_logloss: 0.235969\ttraining's amex_metric: 0.798603\tvalid_1's binary_logloss: 0.243962\tvalid_1's amex_metric: 0.779668\n",
      "[1200]\ttraining's binary_logloss: 0.232655\ttraining's amex_metric: 0.801609\tvalid_1's binary_logloss: 0.241258\tvalid_1's amex_metric: 0.78083\n",
      "[1300]\ttraining's binary_logloss: 0.22867\ttraining's amex_metric: 0.804522\tvalid_1's binary_logloss: 0.238006\tvalid_1's amex_metric: 0.782172\n",
      "[1400]\ttraining's binary_logloss: 0.223965\ttraining's amex_metric: 0.80721\tvalid_1's binary_logloss: 0.234203\tvalid_1's amex_metric: 0.782863\n",
      "[1500]\ttraining's binary_logloss: 0.221722\ttraining's amex_metric: 0.809729\tvalid_1's binary_logloss: 0.232629\tvalid_1's amex_metric: 0.784134\n",
      "[1600]\ttraining's binary_logloss: 0.216463\ttraining's amex_metric: 0.812711\tvalid_1's binary_logloss: 0.22874\tvalid_1's amex_metric: 0.785358\n",
      "[1700]\ttraining's binary_logloss: 0.213977\ttraining's amex_metric: 0.815079\tvalid_1's binary_logloss: 0.227174\tvalid_1's amex_metric: 0.786588\n",
      "[1800]\ttraining's binary_logloss: 0.21263\ttraining's amex_metric: 0.817587\tvalid_1's binary_logloss: 0.226524\tvalid_1's amex_metric: 0.787\n",
      "[1900]\ttraining's binary_logloss: 0.210332\ttraining's amex_metric: 0.820244\tvalid_1's binary_logloss: 0.225202\tvalid_1's amex_metric: 0.788085\n",
      "[2000]\ttraining's binary_logloss: 0.207817\ttraining's amex_metric: 0.822919\tvalid_1's binary_logloss: 0.223802\tvalid_1's amex_metric: 0.789056\n",
      "[2100]\ttraining's binary_logloss: 0.206306\ttraining's amex_metric: 0.824955\tvalid_1's binary_logloss: 0.223131\tvalid_1's amex_metric: 0.790059\n",
      "[2200]\ttraining's binary_logloss: 0.205057\ttraining's amex_metric: 0.826758\tvalid_1's binary_logloss: 0.222629\tvalid_1's amex_metric: 0.790632\n",
      "[2300]\ttraining's binary_logloss: 0.203112\ttraining's amex_metric: 0.829209\tvalid_1's binary_logloss: 0.221738\tvalid_1's amex_metric: 0.790836\n",
      "[2400]\ttraining's binary_logloss: 0.201912\ttraining's amex_metric: 0.831089\tvalid_1's binary_logloss: 0.221356\tvalid_1's amex_metric: 0.792045\n",
      "[2500]\ttraining's binary_logloss: 0.200648\ttraining's amex_metric: 0.833143\tvalid_1's binary_logloss: 0.220885\tvalid_1's amex_metric: 0.792084\n",
      "[2600]\ttraining's binary_logloss: 0.199239\ttraining's amex_metric: 0.835031\tvalid_1's binary_logloss: 0.220419\tvalid_1's amex_metric: 0.792333\n",
      "[2700]\ttraining's binary_logloss: 0.197617\ttraining's amex_metric: 0.837184\tvalid_1's binary_logloss: 0.219821\tvalid_1's amex_metric: 0.792621\n",
      "[2800]\ttraining's binary_logloss: 0.196732\ttraining's amex_metric: 0.838991\tvalid_1's binary_logloss: 0.219649\tvalid_1's amex_metric: 0.792698\n",
      "[2900]\ttraining's binary_logloss: 0.195108\ttraining's amex_metric: 0.840949\tvalid_1's binary_logloss: 0.219101\tvalid_1's amex_metric: 0.793061\n",
      "[3000]\ttraining's binary_logloss: 0.193473\ttraining's amex_metric: 0.84315\tvalid_1's binary_logloss: 0.218596\tvalid_1's amex_metric: 0.793497\n",
      "[3100]\ttraining's binary_logloss: 0.192236\ttraining's amex_metric: 0.845327\tvalid_1's binary_logloss: 0.218319\tvalid_1's amex_metric: 0.793793\n",
      "[3200]\ttraining's binary_logloss: 0.190898\ttraining's amex_metric: 0.84738\tvalid_1's binary_logloss: 0.217997\tvalid_1's amex_metric: 0.793756\n",
      "[3300]\ttraining's binary_logloss: 0.189672\ttraining's amex_metric: 0.849344\tvalid_1's binary_logloss: 0.217734\tvalid_1's amex_metric: 0.794096\n",
      "[3400]\ttraining's binary_logloss: 0.188052\ttraining's amex_metric: 0.851891\tvalid_1's binary_logloss: 0.217354\tvalid_1's amex_metric: 0.794382\n",
      "[3500]\ttraining's binary_logloss: 0.186855\ttraining's amex_metric: 0.853909\tvalid_1's binary_logloss: 0.217129\tvalid_1's amex_metric: 0.79502\n",
      "[3600]\ttraining's binary_logloss: 0.185657\ttraining's amex_metric: 0.85601\tvalid_1's binary_logloss: 0.216903\tvalid_1's amex_metric: 0.795335\n",
      "[3700]\ttraining's binary_logloss: 0.183992\ttraining's amex_metric: 0.858322\tvalid_1's binary_logloss: 0.216541\tvalid_1's amex_metric: 0.795416\n",
      "[3800]\ttraining's binary_logloss: 0.182909\ttraining's amex_metric: 0.860415\tvalid_1's binary_logloss: 0.216405\tvalid_1's amex_metric: 0.795641\n",
      "[3900]\ttraining's binary_logloss: 0.181935\ttraining's amex_metric: 0.862151\tvalid_1's binary_logloss: 0.216307\tvalid_1's amex_metric: 0.795593\n",
      "[4000]\ttraining's binary_logloss: 0.180987\ttraining's amex_metric: 0.864038\tvalid_1's binary_logloss: 0.216193\tvalid_1's amex_metric: 0.795856\n",
      "[4100]\ttraining's binary_logloss: 0.179871\ttraining's amex_metric: 0.865772\tvalid_1's binary_logloss: 0.216035\tvalid_1's amex_metric: 0.796081\n",
      "[4200]\ttraining's binary_logloss: 0.178478\ttraining's amex_metric: 0.868004\tvalid_1's binary_logloss: 0.215815\tvalid_1's amex_metric: 0.796097\n",
      "[4300]\ttraining's binary_logloss: 0.17708\ttraining's amex_metric: 0.870043\tvalid_1's binary_logloss: 0.215581\tvalid_1's amex_metric: 0.796501\n",
      "[4400]\ttraining's binary_logloss: 0.176402\ttraining's amex_metric: 0.871795\tvalid_1's binary_logloss: 0.215573\tvalid_1's amex_metric: 0.796492\n",
      "[4500]\ttraining's binary_logloss: 0.17529\ttraining's amex_metric: 0.873582\tvalid_1's binary_logloss: 0.215451\tvalid_1's amex_metric: 0.796673\n",
      "[4600]\ttraining's binary_logloss: 0.173763\ttraining's amex_metric: 0.875438\tvalid_1's binary_logloss: 0.215183\tvalid_1's amex_metric: 0.796648\n",
      "[4700]\ttraining's binary_logloss: 0.172875\ttraining's amex_metric: 0.877223\tvalid_1's binary_logloss: 0.215134\tvalid_1's amex_metric: 0.796716\n",
      "[4800]\ttraining's binary_logloss: 0.171792\ttraining's amex_metric: 0.878965\tvalid_1's binary_logloss: 0.21502\tvalid_1's amex_metric: 0.796847\n",
      "[4900]\ttraining's binary_logloss: 0.17055\ttraining's amex_metric: 0.88095\tvalid_1's binary_logloss: 0.214913\tvalid_1's amex_metric: 0.796593\n",
      "[5000]\ttraining's binary_logloss: 0.169665\ttraining's amex_metric: 0.88256\tvalid_1's binary_logloss: 0.214849\tvalid_1's amex_metric: 0.796534\n",
      "[5100]\ttraining's binary_logloss: 0.168787\ttraining's amex_metric: 0.884581\tvalid_1's binary_logloss: 0.21482\tvalid_1's amex_metric: 0.797138\n",
      "[5200]\ttraining's binary_logloss: 0.167583\ttraining's amex_metric: 0.886335\tvalid_1's binary_logloss: 0.214711\tvalid_1's amex_metric: 0.796885\n",
      "[5300]\ttraining's binary_logloss: 0.166907\ttraining's amex_metric: 0.887778\tvalid_1's binary_logloss: 0.214703\tvalid_1's amex_metric: 0.797407\n",
      "[5400]\ttraining's binary_logloss: 0.165818\ttraining's amex_metric: 0.889577\tvalid_1's binary_logloss: 0.214628\tvalid_1's amex_metric: 0.797434\n",
      "[5500]\ttraining's binary_logloss: 0.164661\ttraining's amex_metric: 0.891367\tvalid_1's binary_logloss: 0.214532\tvalid_1's amex_metric: 0.797011\n",
      "[5600]\ttraining's binary_logloss: 0.163774\ttraining's amex_metric: 0.89297\tvalid_1's binary_logloss: 0.214481\tvalid_1's amex_metric: 0.797446\n",
      "[5700]\ttraining's binary_logloss: 0.162784\ttraining's amex_metric: 0.894573\tvalid_1's binary_logloss: 0.214426\tvalid_1's amex_metric: 0.796931\n",
      "[5800]\ttraining's binary_logloss: 0.162042\ttraining's amex_metric: 0.895902\tvalid_1's binary_logloss: 0.214393\tvalid_1's amex_metric: 0.797431\n",
      "[5900]\ttraining's binary_logloss: 0.160982\ttraining's amex_metric: 0.897628\tvalid_1's binary_logloss: 0.214285\tvalid_1's amex_metric: 0.797075\n",
      "[6000]\ttraining's binary_logloss: 0.160361\ttraining's amex_metric: 0.899073\tvalid_1's binary_logloss: 0.214304\tvalid_1's amex_metric: 0.797582\n",
      "[6100]\ttraining's binary_logloss: 0.159509\ttraining's amex_metric: 0.900479\tvalid_1's binary_logloss: 0.214252\tvalid_1's amex_metric: 0.797843\n",
      "[6200]\ttraining's binary_logloss: 0.158663\ttraining's amex_metric: 0.901844\tvalid_1's binary_logloss: 0.214199\tvalid_1's amex_metric: 0.797981\n",
      "[6300]\ttraining's binary_logloss: 0.157686\ttraining's amex_metric: 0.903307\tvalid_1's binary_logloss: 0.214108\tvalid_1's amex_metric: 0.798484\n",
      "[6400]\ttraining's binary_logloss: 0.156895\ttraining's amex_metric: 0.904805\tvalid_1's binary_logloss: 0.21409\tvalid_1's amex_metric: 0.798657\n",
      "[6500]\ttraining's binary_logloss: 0.155862\ttraining's amex_metric: 0.906372\tvalid_1's binary_logloss: 0.214016\tvalid_1's amex_metric: 0.798416\n",
      "[6600]\ttraining's binary_logloss: 0.154721\ttraining's amex_metric: 0.908165\tvalid_1's binary_logloss: 0.213948\tvalid_1's amex_metric: 0.798802\n",
      "[6700]\ttraining's binary_logloss: 0.15369\ttraining's amex_metric: 0.909756\tvalid_1's binary_logloss: 0.213887\tvalid_1's amex_metric: 0.798417\n",
      "[6800]\ttraining's binary_logloss: 0.152567\ttraining's amex_metric: 0.911435\tvalid_1's binary_logloss: 0.213832\tvalid_1's amex_metric: 0.798405\n",
      "[6900]\ttraining's binary_logloss: 0.151643\ttraining's amex_metric: 0.913101\tvalid_1's binary_logloss: 0.213838\tvalid_1's amex_metric: 0.798805\n",
      "[7000]\ttraining's binary_logloss: 0.150676\ttraining's amex_metric: 0.91476\tvalid_1's binary_logloss: 0.213775\tvalid_1's amex_metric: 0.799241\n",
      "[7100]\ttraining's binary_logloss: 0.149546\ttraining's amex_metric: 0.916442\tvalid_1's binary_logloss: 0.213728\tvalid_1's amex_metric: 0.799036\n",
      "[7200]\ttraining's binary_logloss: 0.148529\ttraining's amex_metric: 0.918247\tvalid_1's binary_logloss: 0.213692\tvalid_1's amex_metric: 0.798899\n",
      "[7300]\ttraining's binary_logloss: 0.147607\ttraining's amex_metric: 0.919582\tvalid_1's binary_logloss: 0.213655\tvalid_1's amex_metric: 0.798993\n",
      "[7400]\ttraining's binary_logloss: 0.146704\ttraining's amex_metric: 0.921114\tvalid_1's binary_logloss: 0.213604\tvalid_1's amex_metric: 0.799349\n",
      "[7500]\ttraining's binary_logloss: 0.145724\ttraining's amex_metric: 0.922692\tvalid_1's binary_logloss: 0.213613\tvalid_1's amex_metric: 0.799256\n",
      "[7600]\ttraining's binary_logloss: 0.144877\ttraining's amex_metric: 0.924149\tvalid_1's binary_logloss: 0.213615\tvalid_1's amex_metric: 0.799233\n",
      "[7700]\ttraining's binary_logloss: 0.144173\ttraining's amex_metric: 0.925414\tvalid_1's binary_logloss: 0.213596\tvalid_1's amex_metric: 0.799201\n",
      "[7800]\ttraining's binary_logloss: 0.143335\ttraining's amex_metric: 0.926929\tvalid_1's binary_logloss: 0.213568\tvalid_1's amex_metric: 0.799544\n",
      "[7900]\ttraining's binary_logloss: 0.142398\ttraining's amex_metric: 0.9283\tvalid_1's binary_logloss: 0.213526\tvalid_1's amex_metric: 0.799887\n",
      "[8000]\ttraining's binary_logloss: 0.141313\ttraining's amex_metric: 0.929759\tvalid_1's binary_logloss: 0.213493\tvalid_1's amex_metric: 0.800101\n",
      "[8100]\ttraining's binary_logloss: 0.140563\ttraining's amex_metric: 0.931086\tvalid_1's binary_logloss: 0.213476\tvalid_1's amex_metric: 0.799455\n",
      "[8200]\ttraining's binary_logloss: 0.139594\ttraining's amex_metric: 0.932456\tvalid_1's binary_logloss: 0.213482\tvalid_1's amex_metric: 0.799554\n",
      "[8300]\ttraining's binary_logloss: 0.138909\ttraining's amex_metric: 0.933993\tvalid_1's binary_logloss: 0.213507\tvalid_1's amex_metric: 0.799695\n",
      "[8400]\ttraining's binary_logloss: 0.138271\ttraining's amex_metric: 0.93493\tvalid_1's binary_logloss: 0.213504\tvalid_1's amex_metric: 0.799757\n",
      "[8500]\ttraining's binary_logloss: 0.13759\ttraining's amex_metric: 0.936394\tvalid_1's binary_logloss: 0.213524\tvalid_1's amex_metric: 0.799644\n",
      "[8600]\ttraining's binary_logloss: 0.136772\ttraining's amex_metric: 0.937428\tvalid_1's binary_logloss: 0.21351\tvalid_1's amex_metric: 0.799704\n",
      "[8700]\ttraining's binary_logloss: 0.135663\ttraining's amex_metric: 0.938704\tvalid_1's binary_logloss: 0.213495\tvalid_1's amex_metric: 0.799702\n",
      "[8800]\ttraining's binary_logloss: 0.134921\ttraining's amex_metric: 0.939996\tvalid_1's binary_logloss: 0.213449\tvalid_1's amex_metric: 0.799805\n",
      "[8900]\ttraining's binary_logloss: 0.134167\ttraining's amex_metric: 0.941364\tvalid_1's binary_logloss: 0.213408\tvalid_1's amex_metric: 0.799966\n",
      "[9000]\ttraining's binary_logloss: 0.133319\ttraining's amex_metric: 0.942783\tvalid_1's binary_logloss: 0.213391\tvalid_1's amex_metric: 0.799551\n",
      "[9100]\ttraining's binary_logloss: 0.132705\ttraining's amex_metric: 0.94385\tvalid_1's binary_logloss: 0.213389\tvalid_1's amex_metric: 0.799363\n",
      "[9200]\ttraining's binary_logloss: 0.131702\ttraining's amex_metric: 0.944942\tvalid_1's binary_logloss: 0.213332\tvalid_1's amex_metric: 0.799799\n",
      "[9300]\ttraining's binary_logloss: 0.130938\ttraining's amex_metric: 0.946062\tvalid_1's binary_logloss: 0.213347\tvalid_1's amex_metric: 0.79977\n",
      "[9400]\ttraining's binary_logloss: 0.130497\ttraining's amex_metric: 0.947059\tvalid_1's binary_logloss: 0.213351\tvalid_1's amex_metric: 0.799625\n",
      "[9500]\ttraining's binary_logloss: 0.129559\ttraining's amex_metric: 0.9482\tvalid_1's binary_logloss: 0.213309\tvalid_1's amex_metric: 0.799256\n",
      "[9600]\ttraining's binary_logloss: 0.128778\ttraining's amex_metric: 0.949369\tvalid_1's binary_logloss: 0.213288\tvalid_1's amex_metric: 0.799786\n",
      "[9700]\ttraining's binary_logloss: 0.128011\ttraining's amex_metric: 0.950355\tvalid_1's binary_logloss: 0.213283\tvalid_1's amex_metric: 0.799408\n",
      "[9800]\ttraining's binary_logloss: 0.127229\ttraining's amex_metric: 0.95148\tvalid_1's binary_logloss: 0.213246\tvalid_1's amex_metric: 0.799441\n",
      "[9900]\ttraining's binary_logloss: 0.126488\ttraining's amex_metric: 0.952506\tvalid_1's binary_logloss: 0.213232\tvalid_1's amex_metric: 0.799364\n",
      "[10000]\ttraining's binary_logloss: 0.125856\ttraining's amex_metric: 0.953497\tvalid_1's binary_logloss: 0.213224\tvalid_1's amex_metric: 0.799829\n",
      "[10100]\ttraining's binary_logloss: 0.125476\ttraining's amex_metric: 0.95443\tvalid_1's binary_logloss: 0.213233\tvalid_1's amex_metric: 0.799889\n",
      "[10200]\ttraining's binary_logloss: 0.124815\ttraining's amex_metric: 0.955272\tvalid_1's binary_logloss: 0.213234\tvalid_1's amex_metric: 0.80014\n",
      "[10300]\ttraining's binary_logloss: 0.124062\ttraining's amex_metric: 0.956145\tvalid_1's binary_logloss: 0.21324\tvalid_1's amex_metric: 0.800032\n",
      "[10400]\ttraining's binary_logloss: 0.123405\ttraining's amex_metric: 0.957066\tvalid_1's binary_logloss: 0.213224\tvalid_1's amex_metric: 0.800143\n",
      "Our fold 4 CV score is 0.8001426163667096\n",
      "Our out of folds CV score is 0.7974057306116666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = []\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "epoch = [8700,7300,7100,9000,10400]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = epoch[fold],#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1000,\n",
    "#         eval_metric=[lgb_amex_metric],\n",
    "        verbose_eval = 100,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    \n",
    "    # Add to out of folds array\n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(tr_target, oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f08ac4-d86f-4dcd-b281-4788f52bcce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.01965173477415514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeefba0-9b46-4227-b634-0a45793eb9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f92108-afb7-45f5-b3e1-706857cb9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed9500-72c1-48b4-893d-bc1fa5d7a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762416b-52f6-4464-94ff-e76c6e7da87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1dca5-1edb-4513-82d0-b66ee1b4df81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
