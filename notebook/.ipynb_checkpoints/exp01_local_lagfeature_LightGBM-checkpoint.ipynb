{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f5163c-e067-49de-ac94-7cb0b419542d",
   "metadata": {},
   "source": [
    "# Lag Features Are All You Need\n",
    "\n",
    "下記のNotebookのローカル版\n",
    "https://www.kaggle.com/code/thedevastator/lag-features-are-all-you-need\n",
    "\n",
    "ラグ特徴量を用いて、1823 次元の特徴でモデル作成をしている。\n",
    "\n",
    "### exp01\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801c6d58-ddf4-46d8-9029-80aeb1ae544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "\n",
    "import zipfile\n",
    "from collections import OrderedDict\n",
    "import traceback\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from catboost import CatBoostClassifier\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2a7cef-dd17-406b-8335-7767bbb58821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train feature extraction\n",
      "Train shape:  (458913, 1462)\n",
      "Starting test feature extraction\n",
      "Test shape:  (924621, 1461)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    \n",
    "    # Train FE\n",
    "    print('Starting train feature extraction')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "            train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "            train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    print('Train shape: ', train.shape)    \n",
    "    del train_num_agg, train_cat_agg        \n",
    "    gc.collect()\n",
    "    \n",
    "    # Test FE\n",
    "    test = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    print('Starting test feature extraction')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        if 'last' in col and col.replace('last', 'first') in test_num_agg:\n",
    "            test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n",
    "            test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', 'first')]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    \n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID')\n",
    "    print('Test shape: ', test.shape)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # Save files to disk\n",
    "    train.to_parquet('train_fe_plus_plus.parquet')\n",
    "    test.to_parquet('test_fe_plus_plus.parquet')\n",
    "    \n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0e995-4c0f-4a53-a3a1-adb79a0a188c",
   "metadata": {},
   "source": [
    "# Lag feature 取り出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e993f72-6a83-4695-b693-37825200e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train = pd.read_parquet('../feature/exp01_lag_feature/train_fe_plus_plus.parquet')\n",
    "# test = pd.read_parquet('../feature/exp01_lag_feature/test_fe_plus_plus.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c03a7ff9-7619-4e28-9c77-bd4e62e65757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量選定\n",
    "# feature_list = train.columns.to_list()\n",
    "\n",
    "\n",
    "# lag_features = [col for col in feature_list if \"_lag\" in col]\n",
    "# lag_features.extend([\"customer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621c4b3c-fc82-45d8-b928-3757a9d3d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # len(lag_features)\n",
    "# # train[lag_features.extend([\"customer_ID\"])].head()\n",
    "# # lag_features.extend([\"customer_ID\"])\n",
    "# # lag_features\n",
    "\n",
    "# train[lag_features].to_parquet('../feature/exp01_lag_feature/train_lag_feature.parquet')\n",
    "# test[lag_features].to_parquet('../feature/exp01_lag_feature/t_lag_feature.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a417871-a378-4723-bf2e-68a0edc0df7f",
   "metadata": {},
   "source": [
    "# train _ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0784b1e7-b09f-4d11-91a4-10d21d110cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_dir = '../input/amex-fe/'\n",
    "    seed = 45\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f57932-4be7-4d46-8667-f937e1363d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.897926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286621\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303355\ttraining's amex_metric: 0.782033\tvalid_1's binary_logloss: 0.306879\tvalid_1's amex_metric: 0.766515\n",
      "[1000]\ttraining's binary_logloss: 0.238788\ttraining's amex_metric: 0.796756\tvalid_1's binary_logloss: 0.246551\tvalid_1's amex_metric: 0.775687\n",
      "[1500]\ttraining's binary_logloss: 0.218247\ttraining's amex_metric: 0.811551\tvalid_1's binary_logloss: 0.230527\tvalid_1's amex_metric: 0.779932\n",
      "[2000]\ttraining's binary_logloss: 0.208364\ttraining's amex_metric: 0.822377\tvalid_1's binary_logloss: 0.224987\tvalid_1's amex_metric: 0.785728\n",
      "[2500]\ttraining's binary_logloss: 0.198769\ttraining's amex_metric: 0.83365\tvalid_1's binary_logloss: 0.220942\tvalid_1's amex_metric: 0.787676\n",
      "[3000]\ttraining's binary_logloss: 0.192331\ttraining's amex_metric: 0.84417\tvalid_1's binary_logloss: 0.219279\tvalid_1's amex_metric: 0.789249\n",
      "[3500]\ttraining's binary_logloss: 0.186478\ttraining's amex_metric: 0.85367\tvalid_1's binary_logloss: 0.218295\tvalid_1's amex_metric: 0.79066\n",
      "[4000]\ttraining's binary_logloss: 0.180289\ttraining's amex_metric: 0.863908\tvalid_1's binary_logloss: 0.217336\tvalid_1's amex_metric: 0.791446\n",
      "[4500]\ttraining's binary_logloss: 0.175364\ttraining's amex_metric: 0.872443\tvalid_1's binary_logloss: 0.216908\tvalid_1's amex_metric: 0.792118\n",
      "[5000]\ttraining's binary_logloss: 0.169687\ttraining's amex_metric: 0.88196\tvalid_1's binary_logloss: 0.216383\tvalid_1's amex_metric: 0.792958\n",
      "[5500]\ttraining's binary_logloss: 0.164688\ttraining's amex_metric: 0.891172\tvalid_1's binary_logloss: 0.216037\tvalid_1's amex_metric: 0.793606\n",
      "[6000]\ttraining's binary_logloss: 0.159462\ttraining's amex_metric: 0.899297\tvalid_1's binary_logloss: 0.215781\tvalid_1's amex_metric: 0.793232\n",
      "[6500]\ttraining's binary_logloss: 0.154804\ttraining's amex_metric: 0.907893\tvalid_1's binary_logloss: 0.215542\tvalid_1's amex_metric: 0.793104\n",
      "[7000]\ttraining's binary_logloss: 0.150394\ttraining's amex_metric: 0.91532\tvalid_1's binary_logloss: 0.215343\tvalid_1's amex_metric: 0.793754\n",
      "[7500]\ttraining's binary_logloss: 0.146346\ttraining's amex_metric: 0.922274\tvalid_1's binary_logloss: 0.215299\tvalid_1's amex_metric: 0.793595\n",
      "[8000]\ttraining's binary_logloss: 0.142147\ttraining's amex_metric: 0.928916\tvalid_1's binary_logloss: 0.215283\tvalid_1's amex_metric: 0.793591\n",
      "[8500]\ttraining's binary_logloss: 0.13841\ttraining's amex_metric: 0.934701\tvalid_1's binary_logloss: 0.215253\tvalid_1's amex_metric: 0.793664\n",
      "[9000]\ttraining's binary_logloss: 0.134936\ttraining's amex_metric: 0.940399\tvalid_1's binary_logloss: 0.215159\tvalid_1's amex_metric: 0.793677\n",
      "[9500]\ttraining's binary_logloss: 0.131134\ttraining's amex_metric: 0.945733\tvalid_1's binary_logloss: 0.21517\tvalid_1's amex_metric: 0.793696\n",
      "[10000]\ttraining's binary_logloss: 0.127709\ttraining's amex_metric: 0.950614\tvalid_1's binary_logloss: 0.215192\tvalid_1's amex_metric: 0.793632\n",
      "[10500]\ttraining's binary_logloss: 0.124109\ttraining's amex_metric: 0.955505\tvalid_1's binary_logloss: 0.215146\tvalid_1's amex_metric: 0.793423\n",
      "Our fold 0 CV score is 0.7934230365841033\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.216743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286831\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303846\ttraining's amex_metric: 0.780785\tvalid_1's binary_logloss: 0.306238\tvalid_1's amex_metric: 0.772124\n",
      "[1000]\ttraining's binary_logloss: 0.23933\ttraining's amex_metric: 0.795963\tvalid_1's binary_logloss: 0.24512\tvalid_1's amex_metric: 0.781424\n",
      "[1500]\ttraining's binary_logloss: 0.218829\ttraining's amex_metric: 0.809866\tvalid_1's binary_logloss: 0.228829\tvalid_1's amex_metric: 0.787989\n",
      "[2000]\ttraining's binary_logloss: 0.208952\ttraining's amex_metric: 0.821242\tvalid_1's binary_logloss: 0.223129\tvalid_1's amex_metric: 0.791539\n",
      "[2500]\ttraining's binary_logloss: 0.199374\ttraining's amex_metric: 0.831953\tvalid_1's binary_logloss: 0.218859\tvalid_1's amex_metric: 0.793026\n",
      "[3000]\ttraining's binary_logloss: 0.192896\ttraining's amex_metric: 0.842986\tvalid_1's binary_logloss: 0.217095\tvalid_1's amex_metric: 0.795725\n",
      "[3500]\ttraining's binary_logloss: 0.187008\ttraining's amex_metric: 0.852368\tvalid_1's binary_logloss: 0.215954\tvalid_1's amex_metric: 0.796382\n",
      "[4000]\ttraining's binary_logloss: 0.180794\ttraining's amex_metric: 0.862962\tvalid_1's binary_logloss: 0.214958\tvalid_1's amex_metric: 0.7969\n",
      "[4500]\ttraining's binary_logloss: 0.175899\ttraining's amex_metric: 0.871704\tvalid_1's binary_logloss: 0.21444\tvalid_1's amex_metric: 0.797901\n",
      "[5000]\ttraining's binary_logloss: 0.17021\ttraining's amex_metric: 0.88137\tvalid_1's binary_logloss: 0.213923\tvalid_1's amex_metric: 0.797746\n",
      "[5500]\ttraining's binary_logloss: 0.165214\ttraining's amex_metric: 0.88981\tvalid_1's binary_logloss: 0.21362\tvalid_1's amex_metric: 0.797652\n",
      "[6000]\ttraining's binary_logloss: 0.159975\ttraining's amex_metric: 0.8981\tvalid_1's binary_logloss: 0.213222\tvalid_1's amex_metric: 0.797483\n",
      "[6500]\ttraining's binary_logloss: 0.155352\ttraining's amex_metric: 0.906793\tvalid_1's binary_logloss: 0.212996\tvalid_1's amex_metric: 0.798349\n",
      "[7000]\ttraining's binary_logloss: 0.150918\ttraining's amex_metric: 0.914422\tvalid_1's binary_logloss: 0.212897\tvalid_1's amex_metric: 0.798189\n",
      "[7500]\ttraining's binary_logloss: 0.146849\ttraining's amex_metric: 0.921243\tvalid_1's binary_logloss: 0.212794\tvalid_1's amex_metric: 0.798795\n",
      "[8000]\ttraining's binary_logloss: 0.142656\ttraining's amex_metric: 0.927905\tvalid_1's binary_logloss: 0.212758\tvalid_1's amex_metric: 0.798283\n",
      "[8500]\ttraining's binary_logloss: 0.138899\ttraining's amex_metric: 0.934473\tvalid_1's binary_logloss: 0.212706\tvalid_1's amex_metric: 0.79882\n",
      "[9000]\ttraining's binary_logloss: 0.135462\ttraining's amex_metric: 0.940118\tvalid_1's binary_logloss: 0.212662\tvalid_1's amex_metric: 0.798746\n",
      "[9500]\ttraining's binary_logloss: 0.131667\ttraining's amex_metric: 0.945324\tvalid_1's binary_logloss: 0.212599\tvalid_1's amex_metric: 0.799239\n",
      "[10000]\ttraining's binary_logloss: 0.128252\ttraining's amex_metric: 0.950578\tvalid_1's binary_logloss: 0.212595\tvalid_1's amex_metric: 0.798269\n",
      "[10500]\ttraining's binary_logloss: 0.124627\ttraining's amex_metric: 0.955632\tvalid_1's binary_logloss: 0.212583\tvalid_1's amex_metric: 0.798269\n",
      "Our fold 1 CV score is 0.7982689093488936\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.114186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286706\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303159\ttraining's amex_metric: 0.781882\tvalid_1's binary_logloss: 0.307987\tvalid_1's amex_metric: 0.768084\n",
      "[1000]\ttraining's binary_logloss: 0.238621\ttraining's amex_metric: 0.797315\tvalid_1's binary_logloss: 0.247373\tvalid_1's amex_metric: 0.775311\n",
      "[1500]\ttraining's binary_logloss: 0.218134\ttraining's amex_metric: 0.81082\tvalid_1's binary_logloss: 0.231096\tvalid_1's amex_metric: 0.782617\n",
      "[2000]\ttraining's binary_logloss: 0.208306\ttraining's amex_metric: 0.822192\tvalid_1's binary_logloss: 0.225359\tvalid_1's amex_metric: 0.787261\n",
      "[2500]\ttraining's binary_logloss: 0.198742\ttraining's amex_metric: 0.833598\tvalid_1's binary_logloss: 0.221052\tvalid_1's amex_metric: 0.790267\n",
      "[3000]\ttraining's binary_logloss: 0.192334\ttraining's amex_metric: 0.844274\tvalid_1's binary_logloss: 0.219369\tvalid_1's amex_metric: 0.791743\n",
      "[3500]\ttraining's binary_logloss: 0.186477\ttraining's amex_metric: 0.853859\tvalid_1's binary_logloss: 0.218241\tvalid_1's amex_metric: 0.791995\n",
      "[4000]\ttraining's binary_logloss: 0.180266\ttraining's amex_metric: 0.864656\tvalid_1's binary_logloss: 0.217214\tvalid_1's amex_metric: 0.793025\n",
      "[4500]\ttraining's binary_logloss: 0.175374\ttraining's amex_metric: 0.872991\tvalid_1's binary_logloss: 0.216729\tvalid_1's amex_metric: 0.793763\n",
      "[5000]\ttraining's binary_logloss: 0.169698\ttraining's amex_metric: 0.882395\tvalid_1's binary_logloss: 0.216174\tvalid_1's amex_metric: 0.794839\n",
      "[5500]\ttraining's binary_logloss: 0.164702\ttraining's amex_metric: 0.890997\tvalid_1's binary_logloss: 0.215889\tvalid_1's amex_metric: 0.794556\n",
      "[6000]\ttraining's binary_logloss: 0.159452\ttraining's amex_metric: 0.8991\tvalid_1's binary_logloss: 0.215572\tvalid_1's amex_metric: 0.794261\n",
      "[6500]\ttraining's binary_logloss: 0.154822\ttraining's amex_metric: 0.907415\tvalid_1's binary_logloss: 0.215381\tvalid_1's amex_metric: 0.794868\n",
      "[7000]\ttraining's binary_logloss: 0.150424\ttraining's amex_metric: 0.915163\tvalid_1's binary_logloss: 0.21522\tvalid_1's amex_metric: 0.795563\n",
      "[7500]\ttraining's binary_logloss: 0.146404\ttraining's amex_metric: 0.921521\tvalid_1's binary_logloss: 0.215124\tvalid_1's amex_metric: 0.795274\n",
      "[8000]\ttraining's binary_logloss: 0.142204\ttraining's amex_metric: 0.928221\tvalid_1's binary_logloss: 0.215025\tvalid_1's amex_metric: 0.795459\n",
      "[8500]\ttraining's binary_logloss: 0.138433\ttraining's amex_metric: 0.934421\tvalid_1's binary_logloss: 0.214948\tvalid_1's amex_metric: 0.795507\n",
      "[9000]\ttraining's binary_logloss: 0.134977\ttraining's amex_metric: 0.940026\tvalid_1's binary_logloss: 0.214888\tvalid_1's amex_metric: 0.795355\n",
      "[9500]\ttraining's binary_logloss: 0.131202\ttraining's amex_metric: 0.945248\tvalid_1's binary_logloss: 0.214778\tvalid_1's amex_metric: 0.795072\n",
      "[10000]\ttraining's binary_logloss: 0.127781\ttraining's amex_metric: 0.950641\tvalid_1's binary_logloss: 0.214687\tvalid_1's amex_metric: 0.795923\n",
      "[10500]\ttraining's binary_logloss: 0.124183\ttraining's amex_metric: 0.955567\tvalid_1's binary_logloss: 0.214631\tvalid_1's amex_metric: 0.795839\n",
      "Our fold 2 CV score is 0.7958393968551862\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.018307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286735\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.302954\ttraining's amex_metric: 0.781854\tvalid_1's binary_logloss: 0.30818\tvalid_1's amex_metric: 0.7678\n",
      "[1000]\ttraining's binary_logloss: 0.238431\ttraining's amex_metric: 0.797169\tvalid_1's binary_logloss: 0.248237\tvalid_1's amex_metric: 0.775132\n",
      "[1500]\ttraining's binary_logloss: 0.217801\ttraining's amex_metric: 0.811689\tvalid_1's binary_logloss: 0.232232\tvalid_1's amex_metric: 0.781515\n",
      "[2000]\ttraining's binary_logloss: 0.207885\ttraining's amex_metric: 0.822691\tvalid_1's binary_logloss: 0.226637\tvalid_1's amex_metric: 0.784754\n",
      "[2500]\ttraining's binary_logloss: 0.19825\ttraining's amex_metric: 0.834259\tvalid_1's binary_logloss: 0.222623\tvalid_1's amex_metric: 0.786398\n",
      "[3000]\ttraining's binary_logloss: 0.19183\ttraining's amex_metric: 0.845095\tvalid_1's binary_logloss: 0.221042\tvalid_1's amex_metric: 0.788592\n",
      "[3500]\ttraining's binary_logloss: 0.185944\ttraining's amex_metric: 0.854901\tvalid_1's binary_logloss: 0.220008\tvalid_1's amex_metric: 0.789751\n",
      "[4000]\ttraining's binary_logloss: 0.179692\ttraining's amex_metric: 0.865172\tvalid_1's binary_logloss: 0.219203\tvalid_1's amex_metric: 0.791034\n",
      "[4500]\ttraining's binary_logloss: 0.174809\ttraining's amex_metric: 0.873683\tvalid_1's binary_logloss: 0.218767\tvalid_1's amex_metric: 0.790967\n",
      "[5000]\ttraining's binary_logloss: 0.169122\ttraining's amex_metric: 0.883258\tvalid_1's binary_logloss: 0.218325\tvalid_1's amex_metric: 0.79176\n",
      "[5500]\ttraining's binary_logloss: 0.16413\ttraining's amex_metric: 0.891943\tvalid_1's binary_logloss: 0.218003\tvalid_1's amex_metric: 0.791936\n",
      "[6000]\ttraining's binary_logloss: 0.158906\ttraining's amex_metric: 0.900413\tvalid_1's binary_logloss: 0.217689\tvalid_1's amex_metric: 0.792664\n",
      "[6500]\ttraining's binary_logloss: 0.154267\ttraining's amex_metric: 0.908646\tvalid_1's binary_logloss: 0.217504\tvalid_1's amex_metric: 0.792934\n",
      "[7000]\ttraining's binary_logloss: 0.14987\ttraining's amex_metric: 0.916087\tvalid_1's binary_logloss: 0.217404\tvalid_1's amex_metric: 0.792201\n",
      "[7500]\ttraining's binary_logloss: 0.145828\ttraining's amex_metric: 0.922601\tvalid_1's binary_logloss: 0.217331\tvalid_1's amex_metric: 0.792005\n",
      "[8000]\ttraining's binary_logloss: 0.141644\ttraining's amex_metric: 0.929685\tvalid_1's binary_logloss: 0.217236\tvalid_1's amex_metric: 0.792876\n",
      "[8500]\ttraining's binary_logloss: 0.137905\ttraining's amex_metric: 0.93582\tvalid_1's binary_logloss: 0.217196\tvalid_1's amex_metric: 0.792826\n",
      "[9000]\ttraining's binary_logloss: 0.134457\ttraining's amex_metric: 0.941459\tvalid_1's binary_logloss: 0.217114\tvalid_1's amex_metric: 0.793108\n",
      "[9500]\ttraining's binary_logloss: 0.130675\ttraining's amex_metric: 0.946271\tvalid_1's binary_logloss: 0.217155\tvalid_1's amex_metric: 0.793814\n",
      "[10000]\ttraining's binary_logloss: 0.127249\ttraining's amex_metric: 0.95132\tvalid_1's binary_logloss: 0.217165\tvalid_1's amex_metric: 0.794446\n",
      "[10500]\ttraining's binary_logloss: 0.123636\ttraining's amex_metric: 0.956226\tvalid_1's binary_logloss: 0.217233\tvalid_1's amex_metric: 0.794392\n",
      "Our fold 3 CV score is 0.7943917846908378\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1823 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.028612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 286766\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.303111\ttraining's amex_metric: 0.781713\tvalid_1's binary_logloss: 0.308185\tvalid_1's amex_metric: 0.768097\n",
      "[1000]\ttraining's binary_logloss: 0.238579\ttraining's amex_metric: 0.796954\tvalid_1's binary_logloss: 0.247789\tvalid_1's amex_metric: 0.77595\n",
      "[1500]\ttraining's binary_logloss: 0.218096\ttraining's amex_metric: 0.811101\tvalid_1's binary_logloss: 0.231385\tvalid_1's amex_metric: 0.784059\n",
      "[2000]\ttraining's binary_logloss: 0.208283\ttraining's amex_metric: 0.821793\tvalid_1's binary_logloss: 0.2257\tvalid_1's amex_metric: 0.787528\n",
      "[2500]\ttraining's binary_logloss: 0.198731\ttraining's amex_metric: 0.83346\tvalid_1's binary_logloss: 0.221468\tvalid_1's amex_metric: 0.790554\n",
      "[3000]\ttraining's binary_logloss: 0.192315\ttraining's amex_metric: 0.844276\tvalid_1's binary_logloss: 0.219819\tvalid_1's amex_metric: 0.792978\n",
      "[3500]\ttraining's binary_logloss: 0.186393\ttraining's amex_metric: 0.85353\tvalid_1's binary_logloss: 0.218612\tvalid_1's amex_metric: 0.793825\n",
      "[4000]\ttraining's binary_logloss: 0.180185\ttraining's amex_metric: 0.863731\tvalid_1's binary_logloss: 0.217547\tvalid_1's amex_metric: 0.795682\n",
      "[4500]\ttraining's binary_logloss: 0.175301\ttraining's amex_metric: 0.872451\tvalid_1's binary_logloss: 0.217035\tvalid_1's amex_metric: 0.796394\n",
      "[5000]\ttraining's binary_logloss: 0.169598\ttraining's amex_metric: 0.88194\tvalid_1's binary_logloss: 0.216386\tvalid_1's amex_metric: 0.796604\n",
      "[5500]\ttraining's binary_logloss: 0.164634\ttraining's amex_metric: 0.890962\tvalid_1's binary_logloss: 0.216134\tvalid_1's amex_metric: 0.797132\n",
      "[6000]\ttraining's binary_logloss: 0.159414\ttraining's amex_metric: 0.899509\tvalid_1's binary_logloss: 0.21582\tvalid_1's amex_metric: 0.797613\n",
      "[6500]\ttraining's binary_logloss: 0.154775\ttraining's amex_metric: 0.908153\tvalid_1's binary_logloss: 0.215655\tvalid_1's amex_metric: 0.797854\n",
      "[7000]\ttraining's binary_logloss: 0.150389\ttraining's amex_metric: 0.915515\tvalid_1's binary_logloss: 0.215495\tvalid_1's amex_metric: 0.797771\n",
      "[7500]\ttraining's binary_logloss: 0.146338\ttraining's amex_metric: 0.922356\tvalid_1's binary_logloss: 0.215406\tvalid_1's amex_metric: 0.798315\n",
      "[8000]\ttraining's binary_logloss: 0.142144\ttraining's amex_metric: 0.92877\tvalid_1's binary_logloss: 0.215264\tvalid_1's amex_metric: 0.79804\n",
      "[8500]\ttraining's binary_logloss: 0.138391\ttraining's amex_metric: 0.934812\tvalid_1's binary_logloss: 0.215212\tvalid_1's amex_metric: 0.797569\n",
      "[9000]\ttraining's binary_logloss: 0.134941\ttraining's amex_metric: 0.940434\tvalid_1's binary_logloss: 0.215147\tvalid_1's amex_metric: 0.797613\n",
      "[9500]\ttraining's binary_logloss: 0.131137\ttraining's amex_metric: 0.945801\tvalid_1's binary_logloss: 0.215058\tvalid_1's amex_metric: 0.797598\n",
      "[10000]\ttraining's binary_logloss: 0.127717\ttraining's amex_metric: 0.950988\tvalid_1's binary_logloss: 0.215098\tvalid_1's amex_metric: 0.797757\n",
      "[10500]\ttraining's binary_logloss: 0.124096\ttraining's amex_metric: 0.955771\tvalid_1's binary_logloss: 0.21511\tvalid_1's amex_metric: 0.797507\n",
      "Our fold 4 CV score is 0.7975068272698843\n",
      "Our out of folds CV score is 0.795879634981765\n"
     ]
    }
   ],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,#10500\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_{len(features)}feat_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35c890-5ee8-4931-ab88-c3d4c71bfb08",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff553086-fa26-43e7-9335-ecbb26aa4796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d0ca2-5123-40c8-b701-51b6e572f49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9878e90-7da9-4122-ac37-1473c2355c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69cc32-8cff-4af3-a328-c0709904fc94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f72f3-df31-47c5-888f-9d1e4ef57ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
