{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diffのXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
    "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
    "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    train.to_pickle('train_fe_v3_loaded.pkl')\n",
    "    \n",
    "    del train_num_agg, train_cat_agg, train_diff, train\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    test = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    # test = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
    "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
    "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    \n",
    "    test.to_pickle('test_fe_v3_loaded.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a3cd6b-3b61-4ffe-97a6-09b11469f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73091e4f-b894-4ade-84f4-096c9c8fcb5a",
   "metadata": {},
   "source": [
    "# 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717b3c71-f7c0-4fdf-b27c-3e29b6cfa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_parquet('../feature/Bruteforce feature/train_fe_v3_loaded.parquet')\n",
    "# test = pd.read_parquet('../feature/Bruteforce feature/test_fe_v3_loaded.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a06a1c6-a4e6-4179-837e-8ecdd0a9a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 88\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    input_dir = '../input/amex-fe/'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_v3_loaded.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_v3_loaded.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b48aef-8cf8-4e27-8122-c17fdd55616f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/amex-fe/train_fe_v3_loaded.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27692\\2817187628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27692\\4265619844.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train_fe_v3_loaded.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'test_fe_v3_loaded.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/amex-fe/train_fe_v3_loaded.parquet'"
     ]
    }
   ],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp59_xgb_lagdiff_c3_brustforce/'\n",
    "    seed = 614\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"xgb\"\n",
    "    ver = \"exp59\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 1639)\n",
      "(924621, 1638)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "train = pd.read_pickle('../feature/Bruteforce feature/train_fe_v3_loaded.pkl')\n",
    "test = pd.read_pickle('../feature/Bruteforce feature/test_fe_v3_loaded.pkl')\n",
    "\n",
    "train.fillna(value=0, inplace=True)\n",
    "test.fillna(value=0, inplace=True)\n",
    "\n",
    "## infを含むデータを外れ値（－１００００）に置換\n",
    "train = train.replace([np.inf, -np.inf],100000000000)\n",
    "test = test.replace([np.inf, -np.inf],100000000000)\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1637 features...\n",
      "[19:58:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67371\ttrain-amex:0.70544\tvalid-logloss:0.67381\tvalid-amex:0.69920\n",
      "[100]\ttrain-logloss:0.24150\ttrain-amex:0.78125\tvalid-logloss:0.24807\tvalid-amex:0.77072\n",
      "[200]\ttrain-logloss:0.21599\ttrain-amex:0.79911\tvalid-logloss:0.22697\tvalid-amex:0.78106\n",
      "[300]\ttrain-logloss:0.20668\ttrain-amex:0.81236\tvalid-logloss:0.22221\tvalid-amex:0.78721\n",
      "[400]\ttrain-logloss:0.20026\ttrain-amex:0.82259\tvalid-logloss:0.22002\tvalid-amex:0.79006\n",
      "[500]\ttrain-logloss:0.19507\ttrain-amex:0.83155\tvalid-logloss:0.21871\tvalid-amex:0.79059\n",
      "[600]\ttrain-logloss:0.19056\ttrain-amex:0.83915\tvalid-logloss:0.21794\tvalid-amex:0.79122\n",
      "[700]\ttrain-logloss:0.18658\ttrain-amex:0.84588\tvalid-logloss:0.21735\tvalid-amex:0.79256\n",
      "[800]\ttrain-logloss:0.18275\ttrain-amex:0.85279\tvalid-logloss:0.21692\tvalid-amex:0.79354\n",
      "[900]\ttrain-logloss:0.17893\ttrain-amex:0.85978\tvalid-logloss:0.21660\tvalid-amex:0.79401\n",
      "[1000]\ttrain-logloss:0.17529\ttrain-amex:0.86614\tvalid-logloss:0.21635\tvalid-amex:0.79403\n",
      "[1100]\ttrain-logloss:0.17177\ttrain-amex:0.87245\tvalid-logloss:0.21618\tvalid-amex:0.79451\n",
      "[1200]\ttrain-logloss:0.16846\ttrain-amex:0.87885\tvalid-logloss:0.21600\tvalid-amex:0.79424\n",
      "[1300]\ttrain-logloss:0.16513\ttrain-amex:0.88464\tvalid-logloss:0.21588\tvalid-amex:0.79429\n",
      "[1400]\ttrain-logloss:0.16208\ttrain-amex:0.88985\tvalid-logloss:0.21578\tvalid-amex:0.79468\n",
      "[1500]\ttrain-logloss:0.15884\ttrain-amex:0.89571\tvalid-logloss:0.21568\tvalid-amex:0.79438\n",
      "[1600]\ttrain-logloss:0.15584\ttrain-amex:0.90105\tvalid-logloss:0.21563\tvalid-amex:0.79420\n",
      "[1700]\ttrain-logloss:0.15286\ttrain-amex:0.90584\tvalid-logloss:0.21559\tvalid-amex:0.79468\n",
      "[1800]\ttrain-logloss:0.15012\ttrain-amex:0.91067\tvalid-logloss:0.21554\tvalid-amex:0.79483\n",
      "[1900]\ttrain-logloss:0.14730\ttrain-amex:0.91523\tvalid-logloss:0.21553\tvalid-amex:0.79482\n",
      "[2000]\ttrain-logloss:0.14466\ttrain-amex:0.91973\tvalid-logloss:0.21554\tvalid-amex:0.79532\n",
      "[2100]\ttrain-logloss:0.14186\ttrain-amex:0.92416\tvalid-logloss:0.21554\tvalid-amex:0.79548\n",
      "[2200]\ttrain-logloss:0.13910\ttrain-amex:0.92897\tvalid-logloss:0.21552\tvalid-amex:0.79535\n",
      "[2300]\ttrain-logloss:0.13669\ttrain-amex:0.93280\tvalid-logloss:0.21554\tvalid-amex:0.79526\n",
      "[2400]\ttrain-logloss:0.13429\ttrain-amex:0.93663\tvalid-logloss:0.21554\tvalid-amex:0.79563\n",
      "[2500]\ttrain-logloss:0.13176\ttrain-amex:0.94052\tvalid-logloss:0.21557\tvalid-amex:0.79593\n",
      "[2600]\ttrain-logloss:0.12929\ttrain-amex:0.94412\tvalid-logloss:0.21563\tvalid-amex:0.79524\n",
      "[2700]\ttrain-logloss:0.12698\ttrain-amex:0.94728\tvalid-logloss:0.21569\tvalid-amex:0.79480\n",
      "[2800]\ttrain-logloss:0.12462\ttrain-amex:0.95028\tvalid-logloss:0.21571\tvalid-amex:0.79432\n",
      "[2900]\ttrain-logloss:0.12239\ttrain-amex:0.95338\tvalid-logloss:0.21574\tvalid-amex:0.79466\n",
      "[2965]\ttrain-logloss:0.12094\ttrain-amex:0.95535\tvalid-logloss:0.21577\tvalid-amex:0.79455\n",
      "Our fold 0 CV score is 0.7943132860220659\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1637 features...\n",
      "[20:55:32] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67369\ttrain-amex:0.70886\tvalid-logloss:0.67384\tvalid-amex:0.69807\n",
      "[100]\ttrain-logloss:0.24126\ttrain-amex:0.78149\tvalid-logloss:0.24937\tvalid-amex:0.76645\n",
      "[200]\ttrain-logloss:0.21562\ttrain-amex:0.79993\tvalid-logloss:0.22851\tvalid-amex:0.77808\n",
      "[300]\ttrain-logloss:0.20634\ttrain-amex:0.81282\tvalid-logloss:0.22382\tvalid-amex:0.78522\n",
      "[400]\ttrain-logloss:0.19998\ttrain-amex:0.82287\tvalid-logloss:0.22169\tvalid-amex:0.78776\n",
      "[500]\ttrain-logloss:0.19498\ttrain-amex:0.83081\tvalid-logloss:0.22046\tvalid-amex:0.78983\n",
      "[600]\ttrain-logloss:0.19035\ttrain-amex:0.83862\tvalid-logloss:0.21968\tvalid-amex:0.79068\n",
      "[700]\ttrain-logloss:0.18626\ttrain-amex:0.84575\tvalid-logloss:0.21911\tvalid-amex:0.79113\n",
      "[800]\ttrain-logloss:0.18222\ttrain-amex:0.85292\tvalid-logloss:0.21875\tvalid-amex:0.79204\n",
      "[900]\ttrain-logloss:0.17861\ttrain-amex:0.85986\tvalid-logloss:0.21855\tvalid-amex:0.79170\n",
      "[1000]\ttrain-logloss:0.17506\ttrain-amex:0.86614\tvalid-logloss:0.21834\tvalid-amex:0.79221\n",
      "[1100]\ttrain-logloss:0.17157\ttrain-amex:0.87234\tvalid-logloss:0.21813\tvalid-amex:0.79318\n",
      "[1200]\ttrain-logloss:0.16832\ttrain-amex:0.87811\tvalid-logloss:0.21801\tvalid-amex:0.79346\n",
      "[1300]\ttrain-logloss:0.16505\ttrain-amex:0.88400\tvalid-logloss:0.21792\tvalid-amex:0.79375\n",
      "[1400]\ttrain-logloss:0.16187\ttrain-amex:0.88994\tvalid-logloss:0.21786\tvalid-amex:0.79352\n",
      "[1500]\ttrain-logloss:0.15878\ttrain-amex:0.89565\tvalid-logloss:0.21781\tvalid-amex:0.79331\n",
      "[1600]\ttrain-logloss:0.15579\ttrain-amex:0.90068\tvalid-logloss:0.21781\tvalid-amex:0.79399\n",
      "[1700]\ttrain-logloss:0.15305\ttrain-amex:0.90544\tvalid-logloss:0.21776\tvalid-amex:0.79391\n",
      "[1800]\ttrain-logloss:0.15005\ttrain-amex:0.91046\tvalid-logloss:0.21775\tvalid-amex:0.79422\n",
      "[1900]\ttrain-logloss:0.14716\ttrain-amex:0.91544\tvalid-logloss:0.21773\tvalid-amex:0.79425\n",
      "[2000]\ttrain-logloss:0.14447\ttrain-amex:0.91997\tvalid-logloss:0.21776\tvalid-amex:0.79453\n",
      "[2100]\ttrain-logloss:0.14195\ttrain-amex:0.92436\tvalid-logloss:0.21774\tvalid-amex:0.79444\n",
      "[2200]\ttrain-logloss:0.13912\ttrain-amex:0.92862\tvalid-logloss:0.21777\tvalid-amex:0.79439\n",
      "[2300]\ttrain-logloss:0.13655\ttrain-amex:0.93272\tvalid-logloss:0.21783\tvalid-amex:0.79449\n",
      "[2400]\ttrain-logloss:0.13408\ttrain-amex:0.93659\tvalid-logloss:0.21783\tvalid-amex:0.79442\n",
      "[2445]\ttrain-logloss:0.13310\ttrain-amex:0.93825\tvalid-logloss:0.21785\tvalid-amex:0.79416\n",
      "Our fold 1 CV score is 0.7939227360153578\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1637 features...\n",
      "[21:43:56] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67364\ttrain-amex:0.70717\tvalid-logloss:0.67382\tvalid-amex:0.69920\n",
      "[100]\ttrain-logloss:0.24134\ttrain-amex:0.78196\tvalid-logloss:0.24875\tvalid-amex:0.76764\n",
      "[200]\ttrain-logloss:0.21574\ttrain-amex:0.79994\tvalid-logloss:0.22823\tvalid-amex:0.77875\n",
      "[300]\ttrain-logloss:0.20643\ttrain-amex:0.81334\tvalid-logloss:0.22373\tvalid-amex:0.78268\n",
      "[400]\ttrain-logloss:0.20006\ttrain-amex:0.82354\tvalid-logloss:0.22151\tvalid-amex:0.78563\n",
      "[500]\ttrain-logloss:0.19475\ttrain-amex:0.83211\tvalid-logloss:0.22029\tvalid-amex:0.78664\n",
      "[600]\ttrain-logloss:0.19026\ttrain-amex:0.83964\tvalid-logloss:0.21948\tvalid-amex:0.78764\n",
      "[700]\ttrain-logloss:0.18606\ttrain-amex:0.84695\tvalid-logloss:0.21894\tvalid-amex:0.78873\n",
      "[800]\ttrain-logloss:0.18211\ttrain-amex:0.85389\tvalid-logloss:0.21852\tvalid-amex:0.78984\n",
      "[900]\ttrain-logloss:0.17841\ttrain-amex:0.86019\tvalid-logloss:0.21833\tvalid-amex:0.79031\n",
      "[1000]\ttrain-logloss:0.17487\ttrain-amex:0.86667\tvalid-logloss:0.21809\tvalid-amex:0.79045\n",
      "[1100]\ttrain-logloss:0.17128\ttrain-amex:0.87325\tvalid-logloss:0.21791\tvalid-amex:0.79034\n",
      "[1200]\ttrain-logloss:0.16781\ttrain-amex:0.87972\tvalid-logloss:0.21774\tvalid-amex:0.79142\n",
      "[1300]\ttrain-logloss:0.16454\ttrain-amex:0.88556\tvalid-logloss:0.21770\tvalid-amex:0.79153\n",
      "[1400]\ttrain-logloss:0.16148\ttrain-amex:0.89123\tvalid-logloss:0.21762\tvalid-amex:0.79133\n",
      "[1500]\ttrain-logloss:0.15841\ttrain-amex:0.89663\tvalid-logloss:0.21754\tvalid-amex:0.79193\n",
      "[1600]\ttrain-logloss:0.15525\ttrain-amex:0.90196\tvalid-logloss:0.21753\tvalid-amex:0.79188\n",
      "[1700]\ttrain-logloss:0.15238\ttrain-amex:0.90675\tvalid-logloss:0.21756\tvalid-amex:0.79125\n",
      "[1800]\ttrain-logloss:0.14946\ttrain-amex:0.91196\tvalid-logloss:0.21757\tvalid-amex:0.79148\n",
      "[1900]\ttrain-logloss:0.14666\ttrain-amex:0.91656\tvalid-logloss:0.21756\tvalid-amex:0.79164\n",
      "[2000]\ttrain-logloss:0.14385\ttrain-amex:0.92129\tvalid-logloss:0.21760\tvalid-amex:0.79173\n",
      "[2100]\ttrain-logloss:0.14113\ttrain-amex:0.92584\tvalid-logloss:0.21762\tvalid-amex:0.79106\n",
      "[2200]\ttrain-logloss:0.13856\ttrain-amex:0.92967\tvalid-logloss:0.21766\tvalid-amex:0.79128\n",
      "[2300]\ttrain-logloss:0.13593\ttrain-amex:0.93388\tvalid-logloss:0.21772\tvalid-amex:0.79096\n",
      "[2400]\ttrain-logloss:0.13347\ttrain-amex:0.93737\tvalid-logloss:0.21782\tvalid-amex:0.79127\n",
      "[2476]\ttrain-logloss:0.13171\ttrain-amex:0.93990\tvalid-logloss:0.21784\tvalid-amex:0.79090\n",
      "Our fold 2 CV score is 0.790765625436725\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1637 features...\n",
      "[22:36:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67371\ttrain-amex:0.70746\tvalid-logloss:0.67375\tvalid-amex:0.71000\n",
      "[100]\ttrain-logloss:0.24178\ttrain-amex:0.78132\tvalid-logloss:0.24657\tvalid-amex:0.77006\n",
      "[200]\ttrain-logloss:0.21621\ttrain-amex:0.79900\tvalid-logloss:0.22566\tvalid-amex:0.78316\n",
      "[300]\ttrain-logloss:0.20695\ttrain-amex:0.81244\tvalid-logloss:0.22096\tvalid-amex:0.78958\n",
      "[400]\ttrain-logloss:0.20052\ttrain-amex:0.82215\tvalid-logloss:0.21873\tvalid-amex:0.79187\n",
      "[500]\ttrain-logloss:0.19507\ttrain-amex:0.83113\tvalid-logloss:0.21750\tvalid-amex:0.79344\n",
      "[600]\ttrain-logloss:0.19053\ttrain-amex:0.83892\tvalid-logloss:0.21674\tvalid-amex:0.79513\n",
      "[700]\ttrain-logloss:0.18642\ttrain-amex:0.84595\tvalid-logloss:0.21620\tvalid-amex:0.79587\n",
      "[800]\ttrain-logloss:0.18253\ttrain-amex:0.85271\tvalid-logloss:0.21583\tvalid-amex:0.79627\n",
      "[900]\ttrain-logloss:0.17876\ttrain-amex:0.85925\tvalid-logloss:0.21553\tvalid-amex:0.79720\n",
      "[1000]\ttrain-logloss:0.17513\ttrain-amex:0.86621\tvalid-logloss:0.21531\tvalid-amex:0.79744\n",
      "[1100]\ttrain-logloss:0.17186\ttrain-amex:0.87176\tvalid-logloss:0.21515\tvalid-amex:0.79814\n",
      "[1200]\ttrain-logloss:0.16859\ttrain-amex:0.87809\tvalid-logloss:0.21506\tvalid-amex:0.79729\n",
      "[1300]\ttrain-logloss:0.16514\ttrain-amex:0.88388\tvalid-logloss:0.21498\tvalid-amex:0.79765\n",
      "[1400]\ttrain-logloss:0.16195\ttrain-amex:0.88978\tvalid-logloss:0.21484\tvalid-amex:0.79766\n",
      "[1500]\ttrain-logloss:0.15906\ttrain-amex:0.89468\tvalid-logloss:0.21477\tvalid-amex:0.79809\n",
      "[1570]\ttrain-logloss:0.15711\ttrain-amex:0.89835\tvalid-logloss:0.21478\tvalid-amex:0.79754\n",
      "Our fold 3 CV score is 0.7973269949643349\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1637 features...\n",
      "[23:10:02] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67368\ttrain-amex:0.70544\tvalid-logloss:0.67384\tvalid-amex:0.69854\n",
      "[100]\ttrain-logloss:0.24164\ttrain-amex:0.78129\tvalid-logloss:0.24781\tvalid-amex:0.77021\n",
      "[200]\ttrain-logloss:0.21619\ttrain-amex:0.79850\tvalid-logloss:0.22665\tvalid-amex:0.78321\n",
      "[300]\ttrain-logloss:0.20696\ttrain-amex:0.81223\tvalid-logloss:0.22174\tvalid-amex:0.78752\n",
      "[400]\ttrain-logloss:0.20056\ttrain-amex:0.82240\tvalid-logloss:0.21951\tvalid-amex:0.79002\n",
      "[500]\ttrain-logloss:0.19538\ttrain-amex:0.83101\tvalid-logloss:0.21815\tvalid-amex:0.79151\n",
      "[600]\ttrain-logloss:0.19084\ttrain-amex:0.83930\tvalid-logloss:0.21729\tvalid-amex:0.79292\n",
      "[700]\ttrain-logloss:0.18666\ttrain-amex:0.84666\tvalid-logloss:0.21669\tvalid-amex:0.79453\n",
      "[800]\ttrain-logloss:0.18296\ttrain-amex:0.85277\tvalid-logloss:0.21624\tvalid-amex:0.79511\n",
      "[900]\ttrain-logloss:0.17905\ttrain-amex:0.85976\tvalid-logloss:0.21590\tvalid-amex:0.79548\n",
      "[1000]\ttrain-logloss:0.17540\ttrain-amex:0.86613\tvalid-logloss:0.21566\tvalid-amex:0.79594\n",
      "[1100]\ttrain-logloss:0.17191\ttrain-amex:0.87255\tvalid-logloss:0.21552\tvalid-amex:0.79662\n",
      "[1200]\ttrain-logloss:0.16846\ttrain-amex:0.87879\tvalid-logloss:0.21538\tvalid-amex:0.79667\n",
      "[1300]\ttrain-logloss:0.16504\ttrain-amex:0.88528\tvalid-logloss:0.21524\tvalid-amex:0.79602\n",
      "[1400]\ttrain-logloss:0.16182\ttrain-amex:0.89135\tvalid-logloss:0.21515\tvalid-amex:0.79614\n",
      "[1500]\ttrain-logloss:0.15886\ttrain-amex:0.89613\tvalid-logloss:0.21508\tvalid-amex:0.79762\n",
      "[1600]\ttrain-logloss:0.15592\ttrain-amex:0.90118\tvalid-logloss:0.21496\tvalid-amex:0.79743\n",
      "[1700]\ttrain-logloss:0.15298\ttrain-amex:0.90604\tvalid-logloss:0.21490\tvalid-amex:0.79760\n",
      "[1800]\ttrain-logloss:0.15004\ttrain-amex:0.91080\tvalid-logloss:0.21482\tvalid-amex:0.79786\n",
      "[1900]\ttrain-logloss:0.14739\ttrain-amex:0.91512\tvalid-logloss:0.21482\tvalid-amex:0.79696\n",
      "[2000]\ttrain-logloss:0.14463\ttrain-amex:0.91987\tvalid-logloss:0.21483\tvalid-amex:0.79696\n",
      "[2100]\ttrain-logloss:0.14197\ttrain-amex:0.92431\tvalid-logloss:0.21482\tvalid-amex:0.79674\n",
      "[2200]\ttrain-logloss:0.13936\ttrain-amex:0.92850\tvalid-logloss:0.21476\tvalid-amex:0.79746\n",
      "[2217]\ttrain-logloss:0.13883\ttrain-amex:0.92931\tvalid-logloss:0.21475\tvalid-amex:0.79738\n",
      "Our fold 4 CV score is 0.797191073729035\n",
      "Our out of folds CV score is 0.7947135225797426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "\n",
    "xgb_parameters={\n",
    "        'max_depth': 7,\n",
    "        'eta': 0.03,\n",
    "        'subsample': 0.88,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',#gpu_hist\n",
    "        # 'predictor': 'gpu_predictor',\n",
    "        'random_state': 42,\n",
    "        'gamma': 1.5,\n",
    "        'min_child_weight': 8,\n",
    "        'lambda': 70,\n",
    "    }\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "# test_predictions = np.zeros(len(train))\n",
    "test_predictions = []\n",
    "# Create a numpy array to store out of fonp.zeros(len(train))lds predictions\n",
    "oof_predictions = []\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(data=x_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(data=test[features])\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "  \n",
    "    model = xgb.train(\n",
    "            xgb_parameters,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=9999,#9999\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            early_stopping_rounds=500,\n",
    "            feval=xgb_amex,\n",
    "            maximize=True,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    # joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    \n",
    "    model.save_model(f'{CFG.output_dir}{CFG.model}_{CFG.ver}_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.xgb')\n",
    "    \n",
    "    # Predict validation\n",
    "    # val_pred = model.predict(x_val)\n",
    "    val_pred = model.predict(dvalid)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions.extend(val_pred)\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(dtest)\n",
    "    test_predictions.append(test_pred)# += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(tr_target, oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': np.mean(test_predictions, axis = 0)})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cat_features = [\n",
    "#     \"B_30\",\n",
    "#     \"B_38\",\n",
    "#     \"D_114\",\n",
    "#     \"D_116\",\n",
    "#     \"D_117\",\n",
    "#     \"D_120\",\n",
    "#     \"D_126\",\n",
    "#     \"D_63\",\n",
    "#     \"D_64\",\n",
    "#     \"D_66\",\n",
    "#     \"D_68\"\n",
    "# ]\n",
    "\n",
    "# # kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "# cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# # cat_features.extend(kmeans_list)\n",
    "\n",
    "# for cat_col in cat_features:\n",
    "# #     print(cat_col)\n",
    "#     encoder = LabelEncoder()\n",
    "#     train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "#     test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "# features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "# params = {\n",
    "#     'objective': 'binary',\n",
    "#     'metric': CFG.metric,\n",
    "#     'boosting': CFG.boosting_type,\n",
    "#     'seed': CFG.seed,\n",
    "#     'num_leaves': 100,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'feature_fraction': 0.20,\n",
    "#     'bagging_freq': 10,\n",
    "#     'bagging_fraction': 0.50,\n",
    "#     'n_jobs': -1,\n",
    "#     'lambda_l2': 2,\n",
    "#     'min_data_in_leaf': 40,\n",
    "#     }\n",
    "\n",
    "# # Create a numpy array to store test predictions\n",
    "# test_predictions = []\n",
    "# # Create a numpy array to store out of folds predictions\n",
    "# oof_predictions = []\n",
    "# cids = []\n",
    "# tr_target = []\n",
    "\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "# for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "#     print(' ')\n",
    "#     print('-'*50)\n",
    "#     print(f'Training fold {fold} with {len(features)} features...')\n",
    "#     x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "#     y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "#     lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "#     lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "    \n",
    "#     # model = lgb.LGBMClassifier()\n",
    "#     with open(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl', 'rb') as web:\n",
    "#         model = pickle.load(web)\n",
    "#     # model.load_model(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl')\n",
    "    \n",
    "#     print(\"pred test\")\n",
    "#     test_predictions.append(model.predict(test[features]))\n",
    "#     print(\"pred val\")\n",
    "#     val_pred = model.predict(x_val)\n",
    "#     oof_predictions.extend(val_pred)\n",
    "#     cids.extend(train[\"customer_ID\"].iloc[val_ind])\n",
    "#     tr_target.extend(y_val)\n",
    "    \n",
    "#     score = amex_metric(y_val, val_pred)\n",
    "#     print(f'Our fold {fold} CV score is {score}')\n",
    "#     del x_train, x_val, y_train, y_val,  lgb_train ,lgb_valid\n",
    "#     del model\n",
    "#     gc.collect()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4b85b-1c5c-4b4b-b3e7-dea2470726d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dic_oof = {\n",
    "#     \"customer_ID\":cids,\n",
    "#     \"target\":tr_target,\n",
    "#     f\"{CFG.ver}_{CFG.model}_oof\":oof_predictions\n",
    "# }\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame(dic_oof)\n",
    "# # oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "\n",
    "# # Compute out of folds metric\n",
    "# score = amex_metric(oof_df[\"target\"], oof_df[f\"{CFG.ver}_{CFG.model}_oof\"])\n",
    "# print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "# oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.mean(test_predictions,axis = 0)\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_pred})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b80690-de04-4f36-bc0c-6e9bc48f7f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
