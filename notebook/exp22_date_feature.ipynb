{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp22\n",
    "\n",
    "下記のリンクにある曜日などの特徴量を追加したモデル\n",
    "\n",
    "https://www.kaggle.com/code/takeshikobayashi/statement-dates-to-use-or-not-to-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -Uq upgini\n",
    "import pandas as pd, numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt, gc, os\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "# import cupy, cudf\n",
    "\n",
    "# RANDOM SEED\n",
    "SEED = 42\n",
    "# FILL NAN VALUE\n",
    "NAN_VALUE = -127\n",
    "\n",
    "def read_file2cudf(path = '', usecols = None):\n",
    "    # LOAD DATAFRAME\n",
    "    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n",
    "    else: df = cudf.read_parquet(path)\n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    df.S_2 = cudf.to_datetime( df.S_2 )\n",
    "    print('shape of data:', df.shape)\n",
    "    return df\n",
    "\n",
    "# CALCULATE SIZE OF EACH SEPARATE TEST PART\n",
    "def get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n",
    "    chunk = len(customers)//NUM_PARTS\n",
    "    if verbose != '':\n",
    "        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n",
    "        print(f'There will be {chunk} customers in each part (except the last part).')\n",
    "        print('Below are number of rows in each part:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_PARTS):\n",
    "        if k==NUM_PARTS-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': print( rows )\n",
    "    return rows,chunk\n",
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label()), True\n",
    "\n",
    "# code by @https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight * (1 / weight.sum())).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / n_pos\n",
    "\n",
    "    lorentz = (target * (1 / n_pos)).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# official metric\n",
    "import pandas as pd\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training feature engineer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5052\\3364924336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5052\\3364924336.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# Transform float64 columns to float32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_num_agg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_num_agg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'float64'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mtrain_num_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_num_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Transform int64 columns to int32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "#     train = pd.read_parquet('/content/data/train.parquet')\n",
    "    train = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "#     test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    train = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/exp22_fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/exp22_fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc74c6-a551-4352-bb8b-1a95ca42f0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05428e51-9246-482c-9457-0f0c0222854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp19_tsfresh_lgb/'\n",
    "    seed = 46\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp19\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e96cc-3556-4978-9d8b-c7148c48d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsfresh_trian = pd.read_pickle('../feature/exp18_tsfresh/train_partial_autocorrelation.pkl')\n",
    "tsfresh_test = pd.read_pickle('../feature/exp18_tsfresh/test_partial_autocorrelation.pkl')\n",
    "\n",
    "train = train.merge(tsfresh_trian,on = \"customer_ID\",how = \"left\")\n",
    "test = test.merge(tsfresh_test,on = \"customer_ID\",how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5aa37-ce2a-4fd7-a211-d4665fe081b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1796 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.110382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 314284\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.309902\ttraining's amex_metric: 0.780792\tvalid_1's binary_logloss: 0.312839\tvalid_1's amex_metric: 0.771189\n",
      "[1000]\ttraining's binary_logloss: 0.249389\ttraining's amex_metric: 0.794019\tvalid_1's binary_logloss: 0.255349\tvalid_1's amex_metric: 0.778577\n",
      "[1500]\ttraining's binary_logloss: 0.223394\ttraining's amex_metric: 0.807521\tvalid_1's binary_logloss: 0.233252\tvalid_1's amex_metric: 0.784032\n",
      "[2000]\ttraining's binary_logloss: 0.209649\ttraining's amex_metric: 0.820605\tvalid_1's binary_logloss: 0.22448\tvalid_1's amex_metric: 0.787254\n",
      "[2500]\ttraining's binary_logloss: 0.200763\ttraining's amex_metric: 0.832793\tvalid_1's binary_logloss: 0.22072\tvalid_1's amex_metric: 0.789228\n",
      "[3000]\ttraining's binary_logloss: 0.192678\ttraining's amex_metric: 0.845222\tvalid_1's binary_logloss: 0.218476\tvalid_1's amex_metric: 0.791944\n",
      "[3500]\ttraining's binary_logloss: 0.186493\ttraining's amex_metric: 0.85592\tvalid_1's binary_logloss: 0.217366\tvalid_1's amex_metric: 0.793275\n",
      "[4000]\ttraining's binary_logloss: 0.18048\ttraining's amex_metric: 0.866676\tvalid_1's binary_logloss: 0.21646\tvalid_1's amex_metric: 0.795142\n",
      "[4500]\ttraining's binary_logloss: 0.174128\ttraining's amex_metric: 0.876635\tvalid_1's binary_logloss: 0.2156\tvalid_1's amex_metric: 0.795481\n",
      "[5000]\ttraining's binary_logloss: 0.168684\ttraining's amex_metric: 0.885823\tvalid_1's binary_logloss: 0.215146\tvalid_1's amex_metric: 0.795805\n",
      "[5500]\ttraining's binary_logloss: 0.162952\ttraining's amex_metric: 0.895336\tvalid_1's binary_logloss: 0.214663\tvalid_1's amex_metric: 0.796519\n",
      "[6000]\ttraining's binary_logloss: 0.157966\ttraining's amex_metric: 0.904267\tvalid_1's binary_logloss: 0.214398\tvalid_1's amex_metric: 0.796705\n",
      "[6500]\ttraining's binary_logloss: 0.153563\ttraining's amex_metric: 0.912093\tvalid_1's binary_logloss: 0.214235\tvalid_1's amex_metric: 0.79669\n",
      "[7000]\ttraining's binary_logloss: 0.149042\ttraining's amex_metric: 0.919192\tvalid_1's binary_logloss: 0.213992\tvalid_1's amex_metric: 0.797338\n",
      "[7500]\ttraining's binary_logloss: 0.144793\ttraining's amex_metric: 0.925976\tvalid_1's binary_logloss: 0.213804\tvalid_1's amex_metric: 0.797089\n",
      "[8000]\ttraining's binary_logloss: 0.140362\ttraining's amex_metric: 0.932746\tvalid_1's binary_logloss: 0.213584\tvalid_1's amex_metric: 0.796612\n",
      "[8500]\ttraining's binary_logloss: 0.136516\ttraining's amex_metric: 0.938981\tvalid_1's binary_logloss: 0.213521\tvalid_1's amex_metric: 0.79602\n",
      "[9000]\ttraining's binary_logloss: 0.132697\ttraining's amex_metric: 0.944897\tvalid_1's binary_logloss: 0.213436\tvalid_1's amex_metric: 0.796731\n",
      "[9500]\ttraining's binary_logloss: 0.128852\ttraining's amex_metric: 0.950374\tvalid_1's binary_logloss: 0.21336\tvalid_1's amex_metric: 0.796814\n",
      "[10000]\ttraining's binary_logloss: 0.124934\ttraining's amex_metric: 0.956066\tvalid_1's binary_logloss: 0.213294\tvalid_1's amex_metric: 0.796875\n",
      "[10500]\ttraining's binary_logloss: 0.121458\ttraining's amex_metric: 0.960894\tvalid_1's binary_logloss: 0.213326\tvalid_1's amex_metric: 0.796988\n",
      "Our fold 0 CV score is 0.7969883338523035\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1796 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.377391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 314199\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.309469\ttraining's amex_metric: 0.780972\tvalid_1's binary_logloss: 0.314221\tvalid_1's amex_metric: 0.766455\n",
      "[1000]\ttraining's binary_logloss: 0.248787\ttraining's amex_metric: 0.794546\tvalid_1's binary_logloss: 0.257219\tvalid_1's amex_metric: 0.77634\n",
      "[1500]\ttraining's binary_logloss: 0.222802\ttraining's amex_metric: 0.807307\tvalid_1's binary_logloss: 0.235587\tvalid_1's amex_metric: 0.781833\n",
      "[2000]\ttraining's binary_logloss: 0.209026\ttraining's amex_metric: 0.820588\tvalid_1's binary_logloss: 0.226898\tvalid_1's amex_metric: 0.786901\n",
      "[2500]\ttraining's binary_logloss: 0.200138\ttraining's amex_metric: 0.832797\tvalid_1's binary_logloss: 0.22315\tvalid_1's amex_metric: 0.789015\n",
      "[3000]\ttraining's binary_logloss: 0.192019\ttraining's amex_metric: 0.844787\tvalid_1's binary_logloss: 0.220897\tvalid_1's amex_metric: 0.791259\n",
      "[3500]\ttraining's binary_logloss: 0.18582\ttraining's amex_metric: 0.855592\tvalid_1's binary_logloss: 0.219883\tvalid_1's amex_metric: 0.792282\n",
      "[4000]\ttraining's binary_logloss: 0.179835\ttraining's amex_metric: 0.865871\tvalid_1's binary_logloss: 0.219062\tvalid_1's amex_metric: 0.793119\n",
      "[4500]\ttraining's binary_logloss: 0.173492\ttraining's amex_metric: 0.876287\tvalid_1's binary_logloss: 0.218361\tvalid_1's amex_metric: 0.793413\n",
      "[5000]\ttraining's binary_logloss: 0.168037\ttraining's amex_metric: 0.885975\tvalid_1's binary_logloss: 0.217956\tvalid_1's amex_metric: 0.794023\n",
      "[5500]\ttraining's binary_logloss: 0.16233\ttraining's amex_metric: 0.895492\tvalid_1's binary_logloss: 0.217588\tvalid_1's amex_metric: 0.794893\n",
      "[6000]\ttraining's binary_logloss: 0.157386\ttraining's amex_metric: 0.904138\tvalid_1's binary_logloss: 0.217385\tvalid_1's amex_metric: 0.795114\n",
      "[6500]\ttraining's binary_logloss: 0.152962\ttraining's amex_metric: 0.912046\tvalid_1's binary_logloss: 0.217223\tvalid_1's amex_metric: 0.795077\n",
      "[7000]\ttraining's binary_logloss: 0.148456\ttraining's amex_metric: 0.919964\tvalid_1's binary_logloss: 0.21706\tvalid_1's amex_metric: 0.795095\n",
      "[7500]\ttraining's binary_logloss: 0.144185\ttraining's amex_metric: 0.92668\tvalid_1's binary_logloss: 0.216922\tvalid_1's amex_metric: 0.795631\n",
      "[8000]\ttraining's binary_logloss: 0.139762\ttraining's amex_metric: 0.933549\tvalid_1's binary_logloss: 0.216777\tvalid_1's amex_metric: 0.795711\n",
      "[8500]\ttraining's binary_logloss: 0.135897\ttraining's amex_metric: 0.939893\tvalid_1's binary_logloss: 0.216725\tvalid_1's amex_metric: 0.795449\n",
      "[9000]\ttraining's binary_logloss: 0.132087\ttraining's amex_metric: 0.945577\tvalid_1's binary_logloss: 0.216675\tvalid_1's amex_metric: 0.796265\n",
      "[9500]\ttraining's binary_logloss: 0.128215\ttraining's amex_metric: 0.951314\tvalid_1's binary_logloss: 0.216589\tvalid_1's amex_metric: 0.795792\n",
      "[10000]\ttraining's binary_logloss: 0.124275\ttraining's amex_metric: 0.956799\tvalid_1's binary_logloss: 0.216561\tvalid_1's amex_metric: 0.795635\n",
      "[10500]\ttraining's binary_logloss: 0.120798\ttraining's amex_metric: 0.961562\tvalid_1's binary_logloss: 0.216545\tvalid_1's amex_metric: 0.795476\n",
      "Our fold 1 CV score is 0.7954764983395108\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1796 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.247637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 314332\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.309523\ttraining's amex_metric: 0.780987\tvalid_1's binary_logloss: 0.313468\tvalid_1's amex_metric: 0.767808\n",
      "[1000]\ttraining's binary_logloss: 0.248889\ttraining's amex_metric: 0.795008\tvalid_1's binary_logloss: 0.25637\tvalid_1's amex_metric: 0.77447\n",
      "[1500]\ttraining's binary_logloss: 0.222862\ttraining's amex_metric: 0.808398\tvalid_1's binary_logloss: 0.234674\tvalid_1's amex_metric: 0.780593\n",
      "[2000]\ttraining's binary_logloss: 0.209074\ttraining's amex_metric: 0.820881\tvalid_1's binary_logloss: 0.226095\tvalid_1's amex_metric: 0.787317\n",
      "[2500]\ttraining's binary_logloss: 0.200212\ttraining's amex_metric: 0.832457\tvalid_1's binary_logloss: 0.222551\tvalid_1's amex_metric: 0.790187\n",
      "[3000]\ttraining's binary_logloss: 0.192102\ttraining's amex_metric: 0.84488\tvalid_1's binary_logloss: 0.220325\tvalid_1's amex_metric: 0.792101\n",
      "[3500]\ttraining's binary_logloss: 0.185896\ttraining's amex_metric: 0.855541\tvalid_1's binary_logloss: 0.219218\tvalid_1's amex_metric: 0.793435\n",
      "[4000]\ttraining's binary_logloss: 0.179956\ttraining's amex_metric: 0.865913\tvalid_1's binary_logloss: 0.218408\tvalid_1's amex_metric: 0.793736\n",
      "[4500]\ttraining's binary_logloss: 0.173619\ttraining's amex_metric: 0.876614\tvalid_1's binary_logloss: 0.217764\tvalid_1's amex_metric: 0.794117\n",
      "[5000]\ttraining's binary_logloss: 0.168153\ttraining's amex_metric: 0.886097\tvalid_1's binary_logloss: 0.217414\tvalid_1's amex_metric: 0.793984\n",
      "[5500]\ttraining's binary_logloss: 0.162431\ttraining's amex_metric: 0.895744\tvalid_1's binary_logloss: 0.217032\tvalid_1's amex_metric: 0.794369\n",
      "[6000]\ttraining's binary_logloss: 0.15744\ttraining's amex_metric: 0.904628\tvalid_1's binary_logloss: 0.21688\tvalid_1's amex_metric: 0.794512\n",
      "[6500]\ttraining's binary_logloss: 0.15302\ttraining's amex_metric: 0.912848\tvalid_1's binary_logloss: 0.216792\tvalid_1's amex_metric: 0.793748\n",
      "[7000]\ttraining's binary_logloss: 0.148514\ttraining's amex_metric: 0.920106\tvalid_1's binary_logloss: 0.216645\tvalid_1's amex_metric: 0.794872\n",
      "[7500]\ttraining's binary_logloss: 0.144292\ttraining's amex_metric: 0.926803\tvalid_1's binary_logloss: 0.216525\tvalid_1's amex_metric: 0.795127\n",
      "[8000]\ttraining's binary_logloss: 0.139865\ttraining's amex_metric: 0.93343\tvalid_1's binary_logloss: 0.216452\tvalid_1's amex_metric: 0.794591\n",
      "[8500]\ttraining's binary_logloss: 0.135981\ttraining's amex_metric: 0.939807\tvalid_1's binary_logloss: 0.216442\tvalid_1's amex_metric: 0.794734\n",
      "[9000]\ttraining's binary_logloss: 0.132132\ttraining's amex_metric: 0.945805\tvalid_1's binary_logloss: 0.216442\tvalid_1's amex_metric: 0.794478\n",
      "[9500]\ttraining's binary_logloss: 0.128288\ttraining's amex_metric: 0.951308\tvalid_1's binary_logloss: 0.21646\tvalid_1's amex_metric: 0.79432\n",
      "[10000]\ttraining's binary_logloss: 0.12437\ttraining's amex_metric: 0.956938\tvalid_1's binary_logloss: 0.216484\tvalid_1's amex_metric: 0.793627\n",
      "[10500]\ttraining's binary_logloss: 0.120901\ttraining's amex_metric: 0.961403\tvalid_1's binary_logloss: 0.216466\tvalid_1's amex_metric: 0.794168\n",
      "Our fold 2 CV score is 0.7941681892822336\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1796 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.023555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 314202\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.309689\ttraining's amex_metric: 0.781306\tvalid_1's binary_logloss: 0.313142\tvalid_1's amex_metric: 0.766604\n",
      "[1000]\ttraining's binary_logloss: 0.249174\ttraining's amex_metric: 0.795013\tvalid_1's binary_logloss: 0.255918\tvalid_1's amex_metric: 0.774495\n",
      "[1500]\ttraining's binary_logloss: 0.223092\ttraining's amex_metric: 0.80779\tvalid_1's binary_logloss: 0.234139\tvalid_1's amex_metric: 0.780478\n",
      "[2000]\ttraining's binary_logloss: 0.209306\ttraining's amex_metric: 0.8212\tvalid_1's binary_logloss: 0.225498\tvalid_1's amex_metric: 0.786024\n",
      "[2500]\ttraining's binary_logloss: 0.200407\ttraining's amex_metric: 0.832774\tvalid_1's binary_logloss: 0.221916\tvalid_1's amex_metric: 0.787546\n",
      "[3000]\ttraining's binary_logloss: 0.192354\ttraining's amex_metric: 0.844983\tvalid_1's binary_logloss: 0.219682\tvalid_1's amex_metric: 0.790261\n",
      "[3500]\ttraining's binary_logloss: 0.18614\ttraining's amex_metric: 0.855928\tvalid_1's binary_logloss: 0.218607\tvalid_1's amex_metric: 0.792538\n",
      "[4000]\ttraining's binary_logloss: 0.180156\ttraining's amex_metric: 0.86568\tvalid_1's binary_logloss: 0.217812\tvalid_1's amex_metric: 0.793354\n",
      "[4500]\ttraining's binary_logloss: 0.17381\ttraining's amex_metric: 0.876157\tvalid_1's binary_logloss: 0.2171\tvalid_1's amex_metric: 0.793581\n",
      "[5000]\ttraining's binary_logloss: 0.16834\ttraining's amex_metric: 0.88582\tvalid_1's binary_logloss: 0.216797\tvalid_1's amex_metric: 0.793907\n",
      "[5500]\ttraining's binary_logloss: 0.162636\ttraining's amex_metric: 0.895319\tvalid_1's binary_logloss: 0.216432\tvalid_1's amex_metric: 0.794709\n",
      "[6000]\ttraining's binary_logloss: 0.157671\ttraining's amex_metric: 0.90408\tvalid_1's binary_logloss: 0.216286\tvalid_1's amex_metric: 0.794825\n",
      "[6500]\ttraining's binary_logloss: 0.153278\ttraining's amex_metric: 0.912236\tvalid_1's binary_logloss: 0.21618\tvalid_1's amex_metric: 0.794877\n",
      "[7000]\ttraining's binary_logloss: 0.148734\ttraining's amex_metric: 0.919784\tvalid_1's binary_logloss: 0.216036\tvalid_1's amex_metric: 0.795492\n",
      "[7500]\ttraining's binary_logloss: 0.144451\ttraining's amex_metric: 0.926748\tvalid_1's binary_logloss: 0.215913\tvalid_1's amex_metric: 0.795245\n",
      "[8000]\ttraining's binary_logloss: 0.140021\ttraining's amex_metric: 0.932969\tvalid_1's binary_logloss: 0.215843\tvalid_1's amex_metric: 0.795494\n",
      "[8500]\ttraining's binary_logloss: 0.13616\ttraining's amex_metric: 0.93936\tvalid_1's binary_logloss: 0.215731\tvalid_1's amex_metric: 0.795719\n",
      "[9000]\ttraining's binary_logloss: 0.13229\ttraining's amex_metric: 0.944785\tvalid_1's binary_logloss: 0.215672\tvalid_1's amex_metric: 0.795677\n",
      "[9500]\ttraining's binary_logloss: 0.128407\ttraining's amex_metric: 0.950611\tvalid_1's binary_logloss: 0.215705\tvalid_1's amex_metric: 0.795056\n",
      "[10000]\ttraining's binary_logloss: 0.12448\ttraining's amex_metric: 0.956229\tvalid_1's binary_logloss: 0.215644\tvalid_1's amex_metric: 0.795182\n",
      "[10500]\ttraining's binary_logloss: 0.121026\ttraining's amex_metric: 0.961076\tvalid_1's binary_logloss: 0.215711\tvalid_1's amex_metric: 0.794601\n",
      "Our fold 3 CV score is 0.7946013318261542\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1796 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.102342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 314244\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.309367\ttraining's amex_metric: 0.781723\tvalid_1's binary_logloss: 0.314536\tvalid_1's amex_metric: 0.764806\n",
      "[1000]\ttraining's binary_logloss: 0.248899\ttraining's amex_metric: 0.795066\tvalid_1's binary_logloss: 0.257285\tvalid_1's amex_metric: 0.77183\n",
      "[1500]\ttraining's binary_logloss: 0.222995\ttraining's amex_metric: 0.808285\tvalid_1's binary_logloss: 0.235288\tvalid_1's amex_metric: 0.777884\n",
      "[2000]\ttraining's binary_logloss: 0.209285\ttraining's amex_metric: 0.821728\tvalid_1's binary_logloss: 0.226437\tvalid_1's amex_metric: 0.782452\n",
      "[2500]\ttraining's binary_logloss: 0.200387\ttraining's amex_metric: 0.833235\tvalid_1's binary_logloss: 0.222575\tvalid_1's amex_metric: 0.785997\n",
      "[3000]\ttraining's binary_logloss: 0.192303\ttraining's amex_metric: 0.845381\tvalid_1's binary_logloss: 0.220214\tvalid_1's amex_metric: 0.788468\n",
      "[3500]\ttraining's binary_logloss: 0.186027\ttraining's amex_metric: 0.855844\tvalid_1's binary_logloss: 0.218991\tvalid_1's amex_metric: 0.790014\n",
      "[4000]\ttraining's binary_logloss: 0.180067\ttraining's amex_metric: 0.866447\tvalid_1's binary_logloss: 0.21817\tvalid_1's amex_metric: 0.790669\n",
      "[4500]\ttraining's binary_logloss: 0.173745\ttraining's amex_metric: 0.876562\tvalid_1's binary_logloss: 0.217344\tvalid_1's amex_metric: 0.792193\n",
      "[5000]\ttraining's binary_logloss: 0.168293\ttraining's amex_metric: 0.886133\tvalid_1's binary_logloss: 0.216861\tvalid_1's amex_metric: 0.792121\n",
      "[5500]\ttraining's binary_logloss: 0.162553\ttraining's amex_metric: 0.895933\tvalid_1's binary_logloss: 0.216365\tvalid_1's amex_metric: 0.79323\n",
      "[6000]\ttraining's binary_logloss: 0.157618\ttraining's amex_metric: 0.904819\tvalid_1's binary_logloss: 0.21615\tvalid_1's amex_metric: 0.793607\n",
      "[6500]\ttraining's binary_logloss: 0.153243\ttraining's amex_metric: 0.912928\tvalid_1's binary_logloss: 0.215993\tvalid_1's amex_metric: 0.79384\n",
      "[7000]\ttraining's binary_logloss: 0.148731\ttraining's amex_metric: 0.920388\tvalid_1's binary_logloss: 0.21582\tvalid_1's amex_metric: 0.793306\n",
      "[7500]\ttraining's binary_logloss: 0.144481\ttraining's amex_metric: 0.926877\tvalid_1's binary_logloss: 0.215634\tvalid_1's amex_metric: 0.793898\n",
      "[8000]\ttraining's binary_logloss: 0.140043\ttraining's amex_metric: 0.933584\tvalid_1's binary_logloss: 0.215532\tvalid_1's amex_metric: 0.793952\n",
      "[8500]\ttraining's binary_logloss: 0.136187\ttraining's amex_metric: 0.939548\tvalid_1's binary_logloss: 0.215407\tvalid_1's amex_metric: 0.793631\n",
      "[9000]\ttraining's binary_logloss: 0.132367\ttraining's amex_metric: 0.945736\tvalid_1's binary_logloss: 0.215359\tvalid_1's amex_metric: 0.794276\n",
      "[9500]\ttraining's binary_logloss: 0.128511\ttraining's amex_metric: 0.951216\tvalid_1's binary_logloss: 0.215259\tvalid_1's amex_metric: 0.794475\n",
      "[10000]\ttraining's binary_logloss: 0.124578\ttraining's amex_metric: 0.956572\tvalid_1's binary_logloss: 0.215213\tvalid_1's amex_metric: 0.794384\n",
      "[10500]\ttraining's binary_logloss: 0.121132\ttraining's amex_metric: 0.961181\tvalid_1's binary_logloss: 0.215149\tvalid_1's amex_metric: 0.794999\n",
      "Our fold 4 CV score is 0.7949986645880672\n",
      "Our out of folds CV score is 0.79518510573899\n"
     ]
    }
   ],
   "source": [
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 10500,#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1500,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
