{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp15\n",
    "\n",
    "exp03 Tabnet entmax\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import joblib\n",
    "import random\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from itertools import combinations\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import snappy\n",
    "\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "# from ipywidgets import interact, Select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ====================================================\n",
    "# # Get the difference\n",
    "# # ====================================================\n",
    "# def get_difference(data, num_features):\n",
    "#     df1 = []\n",
    "#     customer_ids = []\n",
    "#     for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "#         # Get the differences\n",
    "#         diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "#         # Append to lists\n",
    "#         df1.append(diff_df1)\n",
    "#         customer_ids.append(customer_id)\n",
    "#     # Concatenate\n",
    "#     df1 = np.concatenate(df1, axis = 0)\n",
    "#     # Transform to dataframe\n",
    "#     df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "#     # Add customer id\n",
    "#     df1['customer_ID'] = customer_ids\n",
    "#     return df1\n",
    "\n",
    "# # ====================================================\n",
    "# # Read & preprocess data and save it to disk\n",
    "# # ====================================================\n",
    "# def read_preprocess_data():\n",
    "#     train = pd.read_parquet('/content/data/train.parquet')\n",
    "#     features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "#     cat_features = [\n",
    "#         \"B_30\",\n",
    "#         \"B_38\",\n",
    "#         \"D_114\",\n",
    "#         \"D_116\",\n",
    "#         \"D_117\",\n",
    "#         \"D_120\",\n",
    "#         \"D_126\",\n",
    "#         \"D_63\",\n",
    "#         \"D_64\",\n",
    "#         \"D_66\",\n",
    "#         \"D_68\",\n",
    "#     ]\n",
    "#     num_features = [col for col in features if col not in cat_features]\n",
    "#     print('Starting training feature engineer...')\n",
    "#     train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "#     train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "#     train_num_agg.reset_index(inplace = True)\n",
    "#     train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "#     train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "#     train_cat_agg.reset_index(inplace = True)\n",
    "#     train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "#     # Transform float64 columns to float32\n",
    "#     cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "#     for col in tqdm(cols):\n",
    "#         train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "#     # Transform int64 columns to int32\n",
    "#     cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "#     for col in tqdm(cols):\n",
    "#         train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "#     # Get the difference\n",
    "#     train_diff = get_difference(train, num_features)\n",
    "#     train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "#     del train_num_agg, train_cat_agg, train_diff\n",
    "#     gc.collect()\n",
    "#     test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "#     print('Starting test feature engineer...')\n",
    "#     test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "#     test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "#     test_num_agg.reset_index(inplace = True)\n",
    "#     test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "#     test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "#     test_cat_agg.reset_index(inplace = True)\n",
    "#     # Transform float64 columns to float32\n",
    "#     cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "#     for col in tqdm(cols):\n",
    "#         test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "#     # Transform int64 columns to int32\n",
    "#     cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "#     for col in tqdm(cols):\n",
    "#         test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "#     # Get the difference\n",
    "#     test_diff = get_difference(test, num_featusres)\n",
    "#     test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "#     del test_num_agg, test_cat_agg, test_diff\n",
    "#     gc.collect()\n",
    "#     # Save files to disk\n",
    "#     train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "#     test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# # Read & Preprocess Data\n",
    "# read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3638c92e-7e65-43e5-8a69-e2aef6660f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp08 Tabnet lag feature/'\n",
    "    seed = 46\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"tabnet\"\n",
    "    ver = \"exp08\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# XGB train\n",
    "# ====================================================\n",
    "\n",
    "def cat_train(x, y, xt, yt,cat_features):\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    \n",
    "    prams = {\n",
    "        'depth': 8,\n",
    "        'iterations':5000,\n",
    "#         'learning_rate': 0.05,\n",
    "        'random_state':CFG.seed,\n",
    "    }\n",
    "\n",
    "#     watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    watchlist = [(x, 'train'), (xt, 'eval')]\n",
    "    clf = CatBoostClassifier(**prams)# 5000\n",
    "    clf.fit(x, y, eval_set=[(xt, yt)], cat_features=cat_features,plot=True, verbose_eval = 100)\n",
    "#     print('best ntree_limit:', clf.best_ntree_limit)\n",
    "#     print('best score:', clf.best_score)\n",
    "    # return clf.predict_proba(xt)[:, 1]\n",
    "    return clf.predict_proba(xt)[:, 1], clf\n",
    "\n",
    "def tabnet_train(x, y, xt, yt,cat_features):\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    \n",
    "    prams = {\n",
    "        'depth': 8,\n",
    "        'iterations':5000,\n",
    "#         'learning_rate': 0.05,\n",
    "        'random_state':CFG.seed,\n",
    "    }\n",
    "\n",
    "#     watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    watchlist = [(x, 'train'), (xt, 'eval')]\n",
    "    clf = CatBoostClassifier(**prams)# 5000\n",
    "    clf.fit(x, y, eval_set=[(xt, yt)], cat_features=cat_features,plot=True, verbose_eval = 100)\n",
    "#     print('best ntree_limit:', clf.best_ntree_limit)\n",
    "#     print('best score:', clf.best_score)\n",
    "    # return clf.predict_proba(xt)[:, 1]\n",
    "    return clf.predict_proba(xt)[:, 1], clf\n",
    "\n",
    "\n",
    "\n",
    "# using amex metric to evaluate tabnet\n",
    "class Amex_tabnet(Metric):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._name = 'amex_tabnet'\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        amex = amex_metric_numpy(y_true, y_pred[:, 1])\n",
    "        return max(amex, 0.)\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "# Created by https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def amex_metric_numpy(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b719cd4-45c3-4dba-8955-cb37b93b1f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # ====================================================\n",
    "# # Train & Evaluate\n",
    "# # ====================================================\n",
    "\n",
    "\n",
    "# cat_features = [\n",
    "#     \"B_30\",\n",
    "#     \"B_38\",\n",
    "#     \"D_114\",\n",
    "#     \"D_116\",\n",
    "#     \"D_117\",\n",
    "#     \"D_120\",\n",
    "#     \"D_126\",\n",
    "#     \"D_63\",\n",
    "#     \"D_64\",\n",
    "#     \"D_66\",\n",
    "#     \"D_68\"\n",
    "# ]\n",
    "# cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# for cat_col in cat_features:\n",
    "#     encoder = LabelEncoder()\n",
    "#     train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "#     test[cat_col] = encoder.transform(test[cat_col])\n",
    "# # Round last float features to 2 decimal place\n",
    "# num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "# num_cols = [col for col in num_cols if 'last' in col]\n",
    "# for col in num_cols:\n",
    "#     train[col + '_round2'] = train[col].round(2)\n",
    "#     test[col + '_round2'] = test[col].round(2)\n",
    "# # Get the difference between last and mean\n",
    "# num_cols = [col for col in train.columns if 'last' in col]\n",
    "# num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "# for col in num_cols:\n",
    "#     try:\n",
    "#         train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "#         test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "#     except:\n",
    "#         pass\n",
    "# # Transform float64 and float32 to float16\n",
    "# num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "# for col in tqdm(num_cols):\n",
    "#     train[col] = train[col].astype(np.float16)\n",
    "#     test[col] = test[col].astype(np.float16)\n",
    "\n",
    "\n",
    "# #     params = {\n",
    "# #         'objective': 'binary:logistic', \n",
    "# #         'tree_method': 'hist', #gpu_hist #hist\n",
    "# #         'max_depth': 7,\n",
    "# #         'subsample':0.88,\n",
    "# #         'colsample_bytree': 0.5,\n",
    "# #         'gamma':1.5,\n",
    "# #         'min_child_weight':8,\n",
    "# #         'lambda':70,\n",
    "# #         'eta':0.03}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220db67f-9a57-436b-9ec6-8732c6c6fe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1460 features...\n",
      "start Training fold 0\n",
      "epoch 0  | loss: 0.57556 | val_0_auc: 0.88237 | val_0_accuracy: 0.81341 | val_0_amex_tabnet: 0.54529 |  0:04:39s\n",
      "epoch 1  | loss: 0.40267 | val_0_auc: 0.9087  | val_0_accuracy: 0.84091 | val_0_amex_tabnet: 0.60425 |  0:09:19s\n",
      "epoch 2  | loss: 0.35697 | val_0_auc: 0.91864 | val_0_accuracy: 0.85069 | val_0_amex_tabnet: 0.62819 |  0:13:58s\n",
      "epoch 3  | loss: 0.33654 | val_0_auc: 0.92087 | val_0_accuracy: 0.85545 | val_0_amex_tabnet: 0.63975 |  0:18:37s\n",
      "epoch 4  | loss: 0.32599 | val_0_auc: 0.92358 | val_0_accuracy: 0.85753 | val_0_amex_tabnet: 0.64629 |  0:23:16s\n",
      "epoch 5  | loss: 0.3123  | val_0_auc: 0.92962 | val_0_accuracy: 0.86406 | val_0_amex_tabnet: 0.66731 |  0:27:55s\n",
      "epoch 6  | loss: 0.29858 | val_0_auc: 0.93434 | val_0_accuracy: 0.86867 | val_0_amex_tabnet: 0.68453 |  0:32:34s\n",
      "epoch 7  | loss: 0.2869  | val_0_auc: 0.93894 | val_0_accuracy: 0.87417 | val_0_amex_tabnet: 0.70074 |  0:37:13s\n",
      "epoch 8  | loss: 0.27936 | val_0_auc: 0.94033 | val_0_accuracy: 0.87696 | val_0_amex_tabnet: 0.70514 |  0:41:48s\n",
      "epoch 9  | loss: 0.27657 | val_0_auc: 0.94094 | val_0_accuracy: 0.87597 | val_0_amex_tabnet: 0.70874 |  0:46:19s\n",
      "epoch 10 | loss: 0.27249 | val_0_auc: 0.94531 | val_0_accuracy: 0.88288 | val_0_amex_tabnet: 0.72763 |  0:50:38s\n",
      "epoch 11 | loss: 0.26138 | val_0_auc: 0.95055 | val_0_accuracy: 0.89022 | val_0_amex_tabnet: 0.74426 |  0:54:44s\n",
      "epoch 12 | loss: 0.24898 | val_0_auc: 0.95481 | val_0_accuracy: 0.89497 | val_0_amex_tabnet: 0.75806 |  0:58:44s\n",
      "epoch 13 | loss: 0.23874 | val_0_auc: 0.95743 | val_0_accuracy: 0.89766 | val_0_amex_tabnet: 0.77378 |  1:02:41s\n",
      "epoch 14 | loss: 0.23349 | val_0_auc: 0.95835 | val_0_accuracy: 0.89957 | val_0_amex_tabnet: 0.77725 |  1:06:37s\n",
      "epoch 15 | loss: 0.23845 | val_0_auc: 0.95757 | val_0_accuracy: 0.89704 | val_0_amex_tabnet: 0.77241 |  1:10:31s\n",
      "epoch 16 | loss: 0.23483 | val_0_auc: 0.95819 | val_0_accuracy: 0.89908 | val_0_amex_tabnet: 0.77706 |  1:13:59s\n",
      "epoch 17 | loss: 0.23218 | val_0_auc: 0.95916 | val_0_accuracy: 0.90059 | val_0_amex_tabnet: 0.78086 |  1:17:11s\n",
      "epoch 18 | loss: 0.22906 | val_0_auc: 0.95955 | val_0_accuracy: 0.89957 | val_0_amex_tabnet: 0.78308 |  1:20:36s\n",
      "epoch 19 | loss: 0.22678 | val_0_auc: 0.96025 | val_0_accuracy: 0.90186 | val_0_amex_tabnet: 0.78488 |  1:24:11s\n",
      "epoch 20 | loss: 0.23195 | val_0_auc: 0.9595  | val_0_accuracy: 0.90057 | val_0_amex_tabnet: 0.78225 |  1:27:37s\n",
      "epoch 21 | loss: 0.23    | val_0_auc: 0.95968 | val_0_accuracy: 0.90021 | val_0_amex_tabnet: 0.78272 |  1:30:54s\n",
      "epoch 22 | loss: 0.22929 | val_0_auc: 0.95998 | val_0_accuracy: 0.90048 | val_0_amex_tabnet: 0.78422 |  1:34:09s\n",
      "epoch 23 | loss: 0.22711 | val_0_auc: 0.96026 | val_0_accuracy: 0.90163 | val_0_amex_tabnet: 0.7856  |  1:37:22s\n",
      "epoch 24 | loss: 0.22491 | val_0_auc: 0.96066 | val_0_accuracy: 0.90184 | val_0_amex_tabnet: 0.78749 |  1:40:35s\n",
      "epoch 25 | loss: 0.22931 | val_0_auc: 0.95971 | val_0_accuracy: 0.8997  | val_0_amex_tabnet: 0.78387 |  1:43:48s\n",
      "epoch 26 | loss: 0.2281  | val_0_auc: 0.9599  | val_0_accuracy: 0.90168 | val_0_amex_tabnet: 0.78402 |  1:47:28s\n",
      "epoch 27 | loss: 0.22709 | val_0_auc: 0.96048 | val_0_accuracy: 0.90109 | val_0_amex_tabnet: 0.7861  |  1:51:33s\n",
      "epoch 28 | loss: 0.22559 | val_0_auc: 0.96005 | val_0_accuracy: 0.90024 | val_0_amex_tabnet: 0.78581 |  1:55:33s\n",
      "epoch 29 | loss: 0.22316 | val_0_auc: 0.9608  | val_0_accuracy: 0.90219 | val_0_amex_tabnet: 0.78837 |  1:59:42s\n",
      "epoch 30 | loss: 0.22753 | val_0_auc: 0.96052 | val_0_accuracy: 0.90149 | val_0_amex_tabnet: 0.78741 |  2:04:03s\n",
      "epoch 31 | loss: 0.22651 | val_0_auc: 0.96041 | val_0_accuracy: 0.90169 | val_0_amex_tabnet: 0.78743 |  2:08:17s\n",
      "epoch 32 | loss: 0.2254  | val_0_auc: 0.96038 | val_0_accuracy: 0.90222 | val_0_amex_tabnet: 0.78607 |  2:12:23s\n",
      "epoch 33 | loss: 0.22365 | val_0_auc: 0.96103 | val_0_accuracy: 0.90231 | val_0_amex_tabnet: 0.78883 |  2:16:21s\n",
      "epoch 34 | loss: 0.22155 | val_0_auc: 0.96088 | val_0_accuracy: 0.90194 | val_0_amex_tabnet: 0.78803 |  2:20:09s\n",
      "epoch 35 | loss: 0.22648 | val_0_auc: 0.96049 | val_0_accuracy: 0.90199 | val_0_amex_tabnet: 0.78623 |  2:23:50s\n",
      "epoch 36 | loss: 0.22562 | val_0_auc: 0.96041 | val_0_accuracy: 0.90139 | val_0_amex_tabnet: 0.78625 |  2:27:59s\n",
      "epoch 37 | loss: 0.22438 | val_0_auc: 0.96045 | val_0_accuracy: 0.90105 | val_0_amex_tabnet: 0.78638 |  2:32:17s\n",
      "epoch 38 | loss: 0.22273 | val_0_auc: 0.96089 | val_0_accuracy: 0.90268 | val_0_amex_tabnet: 0.78655 |  2:36:38s\n",
      "epoch 39 | loss: 0.22069 | val_0_auc: 0.96109 | val_0_accuracy: 0.90285 | val_0_amex_tabnet: 0.78816 |  2:41:00s\n",
      "epoch 40 | loss: 0.22596 | val_0_auc: 0.96063 | val_0_accuracy: 0.90199 | val_0_amex_tabnet: 0.78557 |  2:45:23s\n",
      "epoch 41 | loss: 0.22467 | val_0_auc: 0.96025 | val_0_accuracy: 0.90168 | val_0_amex_tabnet: 0.78639 |  2:49:42s\n",
      "epoch 42 | loss: 0.22336 | val_0_auc: 0.96076 | val_0_accuracy: 0.90191 | val_0_amex_tabnet: 0.78835 |  2:53:46s\n",
      "epoch 43 | loss: 0.22163 | val_0_auc: 0.96093 | val_0_accuracy: 0.90155 | val_0_amex_tabnet: 0.78785 |  2:57:55s\n",
      "epoch 44 | loss: 0.21946 | val_0_auc: 0.96096 | val_0_accuracy: 0.90254 | val_0_amex_tabnet: 0.78832 |  3:02:01s\n",
      "epoch 45 | loss: 0.22529 | val_0_auc: 0.96034 | val_0_accuracy: 0.90203 | val_0_amex_tabnet: 0.78726 |  3:06:19s\n",
      "epoch 46 | loss: 0.22434 | val_0_auc: 0.96067 | val_0_accuracy: 0.90205 | val_0_amex_tabnet: 0.78727 |  3:10:45s\n",
      "epoch 47 | loss: 0.22313 | val_0_auc: 0.96097 | val_0_accuracy: 0.90272 | val_0_amex_tabnet: 0.7898  |  3:15:02s\n",
      "epoch 48 | loss: 0.22117 | val_0_auc: 0.96104 | val_0_accuracy: 0.90265 | val_0_amex_tabnet: 0.78874 |  3:18:56s\n",
      "epoch 49 | loss: 0.21939 | val_0_auc: 0.96101 | val_0_accuracy: 0.90269 | val_0_amex_tabnet: 0.78965 |  3:22:27s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_amex_tabnet = 0.7898\n",
      "save fold0 model\n",
      "Successfully saved model at ../output/exp08 Tabnet lag feature/tabnet_fold0.zip\n",
      "preds :  91783\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1460 features...\n",
      "start Training fold 1\n",
      "epoch 0  | loss: 0.57102 | val_0_auc: 0.8841  | val_0_accuracy: 0.81635 | val_0_amex_tabnet: 0.54113 |  0:04:37s\n",
      "epoch 1  | loss: 0.4011  | val_0_auc: 0.90481 | val_0_accuracy: 0.83961 | val_0_amex_tabnet: 0.59291 |  0:09:16s\n",
      "epoch 2  | loss: 0.36337 | val_0_auc: 0.90908 | val_0_accuracy: 0.84824 | val_0_amex_tabnet: 0.61473 |  0:13:58s\n",
      "epoch 3  | loss: 0.33601 | val_0_auc: 0.91823 | val_0_accuracy: 0.85685 | val_0_amex_tabnet: 0.63547 |  0:18:37s\n",
      "epoch 4  | loss: 0.32033 | val_0_auc: 0.92056 | val_0_accuracy: 0.85795 | val_0_amex_tabnet: 0.64637 |  0:23:16s\n",
      "epoch 5  | loss: 0.31038 | val_0_auc: 0.92823 | val_0_accuracy: 0.86401 | val_0_amex_tabnet: 0.66531 |  0:27:56s\n",
      "epoch 6  | loss: 0.29542 | val_0_auc: 0.93319 | val_0_accuracy: 0.86873 | val_0_amex_tabnet: 0.6827  |  0:32:33s\n",
      "epoch 7  | loss: 0.28632 | val_0_auc: 0.93548 | val_0_accuracy: 0.87077 | val_0_amex_tabnet: 0.69265 |  0:37:08s\n",
      "epoch 8  | loss: 0.28163 | val_0_auc: 0.93663 | val_0_accuracy: 0.87081 | val_0_amex_tabnet: 0.69668 |  0:41:40s\n",
      "epoch 9  | loss: 0.27883 | val_0_auc: 0.93729 | val_0_accuracy: 0.87214 | val_0_amex_tabnet: 0.70019 |  0:46:10s\n",
      "epoch 10 | loss: 0.27568 | val_0_auc: 0.94162 | val_0_accuracy: 0.87684 | val_0_amex_tabnet: 0.71077 |  0:50:29s\n",
      "epoch 11 | loss: 0.25949 | val_0_auc: 0.95123 | val_0_accuracy: 0.88947 | val_0_amex_tabnet: 0.74995 |  0:54:38s\n",
      "epoch 12 | loss: 0.24279 | val_0_auc: 0.95401 | val_0_accuracy: 0.89515 | val_0_amex_tabnet: 0.75924 |  0:58:36s\n",
      "epoch 13 | loss: 0.23609 | val_0_auc: 0.95573 | val_0_accuracy: 0.89734 | val_0_amex_tabnet: 0.76761 |  1:02:33s\n",
      "epoch 14 | loss: 0.23126 | val_0_auc: 0.95709 | val_0_accuracy: 0.89787 | val_0_amex_tabnet: 0.77443 |  1:06:26s\n",
      "epoch 15 | loss: 0.23582 | val_0_auc: 0.95598 | val_0_accuracy: 0.8972  | val_0_amex_tabnet: 0.76846 |  1:10:08s\n",
      "epoch 16 | loss: 0.23204 | val_0_auc: 0.95756 | val_0_accuracy: 0.89846 | val_0_amex_tabnet: 0.77798 |  1:13:49s\n",
      "epoch 17 | loss: 0.23002 | val_0_auc: 0.95819 | val_0_accuracy: 0.89992 | val_0_amex_tabnet: 0.77903 |  1:17:28s\n",
      "epoch 18 | loss: 0.22763 | val_0_auc: 0.95884 | val_0_accuracy: 0.90096 | val_0_amex_tabnet: 0.78005 |  1:21:02s\n",
      "epoch 19 | loss: 0.22471 | val_0_auc: 0.95935 | val_0_accuracy: 0.90083 | val_0_amex_tabnet: 0.78334 |  1:24:31s\n",
      "epoch 20 | loss: 0.2292  | val_0_auc: 0.95839 | val_0_accuracy: 0.8986  | val_0_amex_tabnet: 0.77981 |  1:27:54s\n",
      "epoch 21 | loss: 0.22807 | val_0_auc: 0.95867 | val_0_accuracy: 0.89999 | val_0_amex_tabnet: 0.78033 |  1:31:34s\n",
      "epoch 22 | loss: 0.22668 | val_0_auc: 0.95909 | val_0_accuracy: 0.90084 | val_0_amex_tabnet: 0.78149 |  1:34:58s\n",
      "epoch 23 | loss: 0.22476 | val_0_auc: 0.95927 | val_0_accuracy: 0.90074 | val_0_amex_tabnet: 0.78452 |  1:38:36s\n",
      "epoch 24 | loss: 0.22289 | val_0_auc: 0.95959 | val_0_accuracy: 0.90099 | val_0_amex_tabnet: 0.78467 |  1:41:52s\n",
      "epoch 25 | loss: 0.22733 | val_0_auc: 0.95904 | val_0_accuracy: 0.9     | val_0_amex_tabnet: 0.7824  |  1:45:28s\n",
      "epoch 26 | loss: 0.22656 | val_0_auc: 0.9587  | val_0_accuracy: 0.90003 | val_0_amex_tabnet: 0.78191 |  1:48:43s\n",
      "epoch 27 | loss: 0.22462 | val_0_auc: 0.95904 | val_0_accuracy: 0.89919 | val_0_amex_tabnet: 0.78495 |  1:52:04s\n",
      "epoch 28 | loss: 0.22298 | val_0_auc: 0.95968 | val_0_accuracy: 0.90116 | val_0_amex_tabnet: 0.78594 |  1:55:45s\n",
      "epoch 29 | loss: 0.22165 | val_0_auc: 0.9598  | val_0_accuracy: 0.90113 | val_0_amex_tabnet: 0.78753 |  1:58:58s\n",
      "epoch 30 | loss: 0.22647 | val_0_auc: 0.95941 | val_0_accuracy: 0.90109 | val_0_amex_tabnet: 0.78429 |  2:02:40s\n",
      "epoch 31 | loss: 0.22519 | val_0_auc: 0.95942 | val_0_accuracy: 0.90099 | val_0_amex_tabnet: 0.78392 |  2:06:20s\n",
      "epoch 32 | loss: 0.224   | val_0_auc: 0.95968 | val_0_accuracy: 0.90107 | val_0_amex_tabnet: 0.78695 |  2:10:12s\n",
      "epoch 33 | loss: 0.22213 | val_0_auc: 0.95971 | val_0_accuracy: 0.90146 | val_0_amex_tabnet: 0.78702 |  2:13:52s\n",
      "epoch 34 | loss: 0.22073 | val_0_auc: 0.95981 | val_0_accuracy: 0.90139 | val_0_amex_tabnet: 0.78641 |  2:16:47s\n",
      "epoch 35 | loss: 0.22516 | val_0_auc: 0.95897 | val_0_accuracy: 0.90105 | val_0_amex_tabnet: 0.78304 |  2:20:40s\n",
      "epoch 36 | loss: 0.22391 | val_0_auc: 0.95917 | val_0_accuracy: 0.89889 | val_0_amex_tabnet: 0.78398 |  2:24:26s\n",
      "epoch 37 | loss: 0.22327 | val_0_auc: 0.95963 | val_0_accuracy: 0.90116 | val_0_amex_tabnet: 0.78497 |  2:28:23s\n",
      "epoch 38 | loss: 0.22191 | val_0_auc: 0.95967 | val_0_accuracy: 0.90032 | val_0_amex_tabnet: 0.78442 |  2:32:15s\n",
      "epoch 39 | loss: 0.21986 | val_0_auc: 0.95981 | val_0_accuracy: 0.9007  | val_0_amex_tabnet: 0.78574 |  2:36:02s\n",
      "epoch 40 | loss: 0.22505 | val_0_auc: 0.95912 | val_0_accuracy: 0.89969 | val_0_amex_tabnet: 0.7823  |  2:40:13s\n",
      "epoch 41 | loss: 0.22381 | val_0_auc: 0.95938 | val_0_accuracy: 0.89938 | val_0_amex_tabnet: 0.78537 |  2:44:17s\n",
      "epoch 42 | loss: 0.22245 | val_0_auc: 0.9596  | val_0_accuracy: 0.90031 | val_0_amex_tabnet: 0.78641 |  2:48:18s\n",
      "epoch 43 | loss: 0.22117 | val_0_auc: 0.95982 | val_0_accuracy: 0.90128 | val_0_amex_tabnet: 0.78629 |  2:52:15s\n",
      "epoch 44 | loss: 0.21923 | val_0_auc: 0.95967 | val_0_accuracy: 0.90066 | val_0_amex_tabnet: 0.78646 |  2:56:29s\n",
      "epoch 45 | loss: 0.22411 | val_0_auc: 0.95929 | val_0_accuracy: 0.90084 | val_0_amex_tabnet: 0.78345 |  3:00:45s\n",
      "epoch 46 | loss: 0.22303 | val_0_auc: 0.95955 | val_0_accuracy: 0.90108 | val_0_amex_tabnet: 0.78508 |  3:04:56s\n",
      "epoch 47 | loss: 0.22243 | val_0_auc: 0.95936 | val_0_accuracy: 0.90072 | val_0_amex_tabnet: 0.78533 |  3:08:43s\n",
      "epoch 48 | loss: 0.22049 | val_0_auc: 0.95942 | val_0_accuracy: 0.90065 | val_0_amex_tabnet: 0.78445 |  3:12:53s\n",
      "epoch 49 | loss: 0.21909 | val_0_auc: 0.95971 | val_0_accuracy: 0.90094 | val_0_amex_tabnet: 0.78738 |  3:17:01s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 29 and best_val_0_amex_tabnet = 0.78753\n",
      "save fold1 model\n",
      "Successfully saved model at ../output/exp08 Tabnet lag feature/tabnet_fold1.zip\n",
      "preds :  183566\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1460 features...\n",
      "start Training fold 2\n",
      "epoch 0  | loss: 0.58059 | val_0_auc: 0.88229 | val_0_accuracy: 0.80929 | val_0_amex_tabnet: 0.5393  |  0:04:38s\n",
      "epoch 1  | loss: 0.40518 | val_0_auc: 0.91097 | val_0_accuracy: 0.84282 | val_0_amex_tabnet: 0.61039 |  0:09:16s\n",
      "epoch 2  | loss: 0.3612  | val_0_auc: 0.91899 | val_0_accuracy: 0.8525  | val_0_amex_tabnet: 0.63184 |  0:13:55s\n",
      "epoch 3  | loss: 0.34142 | val_0_auc: 0.92211 | val_0_accuracy: 0.85647 | val_0_amex_tabnet: 0.63944 |  0:18:34s\n",
      "epoch 4  | loss: 0.33301 | val_0_auc: 0.92244 | val_0_accuracy: 0.85704 | val_0_amex_tabnet: 0.64042 |  0:23:13s\n",
      "epoch 5  | loss: 0.32445 | val_0_auc: 0.92601 | val_0_accuracy: 0.86076 | val_0_amex_tabnet: 0.65091 |  0:27:53s\n",
      "epoch 6  | loss: 0.30791 | val_0_auc: 0.93062 | val_0_accuracy: 0.86502 | val_0_amex_tabnet: 0.66761 |  0:32:32s\n",
      "epoch 7  | loss: 0.29762 | val_0_auc: 0.93303 | val_0_accuracy: 0.86756 | val_0_amex_tabnet: 0.68008 |  0:37:08s\n",
      "epoch 8  | loss: 0.29269 | val_0_auc: 0.93384 | val_0_accuracy: 0.8683  | val_0_amex_tabnet: 0.68532 |  0:41:40s\n",
      "epoch 9  | loss: 0.28925 | val_0_auc: 0.93472 | val_0_accuracy: 0.86785 | val_0_amex_tabnet: 0.68716 |  0:46:10s\n",
      "epoch 10 | loss: 0.28671 | val_0_auc: 0.93757 | val_0_accuracy: 0.87209 | val_0_amex_tabnet: 0.69937 |  0:50:29s\n",
      "epoch 11 | loss: 0.27768 | val_0_auc: 0.94232 | val_0_accuracy: 0.87984 | val_0_amex_tabnet: 0.71998 |  0:54:35s\n",
      "epoch 12 | loss: 0.26677 | val_0_auc: 0.94626 | val_0_accuracy: 0.88612 | val_0_amex_tabnet: 0.73415 |  0:58:35s\n",
      "epoch 13 | loss: 0.25203 | val_0_auc: 0.95303 | val_0_accuracy: 0.89303 | val_0_amex_tabnet: 0.75701 |  1:02:32s\n",
      "epoch 14 | loss: 0.24418 | val_0_auc: 0.95446 | val_0_accuracy: 0.89422 | val_0_amex_tabnet: 0.76299 |  1:06:27s\n",
      "epoch 15 | loss: 0.24382 | val_0_auc: 0.95549 | val_0_accuracy: 0.89636 | val_0_amex_tabnet: 0.76978 |  1:10:16s\n",
      "epoch 16 | loss: 0.23732 | val_0_auc: 0.95689 | val_0_accuracy: 0.8986  | val_0_amex_tabnet: 0.77251 |  1:13:52s\n",
      "epoch 17 | loss: 0.2335  | val_0_auc: 0.95749 | val_0_accuracy: 0.89894 | val_0_amex_tabnet: 0.77647 |  1:17:13s\n",
      "epoch 18 | loss: 0.2301  | val_0_auc: 0.95863 | val_0_accuracy: 0.89962 | val_0_amex_tabnet: 0.78103 |  1:20:23s\n",
      "epoch 19 | loss: 0.22678 | val_0_auc: 0.95892 | val_0_accuracy: 0.89952 | val_0_amex_tabnet: 0.78089 |  1:23:43s\n",
      "epoch 20 | loss: 0.23179 | val_0_auc: 0.95797 | val_0_accuracy: 0.89769 | val_0_amex_tabnet: 0.77752 |  1:27:33s\n",
      "epoch 21 | loss: 0.22954 | val_0_auc: 0.95869 | val_0_accuracy: 0.90094 | val_0_amex_tabnet: 0.78138 |  1:30:59s\n",
      "epoch 22 | loss: 0.22808 | val_0_auc: 0.95911 | val_0_accuracy: 0.90001 | val_0_amex_tabnet: 0.78325 |  1:34:30s\n",
      "epoch 23 | loss: 0.22599 | val_0_auc: 0.95931 | val_0_accuracy: 0.9014  | val_0_amex_tabnet: 0.78296 |  1:38:17s\n",
      "epoch 24 | loss: 0.22367 | val_0_auc: 0.95944 | val_0_accuracy: 0.8995  | val_0_amex_tabnet: 0.7828  |  1:41:24s\n",
      "epoch 25 | loss: 0.2277  | val_0_auc: 0.95881 | val_0_accuracy: 0.9005  | val_0_amex_tabnet: 0.78095 |  1:44:37s\n",
      "epoch 26 | loss: 0.22703 | val_0_auc: 0.95932 | val_0_accuracy: 0.90031 | val_0_amex_tabnet: 0.78354 |  1:47:34s\n",
      "epoch 27 | loss: 0.22565 | val_0_auc: 0.95952 | val_0_accuracy: 0.90096 | val_0_amex_tabnet: 0.78359 |  1:50:18s\n",
      "epoch 28 | loss: 0.22393 | val_0_auc: 0.95957 | val_0_accuracy: 0.90187 | val_0_amex_tabnet: 0.78344 |  1:53:05s\n",
      "epoch 29 | loss: 0.22179 | val_0_auc: 0.95998 | val_0_accuracy: 0.90245 | val_0_amex_tabnet: 0.78607 |  1:56:22s\n",
      "epoch 30 | loss: 0.22678 | val_0_auc: 0.95964 | val_0_accuracy: 0.90186 | val_0_amex_tabnet: 0.78487 |  1:59:37s\n",
      "epoch 31 | loss: 0.225   | val_0_auc: 0.95955 | val_0_accuracy: 0.90095 | val_0_amex_tabnet: 0.7828  |  2:03:16s\n",
      "epoch 32 | loss: 0.224   | val_0_auc: 0.95973 | val_0_accuracy: 0.90226 | val_0_amex_tabnet: 0.78549 |  2:07:01s\n",
      "epoch 33 | loss: 0.22228 | val_0_auc: 0.95998 | val_0_accuracy: 0.90262 | val_0_amex_tabnet: 0.78616 |  2:10:47s\n",
      "epoch 34 | loss: 0.22061 | val_0_auc: 0.95989 | val_0_accuracy: 0.90242 | val_0_amex_tabnet: 0.78595 |  2:14:28s\n",
      "epoch 35 | loss: 0.22531 | val_0_auc: 0.95945 | val_0_accuracy: 0.90179 | val_0_amex_tabnet: 0.78298 |  2:18:17s\n",
      "epoch 36 | loss: 0.2243  | val_0_auc: 0.95919 | val_0_accuracy: 0.90131 | val_0_amex_tabnet: 0.78017 |  2:21:59s\n",
      "epoch 37 | loss: 0.22355 | val_0_auc: 0.95991 | val_0_accuracy: 0.90263 | val_0_amex_tabnet: 0.78559 |  2:25:34s\n",
      "epoch 38 | loss: 0.22152 | val_0_auc: 0.96009 | val_0_accuracy: 0.90207 | val_0_amex_tabnet: 0.78659 |  2:29:25s\n",
      "epoch 39 | loss: 0.21939 | val_0_auc: 0.96005 | val_0_accuracy: 0.90285 | val_0_amex_tabnet: 0.78467 |  2:33:13s\n",
      "epoch 40 | loss: 0.22453 | val_0_auc: 0.95939 | val_0_accuracy: 0.90053 | val_0_amex_tabnet: 0.78382 |  2:37:03s\n",
      "epoch 41 | loss: 0.22372 | val_0_auc: 0.95978 | val_0_accuracy: 0.90128 | val_0_amex_tabnet: 0.78511 |  2:40:50s\n",
      "epoch 42 | loss: 0.22262 | val_0_auc: 0.95989 | val_0_accuracy: 0.90271 | val_0_amex_tabnet: 0.7856  |  2:44:05s\n",
      "epoch 43 | loss: 0.22087 | val_0_auc: 0.96004 | val_0_accuracy: 0.90263 | val_0_amex_tabnet: 0.7867  |  2:47:43s\n",
      "epoch 44 | loss: 0.21895 | val_0_auc: 0.96006 | val_0_accuracy: 0.90294 | val_0_amex_tabnet: 0.78563 |  2:51:35s\n",
      "epoch 45 | loss: 0.22446 | val_0_auc: 0.95941 | val_0_accuracy: 0.90232 | val_0_amex_tabnet: 0.78374 |  2:55:24s\n",
      "epoch 46 | loss: 0.223   | val_0_auc: 0.96005 | val_0_accuracy: 0.90284 | val_0_amex_tabnet: 0.78596 |  2:59:34s\n",
      "epoch 47 | loss: 0.22243 | val_0_auc: 0.96011 | val_0_accuracy: 0.90316 | val_0_amex_tabnet: 0.78444 |  3:03:39s\n",
      "epoch 48 | loss: 0.22035 | val_0_auc: 0.95989 | val_0_accuracy: 0.90219 | val_0_amex_tabnet: 0.78481 |  3:07:30s\n",
      "epoch 49 | loss: 0.21866 | val_0_auc: 0.96007 | val_0_accuracy: 0.90274 | val_0_amex_tabnet: 0.78543 |  3:11:05s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 43 and best_val_0_amex_tabnet = 0.7867\n",
      "save fold2 model\n",
      "Successfully saved model at ../output/exp08 Tabnet lag feature/tabnet_fold2.zip\n",
      "preds :  275349\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1460 features...\n",
      "start Training fold 3\n",
      "epoch 0  | loss: 0.57761 | val_0_auc: 0.88334 | val_0_accuracy: 0.81395 | val_0_amex_tabnet: 0.54492 |  0:04:38s\n",
      "epoch 1  | loss: 0.40372 | val_0_auc: 0.91051 | val_0_accuracy: 0.84469 | val_0_amex_tabnet: 0.6115  |  0:09:15s\n",
      "epoch 2  | loss: 0.35888 | val_0_auc: 0.91843 | val_0_accuracy: 0.85517 | val_0_amex_tabnet: 0.63237 |  0:13:52s\n",
      "epoch 3  | loss: 0.33317 | val_0_auc: 0.92269 | val_0_accuracy: 0.85805 | val_0_amex_tabnet: 0.64592 |  0:18:29s\n",
      "epoch 4  | loss: 0.31813 | val_0_auc: 0.92359 | val_0_accuracy: 0.85926 | val_0_amex_tabnet: 0.65228 |  0:23:05s\n",
      "epoch 5  | loss: 0.30917 | val_0_auc: 0.92772 | val_0_accuracy: 0.8627  | val_0_amex_tabnet: 0.66372 |  0:27:42s\n",
      "epoch 6  | loss: 0.30001 | val_0_auc: 0.92945 | val_0_accuracy: 0.86388 | val_0_amex_tabnet: 0.67151 |  0:32:18s\n",
      "epoch 7  | loss: 0.29465 | val_0_auc: 0.93243 | val_0_accuracy: 0.86733 | val_0_amex_tabnet: 0.67901 |  0:36:54s\n",
      "epoch 8  | loss: 0.28991 | val_0_auc: 0.93335 | val_0_accuracy: 0.86945 | val_0_amex_tabnet: 0.68706 |  0:41:25s\n",
      "epoch 9  | loss: 0.2867  | val_0_auc: 0.9345  | val_0_accuracy: 0.8705  | val_0_amex_tabnet: 0.68983 |  0:45:55s\n",
      "epoch 10 | loss: 0.28219 | val_0_auc: 0.93964 | val_0_accuracy: 0.87404 | val_0_amex_tabnet: 0.7074  |  0:50:12s\n",
      "epoch 11 | loss: 0.26877 | val_0_auc: 0.94736 | val_0_accuracy: 0.88743 | val_0_amex_tabnet: 0.73832 |  0:54:15s\n",
      "epoch 12 | loss: 0.25437 | val_0_auc: 0.95127 | val_0_accuracy: 0.89001 | val_0_amex_tabnet: 0.74965 |  0:58:12s\n",
      "epoch 13 | loss: 0.24372 | val_0_auc: 0.9545  | val_0_accuracy: 0.8944  | val_0_amex_tabnet: 0.76266 |  1:02:05s\n",
      "epoch 14 | loss: 0.23756 | val_0_auc: 0.95575 | val_0_accuracy: 0.89683 | val_0_amex_tabnet: 0.77101 |  1:05:58s\n",
      "epoch 15 | loss: 0.24134 | val_0_auc: 0.95601 | val_0_accuracy: 0.89638 | val_0_amex_tabnet: 0.76874 |  1:09:43s\n",
      "epoch 16 | loss: 0.23606 | val_0_auc: 0.95729 | val_0_accuracy: 0.8978  | val_0_amex_tabnet: 0.77479 |  1:13:08s\n",
      "epoch 17 | loss: 0.23256 | val_0_auc: 0.95791 | val_0_accuracy: 0.89914 | val_0_amex_tabnet: 0.77676 |  1:16:18s\n",
      "epoch 18 | loss: 0.22961 | val_0_auc: 0.95871 | val_0_accuracy: 0.90072 | val_0_amex_tabnet: 0.7809  |  1:19:17s\n",
      "epoch 19 | loss: 0.22689 | val_0_auc: 0.9588  | val_0_accuracy: 0.90074 | val_0_amex_tabnet: 0.78205 |  1:22:12s\n",
      "epoch 20 | loss: 0.2313  | val_0_auc: 0.95868 | val_0_accuracy: 0.90034 | val_0_amex_tabnet: 0.78275 |  1:25:07s\n",
      "epoch 21 | loss: 0.22918 | val_0_auc: 0.95926 | val_0_accuracy: 0.90059 | val_0_amex_tabnet: 0.78533 |  1:28:21s\n",
      "epoch 22 | loss: 0.22718 | val_0_auc: 0.9591  | val_0_accuracy: 0.90145 | val_0_amex_tabnet: 0.78319 |  1:31:33s\n",
      "epoch 23 | loss: 0.22546 | val_0_auc: 0.95973 | val_0_accuracy: 0.90131 | val_0_amex_tabnet: 0.78493 |  1:34:44s\n",
      "epoch 24 | loss: 0.22299 | val_0_auc: 0.9599  | val_0_accuracy: 0.90173 | val_0_amex_tabnet: 0.78694 |  1:37:53s\n",
      "epoch 25 | loss: 0.22739 | val_0_auc: 0.95939 | val_0_accuracy: 0.9019  | val_0_amex_tabnet: 0.78428 |  1:41:02s\n",
      "epoch 26 | loss: 0.22625 | val_0_auc: 0.95924 | val_0_accuracy: 0.89854 | val_0_amex_tabnet: 0.78467 |  1:44:08s\n",
      "epoch 27 | loss: 0.22496 | val_0_auc: 0.95955 | val_0_accuracy: 0.90117 | val_0_amex_tabnet: 0.78555 |  1:47:47s\n",
      "epoch 28 | loss: 0.22359 | val_0_auc: 0.95996 | val_0_accuracy: 0.90225 | val_0_amex_tabnet: 0.78771 |  1:51:26s\n",
      "epoch 29 | loss: 0.2212  | val_0_auc: 0.96018 | val_0_accuracy: 0.90264 | val_0_amex_tabnet: 0.7892  |  1:54:53s\n",
      "epoch 30 | loss: 0.22605 | val_0_auc: 0.95979 | val_0_accuracy: 0.90015 | val_0_amex_tabnet: 0.78468 |  1:58:31s\n",
      "epoch 31 | loss: 0.22459 | val_0_auc: 0.95952 | val_0_accuracy: 0.90078 | val_0_amex_tabnet: 0.78514 |  2:02:02s\n",
      "epoch 32 | loss: 0.22377 | val_0_auc: 0.96001 | val_0_accuracy: 0.90193 | val_0_amex_tabnet: 0.78713 |  2:05:40s\n",
      "epoch 33 | loss: 0.22179 | val_0_auc: 0.96018 | val_0_accuracy: 0.90249 | val_0_amex_tabnet: 0.78755 |  2:09:21s\n",
      "epoch 34 | loss: 0.21972 | val_0_auc: 0.9604  | val_0_accuracy: 0.90225 | val_0_amex_tabnet: 0.78786 |  2:12:53s\n",
      "epoch 35 | loss: 0.22504 | val_0_auc: 0.95954 | val_0_accuracy: 0.90131 | val_0_amex_tabnet: 0.78407 |  2:17:00s\n",
      "epoch 36 | loss: 0.22372 | val_0_auc: 0.95982 | val_0_accuracy: 0.90181 | val_0_amex_tabnet: 0.7858  |  2:21:16s\n",
      "epoch 37 | loss: 0.22267 | val_0_auc: 0.95994 | val_0_accuracy: 0.90229 | val_0_amex_tabnet: 0.78733 |  2:25:23s\n",
      "epoch 38 | loss: 0.22095 | val_0_auc: 0.96023 | val_0_accuracy: 0.90255 | val_0_amex_tabnet: 0.7883  |  2:29:20s\n",
      "epoch 39 | loss: 0.21903 | val_0_auc: 0.96027 | val_0_accuracy: 0.90215 | val_0_amex_tabnet: 0.78899 |  2:32:55s\n",
      "epoch 40 | loss: 0.22436 | val_0_auc: 0.96014 | val_0_accuracy: 0.90213 | val_0_amex_tabnet: 0.78608 |  2:37:11s\n",
      "epoch 41 | loss: 0.22344 | val_0_auc: 0.96006 | val_0_accuracy: 0.90187 | val_0_amex_tabnet: 0.78531 |  2:41:10s\n",
      "epoch 42 | loss: 0.22227 | val_0_auc: 0.95994 | val_0_accuracy: 0.90183 | val_0_amex_tabnet: 0.78689 |  2:45:01s\n",
      "epoch 43 | loss: 0.22033 | val_0_auc: 0.96031 | val_0_accuracy: 0.90151 | val_0_amex_tabnet: 0.78716 |  2:49:09s\n",
      "epoch 44 | loss: 0.21826 | val_0_auc: 0.96031 | val_0_accuracy: 0.90226 | val_0_amex_tabnet: 0.78789 |  2:53:01s\n",
      "epoch 45 | loss: 0.22436 | val_0_auc: 0.95931 | val_0_accuracy: 0.89913 | val_0_amex_tabnet: 0.78397 |  2:57:09s\n",
      "epoch 46 | loss: 0.22303 | val_0_auc: 0.95999 | val_0_accuracy: 0.90109 | val_0_amex_tabnet: 0.78845 |  3:01:17s\n",
      "epoch 47 | loss: 0.22165 | val_0_auc: 0.9599  | val_0_accuracy: 0.90142 | val_0_amex_tabnet: 0.78614 |  3:05:28s\n",
      "epoch 48 | loss: 0.22011 | val_0_auc: 0.96001 | val_0_accuracy: 0.90213 | val_0_amex_tabnet: 0.7866  |  3:09:46s\n",
      "epoch 49 | loss: 0.21786 | val_0_auc: 0.96029 | val_0_accuracy: 0.90239 | val_0_amex_tabnet: 0.78774 |  3:13:43s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 29 and best_val_0_amex_tabnet = 0.7892\n",
      "save fold3 model\n",
      "Successfully saved model at ../output/exp08 Tabnet lag feature/tabnet_fold3.zip\n",
      "preds :  367131\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1460 features...\n",
      "start Training fold 4\n",
      "epoch 0  | loss: 0.5819  | val_0_auc: 0.88164 | val_0_accuracy: 0.80868 | val_0_amex_tabnet: 0.53626 |  0:04:35s\n",
      "epoch 1  | loss: 0.40286 | val_0_auc: 0.90812 | val_0_accuracy: 0.83945 | val_0_amex_tabnet: 0.59933 |  0:09:11s\n",
      "epoch 2  | loss: 0.35947 | val_0_auc: 0.91466 | val_0_accuracy: 0.84754 | val_0_amex_tabnet: 0.62198 |  0:13:49s\n",
      "epoch 3  | loss: 0.3406  | val_0_auc: 0.91923 | val_0_accuracy: 0.85191 | val_0_amex_tabnet: 0.63507 |  0:18:24s\n",
      "epoch 4  | loss: 0.33332 | val_0_auc: 0.92038 | val_0_accuracy: 0.85365 | val_0_amex_tabnet: 0.64051 |  0:23:01s\n",
      "epoch 5  | loss: 0.32297 | val_0_auc: 0.92397 | val_0_accuracy: 0.85743 | val_0_amex_tabnet: 0.64784 |  0:27:37s\n",
      "epoch 6  | loss: 0.30851 | val_0_auc: 0.92696 | val_0_accuracy: 0.86031 | val_0_amex_tabnet: 0.66012 |  0:32:13s\n",
      "epoch 7  | loss: 0.29905 | val_0_auc: 0.92986 | val_0_accuracy: 0.8627  | val_0_amex_tabnet: 0.67007 |  0:36:46s\n",
      "epoch 8  | loss: 0.29298 | val_0_auc: 0.93299 | val_0_accuracy: 0.86725 | val_0_amex_tabnet: 0.68275 |  0:41:18s\n",
      "epoch 9  | loss: 0.2874  | val_0_auc: 0.93514 | val_0_accuracy: 0.86947 | val_0_amex_tabnet: 0.69039 |  0:45:48s\n",
      "epoch 10 | loss: 0.27925 | val_0_auc: 0.94164 | val_0_accuracy: 0.87721 | val_0_amex_tabnet: 0.71301 |  0:50:06s\n",
      "epoch 11 | loss: 0.26387 | val_0_auc: 0.94862 | val_0_accuracy: 0.88712 | val_0_amex_tabnet: 0.73804 |  0:54:10s\n",
      "epoch 12 | loss: 0.25013 | val_0_auc: 0.95294 | val_0_accuracy: 0.89266 | val_0_amex_tabnet: 0.75669 |  0:58:07s\n",
      "epoch 13 | loss: 0.24104 | val_0_auc: 0.95522 | val_0_accuracy: 0.8952  | val_0_amex_tabnet: 0.76374 |  1:02:02s\n",
      "epoch 14 | loss: 0.23534 | val_0_auc: 0.95645 | val_0_accuracy: 0.89781 | val_0_amex_tabnet: 0.77017 |  1:05:55s\n",
      "epoch 15 | loss: 0.23962 | val_0_auc: 0.95609 | val_0_accuracy: 0.89522 | val_0_amex_tabnet: 0.76953 |  1:09:41s\n",
      "epoch 16 | loss: 0.23574 | val_0_auc: 0.95768 | val_0_accuracy: 0.89838 | val_0_amex_tabnet: 0.77527 |  1:13:19s\n",
      "epoch 17 | loss: 0.23221 | val_0_auc: 0.95813 | val_0_accuracy: 0.89925 | val_0_amex_tabnet: 0.77693 |  1:16:54s\n",
      "epoch 18 | loss: 0.2292  | val_0_auc: 0.95875 | val_0_accuracy: 0.89954 | val_0_amex_tabnet: 0.77971 |  1:20:38s\n",
      "epoch 19 | loss: 0.22574 | val_0_auc: 0.95932 | val_0_accuracy: 0.90065 | val_0_amex_tabnet: 0.7813  |  1:24:46s\n",
      "epoch 20 | loss: 0.23172 | val_0_auc: 0.95867 | val_0_accuracy: 0.90008 | val_0_amex_tabnet: 0.77882 |  1:28:47s\n",
      "epoch 21 | loss: 0.2298  | val_0_auc: 0.95893 | val_0_accuracy: 0.89969 | val_0_amex_tabnet: 0.77976 |  1:32:31s\n",
      "epoch 22 | loss: 0.22815 | val_0_auc: 0.95904 | val_0_accuracy: 0.90048 | val_0_amex_tabnet: 0.7801  |  1:36:18s\n",
      "epoch 23 | loss: 0.22604 | val_0_auc: 0.95945 | val_0_accuracy: 0.90099 | val_0_amex_tabnet: 0.78062 |  1:39:50s\n",
      "epoch 24 | loss: 0.22383 | val_0_auc: 0.95982 | val_0_accuracy: 0.9014  | val_0_amex_tabnet: 0.78336 |  1:43:16s\n",
      "epoch 25 | loss: 0.22855 | val_0_auc: 0.95905 | val_0_accuracy: 0.90031 | val_0_amex_tabnet: 0.77858 |  1:47:02s\n",
      "epoch 26 | loss: 0.22738 | val_0_auc: 0.95971 | val_0_accuracy: 0.90116 | val_0_amex_tabnet: 0.78241 |  1:50:28s\n",
      "epoch 27 | loss: 0.22622 | val_0_auc: 0.95938 | val_0_accuracy: 0.90103 | val_0_amex_tabnet: 0.78227 |  1:54:17s\n",
      "epoch 28 | loss: 0.22429 | val_0_auc: 0.95991 | val_0_accuracy: 0.90128 | val_0_amex_tabnet: 0.78366 |  1:58:18s\n",
      "epoch 29 | loss: 0.22277 | val_0_auc: 0.96004 | val_0_accuracy: 0.90172 | val_0_amex_tabnet: 0.78552 |  2:02:33s\n",
      "epoch 30 | loss: 0.22718 | val_0_auc: 0.95931 | val_0_accuracy: 0.90111 | val_0_amex_tabnet: 0.78088 |  2:06:49s\n",
      "epoch 31 | loss: 0.22592 | val_0_auc: 0.95908 | val_0_accuracy: 0.89986 | val_0_amex_tabnet: 0.78333 |  2:11:13s\n",
      "epoch 32 | loss: 0.22452 | val_0_auc: 0.95946 | val_0_accuracy: 0.90119 | val_0_amex_tabnet: 0.7831  |  2:15:36s\n",
      "epoch 33 | loss: 0.22277 | val_0_auc: 0.96021 | val_0_accuracy: 0.90192 | val_0_amex_tabnet: 0.78606 |  2:19:52s\n",
      "epoch 34 | loss: 0.22133 | val_0_auc: 0.96033 | val_0_accuracy: 0.90203 | val_0_amex_tabnet: 0.78646 |  2:23:57s\n",
      "epoch 35 | loss: 0.22642 | val_0_auc: 0.95961 | val_0_accuracy: 0.90051 | val_0_amex_tabnet: 0.78354 |  2:28:02s\n",
      "epoch 36 | loss: 0.22501 | val_0_auc: 0.9594  | val_0_accuracy: 0.9007  | val_0_amex_tabnet: 0.78181 |  2:32:04s\n",
      "epoch 37 | loss: 0.22356 | val_0_auc: 0.95999 | val_0_accuracy: 0.90194 | val_0_amex_tabnet: 0.78561 |  2:35:55s\n",
      "epoch 38 | loss: 0.22203 | val_0_auc: 0.96012 | val_0_accuracy: 0.90193 | val_0_amex_tabnet: 0.78507 |  2:39:39s\n",
      "epoch 39 | loss: 0.22018 | val_0_auc: 0.96034 | val_0_accuracy: 0.90272 | val_0_amex_tabnet: 0.78498 |  2:43:20s\n",
      "epoch 40 | loss: 0.22501 | val_0_auc: 0.95974 | val_0_accuracy: 0.90239 | val_0_amex_tabnet: 0.78278 |  2:47:03s\n",
      "epoch 41 | loss: 0.22426 | val_0_auc: 0.95988 | val_0_accuracy: 0.90103 | val_0_amex_tabnet: 0.78348 |  2:50:32s\n",
      "epoch 42 | loss: 0.22332 | val_0_auc: 0.96018 | val_0_accuracy: 0.90126 | val_0_amex_tabnet: 0.78606 |  2:54:04s\n",
      "epoch 43 | loss: 0.22187 | val_0_auc: 0.96034 | val_0_accuracy: 0.90272 | val_0_amex_tabnet: 0.78527 |  2:57:46s\n",
      "epoch 44 | loss: 0.22021 | val_0_auc: 0.96039 | val_0_accuracy: 0.90257 | val_0_amex_tabnet: 0.78629 |  3:01:17s\n",
      "epoch 45 | loss: 0.22536 | val_0_auc: 0.96002 | val_0_accuracy: 0.90072 | val_0_amex_tabnet: 0.78566 |  3:05:03s\n",
      "epoch 46 | loss: 0.22389 | val_0_auc: 0.96001 | val_0_accuracy: 0.90103 | val_0_amex_tabnet: 0.78378 |  3:08:38s\n",
      "epoch 47 | loss: 0.22311 | val_0_auc: 0.95998 | val_0_accuracy: 0.90147 | val_0_amex_tabnet: 0.78542 |  3:12:06s\n",
      "epoch 48 | loss: 0.22151 | val_0_auc: 0.96028 | val_0_accuracy: 0.90293 | val_0_amex_tabnet: 0.7861  |  3:15:53s\n",
      "epoch 49 | loss: 0.21982 | val_0_auc: 0.96049 | val_0_accuracy: 0.90312 | val_0_amex_tabnet: 0.78732 |  3:19:28s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_amex_tabnet = 0.78732\n",
      "save fold4 model\n",
      "Successfully saved model at ../output/exp08 Tabnet lag feature/tabnet_fold4.zip\n",
      "preds :  458913\n"
     ]
    }
   ],
   "source": [
    "# Get feature list\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "# all_data = features.extend(\"customer_ID\")\n",
    "\n",
    "\n",
    "train.fillna(value=0, inplace=True)\n",
    "test.fillna(value=0, inplace=True)\n",
    "\n",
    "## infを含むデータを外れ値（－１００００）に置換\n",
    "train = train.replace([np.inf, -np.inf],0)\n",
    "test = test.replace([np.inf, -np.inf],0)\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "\n",
    "# cid = []\n",
    "# cids = []\n",
    "\n",
    "preds = []\n",
    "cids = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    \n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "#     cid = train[\"customer_ID\"].loc[val_ind]\n",
    "    cid = train[\"customer_ID\"].loc[val_ind]\n",
    "    cids.extend(cid)\n",
    "    # lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    # lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "\n",
    "    \n",
    "    model = TabNetClassifier(n_d = 32,\n",
    "                         n_a = 32,\n",
    "                         n_steps = 3,\n",
    "                         gamma = 1.3,\n",
    "                         n_independent = 2,\n",
    "                         n_shared = 2,\n",
    "                         momentum = 0.02,\n",
    "                         clip_value = None,\n",
    "                         lambda_sparse = 1e-3,\n",
    "                         optimizer_fn = torch.optim.Adam,\n",
    "                         optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
    "                         scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                         scheduler_params = {'T_0':5,\n",
    "                                             'eta_min':1e-4,\n",
    "                                             'T_mult':1,\n",
    "                                             'last_epoch':-1},\n",
    "                         mask_type = 'sparsemax',\n",
    "                         seed = CFG.seed)\n",
    "\n",
    "    print(f'start Training fold {fold}')\n",
    "    model.fit(np.array(x_train),\n",
    "                    np.array(y_train.values.ravel()),\n",
    "                    eval_set = [(np.array(x_val), np.array(y_val.values.ravel()))],\n",
    "                    max_epochs = 1,#50\n",
    "                    patience = 1,#50\n",
    "                    batch_size = 512,\n",
    "                    eval_metric = ['auc', 'accuracy', Amex_tabnet]) # Last metric is used for early stoppin\n",
    "    \n",
    "    print(f'start Training fold {fold}')\n",
    "    file_path = f\"{CFG.output_dir}{CFG.model}_fold{fold}\"\n",
    "    saved_filepath = model.save_model(file_path)\n",
    "    \n",
    "#     oof_predictions[valid_idx] = model.predict_proba(x_val.values)[:, 1]\n",
    "    preds.extend(model.predict_proba(np.array(x_val[features]))[:, 1])\n",
    "    \n",
    "#     preds.extend(oof_predictions)\n",
    "    print(\"preds : \",len(preds))\n",
    "\n",
    "    # Add to out of folds array\n",
    "#     oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    # OOF用のcustomer_IDをExtend\n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    \n",
    "    # test predct\n",
    "    pred = model.predict_proba(np.array(test[features]))[:, 1]\n",
    "    preds.append(pred)\n",
    "#     print(f'OOF score across folds: {amex_metric_numpy(target, oof_predictions.flatten())}')\n",
    "\n",
    "#     # Compute fold metric\n",
    "    del x_train, x_val, y_train, y_val\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# # Compute out of folds metric\n",
    "# score = amex_metric(train[CFG.target], preds)\n",
    "# print(f'Our out of folds CV score is {score}')\n",
    "# test_predictions = np.mean(preds,axis = 0)\n",
    "\n",
    "# dic_oof = {\n",
    "#     \"customer_ID\":cids,\n",
    "#     \"target\":train[CFG.target],\n",
    "#     \"tabnet_oot\":oof_predictions\n",
    "# }\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame(dic_oof)\n",
    "# # oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.020569053477342766\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute out of folds metric\n",
    "test_predictions = np.mean(test_preds,axis = 0)\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "score = amex_metric(train[CFG.target], preds)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":train[CFG.target],\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # oof_predictions[valid_idx] = model.predict_proba(x_val.values)[:, 1]\n",
    "# preds.extend(model.predict_proba(x_val.values)[:, 1])\n",
    "# #     print(\"cid : \",len(cid))\n",
    "# print(\"preds : \",len(preds))\n",
    "\n",
    "# # Add to out of folds array\n",
    "# # oof_predictions[valid_idx] = val_pred\n",
    "\n",
    "# # OOF用のcustomer_IDをExtend\n",
    "# cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "\n",
    "# # test predct\n",
    "# test_predictions += model.predict_proba(test.values)[:, 1]/5\n",
    "# print(f'OOF score across folds: {amex_metric_numpy(target, oof_predictions.flatten())}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
