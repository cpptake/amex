{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp43\n",
    "\n",
    "lag_diff„ÅÆXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979ad950-7ef8-4116-97b7-0081786d9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25092\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25092\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp43_xgb_lagdiff_c3_hypar/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"xgb\"\n",
    "    ver =\"exp43\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "train = pd.read_parquet('../feature/exp38_lagdiff_c3/train_lagdiff_c3.parquet')\n",
    "test = pd.read_parquet('../feature/exp38_lagdiff_c3/test_lagdiff_c3.parquet')\n",
    "\n",
    "\n",
    "# OneDrive/„Éá„Çπ„ÇØ„Éà„ÉÉ„Éó/code/AMEX/feature/exp38_lagdiff_c3/test_lagdiff_c3.parquet\n",
    "\n",
    "# train = pd.read_parquet('../feature/exp35_lagdiff/train_lagdiff.parquet')\n",
    "# test = pd.read_parquet('../feature/exp35_lagdiff/test_lagdiff.parquet')\n",
    "\n",
    "# train, test = read_data()\n",
    "\n",
    "# train_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/train_c3.pkl')\n",
    "# test_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/test_c3.pkl')\n",
    "\n",
    "# train = train.merge(train_c3,on = \"customer_ID\",how = \"left\")\n",
    "# test = test.merge(test_c3,on = \"customer_ID\",how = \"left\")\n",
    "\n",
    "# del train_c3,test_c3\n",
    "# gc.collect\n",
    "\n",
    "# print(train.shape)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d173dd-7490-4e18-83e2-0f40ffdf7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.fillna(value=0, inplace=True)\n",
    "test.fillna(value=0, inplace=True)\n",
    "\n",
    "## inf„ÇíÂê´„ÇÄ„Éá„Éº„Çø„ÇíÂ§ñ„ÇåÂÄ§ÔºàÔºçÔºëÔºêÔºêÔºêÔºêÔºâ„Å´ÁΩÆÊèõ\n",
    "train = train.replace([np.inf, -np.inf],100000000000)\n",
    "test = test.replace([np.inf, -np.inf],100000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2011 features...\n",
      "[08:51:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68729\ttrain-amex:0.71588\tvalid-logloss:0.68729\tvalid-amex:0.71084\n",
      "[100]\ttrain-logloss:0.37777\ttrain-amex:0.77306\tvalid-logloss:0.38068\tvalid-amex:0.76193\n",
      "[200]\ttrain-logloss:0.28027\ttrain-amex:0.78488\tvalid-logloss:0.28607\tvalid-amex:0.76953\n",
      "[300]\ttrain-logloss:0.24195\ttrain-amex:0.79331\tvalid-logloss:0.25034\tvalid-amex:0.77583\n",
      "[400]\ttrain-logloss:0.22462\ttrain-amex:0.80104\tvalid-logloss:0.23532\tvalid-amex:0.77896\n",
      "[500]\ttrain-logloss:0.21543\ttrain-amex:0.80762\tvalid-logloss:0.22818\tvalid-amex:0.78279\n",
      "[600]\ttrain-logloss:0.20966\ttrain-amex:0.81364\tvalid-logloss:0.22441\tvalid-amex:0.78583\n",
      "[700]\ttrain-logloss:0.20519\ttrain-amex:0.81919\tvalid-logloss:0.22205\tvalid-amex:0.78918\n",
      "[800]\ttrain-logloss:0.20140\ttrain-amex:0.82439\tvalid-logloss:0.22044\tvalid-amex:0.79116\n",
      "[900]\ttrain-logloss:0.19807\ttrain-amex:0.82927\tvalid-logloss:0.21919\tvalid-amex:0.79285\n",
      "[1000]\ttrain-logloss:0.19503\ttrain-amex:0.83424\tvalid-logloss:0.21827\tvalid-amex:0.79364\n",
      "[1100]\ttrain-logloss:0.19234\ttrain-amex:0.83843\tvalid-logloss:0.21751\tvalid-amex:0.79427\n",
      "[1200]\ttrain-logloss:0.18986\ttrain-amex:0.84231\tvalid-logloss:0.21689\tvalid-amex:0.79523\n",
      "[1300]\ttrain-logloss:0.18743\ttrain-amex:0.84661\tvalid-logloss:0.21638\tvalid-amex:0.79639\n",
      "[1400]\ttrain-logloss:0.18524\ttrain-amex:0.85018\tvalid-logloss:0.21596\tvalid-amex:0.79691\n",
      "[1500]\ttrain-logloss:0.18315\ttrain-amex:0.85341\tvalid-logloss:0.21561\tvalid-amex:0.79774\n",
      "[1600]\ttrain-logloss:0.18119\ttrain-amex:0.85699\tvalid-logloss:0.21531\tvalid-amex:0.79797\n",
      "[1700]\ttrain-logloss:0.17931\ttrain-amex:0.86050\tvalid-logloss:0.21505\tvalid-amex:0.79833\n",
      "[1800]\ttrain-logloss:0.17752\ttrain-amex:0.86344\tvalid-logloss:0.21483\tvalid-amex:0.79857\n",
      "[1900]\ttrain-logloss:0.17583\ttrain-amex:0.86668\tvalid-logloss:0.21464\tvalid-amex:0.79900\n",
      "[2000]\ttrain-logloss:0.17400\ttrain-amex:0.86987\tvalid-logloss:0.21447\tvalid-amex:0.79920\n",
      "[2100]\ttrain-logloss:0.17230\ttrain-amex:0.87296\tvalid-logloss:0.21431\tvalid-amex:0.79959\n",
      "[2200]\ttrain-logloss:0.17059\ttrain-amex:0.87594\tvalid-logloss:0.21418\tvalid-amex:0.79959\n",
      "[2300]\ttrain-logloss:0.16899\ttrain-amex:0.87894\tvalid-logloss:0.21406\tvalid-amex:0.79982\n",
      "[2400]\ttrain-logloss:0.16738\ttrain-amex:0.88177\tvalid-logloss:0.21395\tvalid-amex:0.79973\n",
      "[2500]\ttrain-logloss:0.16579\ttrain-amex:0.88482\tvalid-logloss:0.21384\tvalid-amex:0.80015\n",
      "[2600]\ttrain-logloss:0.16419\ttrain-amex:0.88789\tvalid-logloss:0.21377\tvalid-amex:0.80035\n",
      "[2700]\ttrain-logloss:0.16275\ttrain-amex:0.89048\tvalid-logloss:0.21370\tvalid-amex:0.79990\n",
      "[2800]\ttrain-logloss:0.16122\ttrain-amex:0.89305\tvalid-logloss:0.21363\tvalid-amex:0.80035\n",
      "[2900]\ttrain-logloss:0.15972\ttrain-amex:0.89552\tvalid-logloss:0.21355\tvalid-amex:0.80021\n",
      "[3000]\ttrain-logloss:0.15822\ttrain-amex:0.89815\tvalid-logloss:0.21349\tvalid-amex:0.80027\n",
      "[3100]\ttrain-logloss:0.15687\ttrain-amex:0.90030\tvalid-logloss:0.21343\tvalid-amex:0.80055\n",
      "[3200]\ttrain-logloss:0.15540\ttrain-amex:0.90311\tvalid-logloss:0.21339\tvalid-amex:0.80060\n",
      "[3300]\ttrain-logloss:0.15398\ttrain-amex:0.90576\tvalid-logloss:0.21334\tvalid-amex:0.80085\n",
      "[3400]\ttrain-logloss:0.15255\ttrain-amex:0.90830\tvalid-logloss:0.21329\tvalid-amex:0.80108\n",
      "[3500]\ttrain-logloss:0.15122\ttrain-amex:0.91048\tvalid-logloss:0.21325\tvalid-amex:0.80139\n",
      "[3600]\ttrain-logloss:0.14986\ttrain-amex:0.91276\tvalid-logloss:0.21321\tvalid-amex:0.80128\n",
      "[3700]\ttrain-logloss:0.14855\ttrain-amex:0.91499\tvalid-logloss:0.21315\tvalid-amex:0.80155\n",
      "[3800]\ttrain-logloss:0.14721\ttrain-amex:0.91739\tvalid-logloss:0.21310\tvalid-amex:0.80104\n",
      "[3900]\ttrain-logloss:0.14589\ttrain-amex:0.91965\tvalid-logloss:0.21307\tvalid-amex:0.80139\n",
      "[4000]\ttrain-logloss:0.14458\ttrain-amex:0.92162\tvalid-logloss:0.21305\tvalid-amex:0.80153\n",
      "[4100]\ttrain-logloss:0.14327\ttrain-amex:0.92366\tvalid-logloss:0.21303\tvalid-amex:0.80153\n",
      "[4200]\ttrain-logloss:0.14198\ttrain-amex:0.92582\tvalid-logloss:0.21301\tvalid-amex:0.80186\n",
      "[4300]\ttrain-logloss:0.14072\ttrain-amex:0.92797\tvalid-logloss:0.21298\tvalid-amex:0.80202\n",
      "[4400]\ttrain-logloss:0.13941\ttrain-amex:0.93011\tvalid-logloss:0.21297\tvalid-amex:0.80213\n",
      "[4500]\ttrain-logloss:0.13811\ttrain-amex:0.93227\tvalid-logloss:0.21293\tvalid-amex:0.80208\n",
      "[4600]\ttrain-logloss:0.13695\ttrain-amex:0.93407\tvalid-logloss:0.21291\tvalid-amex:0.80196\n",
      "[4700]\ttrain-logloss:0.13563\ttrain-amex:0.93620\tvalid-logloss:0.21291\tvalid-amex:0.80209\n",
      "[4800]\ttrain-logloss:0.13447\ttrain-amex:0.93780\tvalid-logloss:0.21292\tvalid-amex:0.80148\n",
      "[4848]\ttrain-logloss:0.13391\ttrain-amex:0.93863\tvalid-logloss:0.21290\tvalid-amex:0.80176\n",
      "Our fold 0 CV score is 0.8016302040643505\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2011 features...\n",
      "[11:29:05] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68724\ttrain-amex:0.71575\tvalid-logloss:0.68723\tvalid-amex:0.70868\n",
      "[100]\ttrain-logloss:0.37749\ttrain-amex:0.77487\tvalid-logloss:0.38149\tvalid-amex:0.75388\n",
      "[200]\ttrain-logloss:0.27975\ttrain-amex:0.78713\tvalid-logloss:0.28756\tvalid-amex:0.76083\n",
      "[300]\ttrain-logloss:0.24130\ttrain-amex:0.79560\tvalid-logloss:0.25226\tvalid-amex:0.76821\n",
      "[400]\ttrain-logloss:0.22392\ttrain-amex:0.80301\tvalid-logloss:0.23761\tvalid-amex:0.77218\n",
      "[500]\ttrain-logloss:0.21471\ttrain-amex:0.80944\tvalid-logloss:0.23083\tvalid-amex:0.77526\n",
      "[600]\ttrain-logloss:0.20894\ttrain-amex:0.81554\tvalid-logloss:0.22726\tvalid-amex:0.77901\n",
      "[700]\ttrain-logloss:0.20442\ttrain-amex:0.82084\tvalid-logloss:0.22506\tvalid-amex:0.78169\n",
      "[800]\ttrain-logloss:0.20051\ttrain-amex:0.82644\tvalid-logloss:0.22349\tvalid-amex:0.78312\n",
      "[900]\ttrain-logloss:0.19716\ttrain-amex:0.83136\tvalid-logloss:0.22241\tvalid-amex:0.78478\n",
      "[1000]\ttrain-logloss:0.19426\ttrain-amex:0.83578\tvalid-logloss:0.22158\tvalid-amex:0.78583\n",
      "[1100]\ttrain-logloss:0.19166\ttrain-amex:0.84008\tvalid-logloss:0.22091\tvalid-amex:0.78644\n",
      "[1200]\ttrain-logloss:0.18912\ttrain-amex:0.84365\tvalid-logloss:0.22038\tvalid-amex:0.78739\n",
      "[1300]\ttrain-logloss:0.18675\ttrain-amex:0.84742\tvalid-logloss:0.21993\tvalid-amex:0.78761\n",
      "[1400]\ttrain-logloss:0.18457\ttrain-amex:0.85130\tvalid-logloss:0.21956\tvalid-amex:0.78788\n",
      "[1500]\ttrain-logloss:0.18246\ttrain-amex:0.85492\tvalid-logloss:0.21926\tvalid-amex:0.78816\n",
      "[1600]\ttrain-logloss:0.18046\ttrain-amex:0.85817\tvalid-logloss:0.21901\tvalid-amex:0.78878\n",
      "[1700]\ttrain-logloss:0.17838\ttrain-amex:0.86177\tvalid-logloss:0.21879\tvalid-amex:0.78934\n",
      "[1800]\ttrain-logloss:0.17648\ttrain-amex:0.86536\tvalid-logloss:0.21859\tvalid-amex:0.78992\n",
      "[1900]\ttrain-logloss:0.17464\ttrain-amex:0.86851\tvalid-logloss:0.21845\tvalid-amex:0.79020\n",
      "[2000]\ttrain-logloss:0.17297\ttrain-amex:0.87163\tvalid-logloss:0.21831\tvalid-amex:0.79078\n",
      "[2100]\ttrain-logloss:0.17120\ttrain-amex:0.87479\tvalid-logloss:0.21819\tvalid-amex:0.79059\n",
      "[2200]\ttrain-logloss:0.16942\ttrain-amex:0.87781\tvalid-logloss:0.21808\tvalid-amex:0.79051\n",
      "[2300]\ttrain-logloss:0.16775\ttrain-amex:0.88106\tvalid-logloss:0.21797\tvalid-amex:0.79046\n",
      "[2400]\ttrain-logloss:0.16608\ttrain-amex:0.88406\tvalid-logloss:0.21786\tvalid-amex:0.79057\n",
      "[2500]\ttrain-logloss:0.16452\ttrain-amex:0.88685\tvalid-logloss:0.21776\tvalid-amex:0.79075\n",
      "[2553]\ttrain-logloss:0.16371\ttrain-amex:0.88816\tvalid-logloss:0.21773\tvalid-amex:0.79051\n",
      "Our fold 1 CV score is 0.7903123032350816\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2011 features...\n",
      "[12:54:55] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68726\ttrain-amex:0.71614\tvalid-logloss:0.68723\tvalid-amex:0.70829\n",
      "[100]\ttrain-logloss:0.37733\ttrain-amex:0.77412\tvalid-logloss:0.38187\tvalid-amex:0.75841\n",
      "[200]\ttrain-logloss:0.27967\ttrain-amex:0.78611\tvalid-logloss:0.28795\tvalid-amex:0.76619\n",
      "[300]\ttrain-logloss:0.24121\ttrain-amex:0.79474\tvalid-logloss:0.25252\tvalid-amex:0.77227\n",
      "[400]\ttrain-logloss:0.22393\ttrain-amex:0.80228\tvalid-logloss:0.23774\tvalid-amex:0.77662\n",
      "[500]\ttrain-logloss:0.21471\ttrain-amex:0.80944\tvalid-logloss:0.23079\tvalid-amex:0.78007\n",
      "[600]\ttrain-logloss:0.20894\ttrain-amex:0.81498\tvalid-logloss:0.22710\tvalid-amex:0.78294\n",
      "[700]\ttrain-logloss:0.20443\ttrain-amex:0.82025\tvalid-logloss:0.22479\tvalid-amex:0.78560\n",
      "[800]\ttrain-logloss:0.20052\ttrain-amex:0.82577\tvalid-logloss:0.22318\tvalid-amex:0.78808\n",
      "[900]\ttrain-logloss:0.19720\ttrain-amex:0.83065\tvalid-logloss:0.22204\tvalid-amex:0.78912\n",
      "[1000]\ttrain-logloss:0.19424\ttrain-amex:0.83568\tvalid-logloss:0.22115\tvalid-amex:0.79082\n",
      "[1100]\ttrain-logloss:0.19147\ttrain-amex:0.83994\tvalid-logloss:0.22039\tvalid-amex:0.79164\n",
      "[1200]\ttrain-logloss:0.18890\ttrain-amex:0.84410\tvalid-logloss:0.21980\tvalid-amex:0.79220\n",
      "[1300]\ttrain-logloss:0.18662\ttrain-amex:0.84795\tvalid-logloss:0.21935\tvalid-amex:0.79239\n",
      "[1400]\ttrain-logloss:0.18443\ttrain-amex:0.85168\tvalid-logloss:0.21897\tvalid-amex:0.79311\n",
      "[1500]\ttrain-logloss:0.18228\ttrain-amex:0.85551\tvalid-logloss:0.21863\tvalid-amex:0.79342\n",
      "[1600]\ttrain-logloss:0.18024\ttrain-amex:0.85908\tvalid-logloss:0.21835\tvalid-amex:0.79387\n",
      "[1700]\ttrain-logloss:0.17835\ttrain-amex:0.86233\tvalid-logloss:0.21810\tvalid-amex:0.79417\n",
      "[1800]\ttrain-logloss:0.17649\ttrain-amex:0.86555\tvalid-logloss:0.21786\tvalid-amex:0.79439\n",
      "[1900]\ttrain-logloss:0.17471\ttrain-amex:0.86897\tvalid-logloss:0.21768\tvalid-amex:0.79475\n",
      "[2000]\ttrain-logloss:0.17291\ttrain-amex:0.87202\tvalid-logloss:0.21751\tvalid-amex:0.79483\n",
      "[2100]\ttrain-logloss:0.17116\ttrain-amex:0.87524\tvalid-logloss:0.21737\tvalid-amex:0.79520\n",
      "[2200]\ttrain-logloss:0.16938\ttrain-amex:0.87838\tvalid-logloss:0.21725\tvalid-amex:0.79564\n",
      "[2300]\ttrain-logloss:0.16773\ttrain-amex:0.88151\tvalid-logloss:0.21715\tvalid-amex:0.79633\n",
      "[2400]\ttrain-logloss:0.16605\ttrain-amex:0.88468\tvalid-logloss:0.21706\tvalid-amex:0.79623\n",
      "[2500]\ttrain-logloss:0.16451\ttrain-amex:0.88736\tvalid-logloss:0.21697\tvalid-amex:0.79656\n",
      "[2600]\ttrain-logloss:0.16296\ttrain-amex:0.89021\tvalid-logloss:0.21686\tvalid-amex:0.79664\n",
      "[2700]\ttrain-logloss:0.16137\ttrain-amex:0.89304\tvalid-logloss:0.21678\tvalid-amex:0.79660\n",
      "[2800]\ttrain-logloss:0.15987\ttrain-amex:0.89580\tvalid-logloss:0.21672\tvalid-amex:0.79665\n",
      "[2900]\ttrain-logloss:0.15834\ttrain-amex:0.89842\tvalid-logloss:0.21666\tvalid-amex:0.79665\n",
      "[3000]\ttrain-logloss:0.15677\ttrain-amex:0.90108\tvalid-logloss:0.21659\tvalid-amex:0.79690\n",
      "[3100]\ttrain-logloss:0.15520\ttrain-amex:0.90375\tvalid-logloss:0.21654\tvalid-amex:0.79634\n",
      "[3200]\ttrain-logloss:0.15387\ttrain-amex:0.90602\tvalid-logloss:0.21647\tvalid-amex:0.79638\n",
      "[3300]\ttrain-logloss:0.15251\ttrain-amex:0.90817\tvalid-logloss:0.21643\tvalid-amex:0.79682\n",
      "[3400]\ttrain-logloss:0.15101\ttrain-amex:0.91079\tvalid-logloss:0.21639\tvalid-amex:0.79675\n",
      "[3500]\ttrain-logloss:0.14955\ttrain-amex:0.91319\tvalid-logloss:0.21634\tvalid-amex:0.79721\n",
      "[3600]\ttrain-logloss:0.14814\ttrain-amex:0.91577\tvalid-logloss:0.21631\tvalid-amex:0.79754\n",
      "[3700]\ttrain-logloss:0.14677\ttrain-amex:0.91801\tvalid-logloss:0.21627\tvalid-amex:0.79713\n",
      "[3800]\ttrain-logloss:0.14547\ttrain-amex:0.92008\tvalid-logloss:0.21623\tvalid-amex:0.79746\n",
      "[3900]\ttrain-logloss:0.14418\ttrain-amex:0.92219\tvalid-logloss:0.21621\tvalid-amex:0.79787\n",
      "[4000]\ttrain-logloss:0.14288\ttrain-amex:0.92439\tvalid-logloss:0.21618\tvalid-amex:0.79766\n",
      "[4100]\ttrain-logloss:0.14158\ttrain-amex:0.92642\tvalid-logloss:0.21615\tvalid-amex:0.79822\n",
      "[4200]\ttrain-logloss:0.14019\ttrain-amex:0.92868\tvalid-logloss:0.21613\tvalid-amex:0.79800\n",
      "[4300]\ttrain-logloss:0.13882\ttrain-amex:0.93075\tvalid-logloss:0.21612\tvalid-amex:0.79762\n",
      "[4400]\ttrain-logloss:0.13768\ttrain-amex:0.93248\tvalid-logloss:0.21610\tvalid-amex:0.79784\n",
      "[4500]\ttrain-logloss:0.13646\ttrain-amex:0.93449\tvalid-logloss:0.21611\tvalid-amex:0.79797\n",
      "[4600]\ttrain-logloss:0.13522\ttrain-amex:0.93633\tvalid-logloss:0.21610\tvalid-amex:0.79810\n",
      "[4700]\ttrain-logloss:0.13407\ttrain-amex:0.93820\tvalid-logloss:0.21609\tvalid-amex:0.79773\n",
      "[4723]\ttrain-logloss:0.13383\ttrain-amex:0.93857\tvalid-logloss:0.21609\tvalid-amex:0.79781\n",
      "Our fold 2 CV score is 0.7975484620626645\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2011 features...\n",
      "[15:33:41] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68724\ttrain-amex:0.71600\tvalid-logloss:0.68726\tvalid-amex:0.70480\n",
      "[100]\ttrain-logloss:0.37716\ttrain-amex:0.77484\tvalid-logloss:0.38267\tvalid-amex:0.75441\n",
      "[200]\ttrain-logloss:0.27950\ttrain-amex:0.78657\tvalid-logloss:0.28893\tvalid-amex:0.76108\n",
      "[300]\ttrain-logloss:0.24108\ttrain-amex:0.79530\tvalid-logloss:0.25359\tvalid-amex:0.76663\n",
      "[400]\ttrain-logloss:0.22382\ttrain-amex:0.80282\tvalid-logloss:0.23885\tvalid-amex:0.77104\n",
      "[500]\ttrain-logloss:0.21452\ttrain-amex:0.80961\tvalid-logloss:0.23199\tvalid-amex:0.77512\n",
      "[600]\ttrain-logloss:0.20872\ttrain-amex:0.81521\tvalid-logloss:0.22838\tvalid-amex:0.77757\n",
      "[700]\ttrain-logloss:0.20428\ttrain-amex:0.82105\tvalid-logloss:0.22614\tvalid-amex:0.77949\n",
      "[800]\ttrain-logloss:0.20043\ttrain-amex:0.82674\tvalid-logloss:0.22459\tvalid-amex:0.78197\n",
      "[900]\ttrain-logloss:0.19713\ttrain-amex:0.83153\tvalid-logloss:0.22343\tvalid-amex:0.78290\n",
      "[1000]\ttrain-logloss:0.19410\ttrain-amex:0.83621\tvalid-logloss:0.22251\tvalid-amex:0.78429\n",
      "[1100]\ttrain-logloss:0.19139\ttrain-amex:0.84042\tvalid-logloss:0.22178\tvalid-amex:0.78532\n",
      "[1200]\ttrain-logloss:0.18893\ttrain-amex:0.84443\tvalid-logloss:0.22122\tvalid-amex:0.78589\n",
      "[1300]\ttrain-logloss:0.18665\ttrain-amex:0.84820\tvalid-logloss:0.22073\tvalid-amex:0.78663\n",
      "[1400]\ttrain-logloss:0.18430\ttrain-amex:0.85233\tvalid-logloss:0.22036\tvalid-amex:0.78722\n",
      "[1500]\ttrain-logloss:0.18220\ttrain-amex:0.85618\tvalid-logloss:0.22003\tvalid-amex:0.78741\n",
      "[1600]\ttrain-logloss:0.18012\ttrain-amex:0.85979\tvalid-logloss:0.21977\tvalid-amex:0.78748\n",
      "[1700]\ttrain-logloss:0.17809\ttrain-amex:0.86299\tvalid-logloss:0.21952\tvalid-amex:0.78799\n",
      "[1800]\ttrain-logloss:0.17617\ttrain-amex:0.86621\tvalid-logloss:0.21931\tvalid-amex:0.78855\n",
      "[1900]\ttrain-logloss:0.17435\ttrain-amex:0.86954\tvalid-logloss:0.21910\tvalid-amex:0.78883\n",
      "[2000]\ttrain-logloss:0.17263\ttrain-amex:0.87257\tvalid-logloss:0.21894\tvalid-amex:0.78911\n",
      "[2100]\ttrain-logloss:0.17099\ttrain-amex:0.87578\tvalid-logloss:0.21880\tvalid-amex:0.78924\n",
      "[2200]\ttrain-logloss:0.16926\ttrain-amex:0.87915\tvalid-logloss:0.21866\tvalid-amex:0.78965\n",
      "[2300]\ttrain-logloss:0.16762\ttrain-amex:0.88204\tvalid-logloss:0.21855\tvalid-amex:0.78957\n",
      "[2400]\ttrain-logloss:0.16603\ttrain-amex:0.88481\tvalid-logloss:0.21845\tvalid-amex:0.78933\n",
      "[2500]\ttrain-logloss:0.16439\ttrain-amex:0.88760\tvalid-logloss:0.21834\tvalid-amex:0.78994\n",
      "[2600]\ttrain-logloss:0.16271\ttrain-amex:0.89066\tvalid-logloss:0.21826\tvalid-amex:0.79001\n",
      "[2700]\ttrain-logloss:0.16115\ttrain-amex:0.89322\tvalid-logloss:0.21818\tvalid-amex:0.79013\n",
      "[2800]\ttrain-logloss:0.15959\ttrain-amex:0.89583\tvalid-logloss:0.21811\tvalid-amex:0.78978\n",
      "[2900]\ttrain-logloss:0.15808\ttrain-amex:0.89829\tvalid-logloss:0.21804\tvalid-amex:0.79016\n",
      "[3000]\ttrain-logloss:0.15654\ttrain-amex:0.90096\tvalid-logloss:0.21799\tvalid-amex:0.79050\n",
      "[3100]\ttrain-logloss:0.15494\ttrain-amex:0.90366\tvalid-logloss:0.21794\tvalid-amex:0.79064\n",
      "[3200]\ttrain-logloss:0.15350\ttrain-amex:0.90617\tvalid-logloss:0.21788\tvalid-amex:0.79060\n",
      "[3300]\ttrain-logloss:0.15212\ttrain-amex:0.90840\tvalid-logloss:0.21784\tvalid-amex:0.79007\n",
      "[3400]\ttrain-logloss:0.15067\ttrain-amex:0.91087\tvalid-logloss:0.21777\tvalid-amex:0.79021\n",
      "[3500]\ttrain-logloss:0.14927\ttrain-amex:0.91330\tvalid-logloss:0.21774\tvalid-amex:0.79060\n",
      "[3600]\ttrain-logloss:0.14785\ttrain-amex:0.91573\tvalid-logloss:0.21770\tvalid-amex:0.79044\n",
      "[3700]\ttrain-logloss:0.14657\ttrain-amex:0.91775\tvalid-logloss:0.21768\tvalid-amex:0.79069\n",
      "[3800]\ttrain-logloss:0.14521\ttrain-amex:0.91997\tvalid-logloss:0.21764\tvalid-amex:0.79058\n",
      "[3900]\ttrain-logloss:0.14385\ttrain-amex:0.92231\tvalid-logloss:0.21760\tvalid-amex:0.79078\n",
      "[4000]\ttrain-logloss:0.14248\ttrain-amex:0.92457\tvalid-logloss:0.21756\tvalid-amex:0.79082\n",
      "[4100]\ttrain-logloss:0.14117\ttrain-amex:0.92678\tvalid-logloss:0.21754\tvalid-amex:0.79085\n",
      "[4200]\ttrain-logloss:0.13978\ttrain-amex:0.92897\tvalid-logloss:0.21751\tvalid-amex:0.79158\n",
      "[4300]\ttrain-logloss:0.13856\ttrain-amex:0.93080\tvalid-logloss:0.21748\tvalid-amex:0.79172\n",
      "[4400]\ttrain-logloss:0.13728\ttrain-amex:0.93285\tvalid-logloss:0.21747\tvalid-amex:0.79142\n",
      "[4500]\ttrain-logloss:0.13611\ttrain-amex:0.93465\tvalid-logloss:0.21746\tvalid-amex:0.79121\n",
      "[4600]\ttrain-logloss:0.13491\ttrain-amex:0.93632\tvalid-logloss:0.21744\tvalid-amex:0.79127\n",
      "[4700]\ttrain-logloss:0.13363\ttrain-amex:0.93824\tvalid-logloss:0.21743\tvalid-amex:0.79140\n",
      "[4739]\ttrain-logloss:0.13312\ttrain-amex:0.93904\tvalid-logloss:0.21743\tvalid-amex:0.79141\n",
      "Our fold 3 CV score is 0.791214974327731\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2011 features...\n",
      "[18:09:02] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.68726\ttrain-amex:0.71428\tvalid-logloss:0.68725\tvalid-amex:0.70849\n",
      "[100]\ttrain-logloss:0.37763\ttrain-amex:0.77389\tvalid-logloss:0.38149\tvalid-amex:0.75733\n",
      "[200]\ttrain-logloss:0.28003\ttrain-amex:0.78577\tvalid-logloss:0.28710\tvalid-amex:0.76494\n",
      "[300]\ttrain-logloss:0.24167\ttrain-amex:0.79446\tvalid-logloss:0.25140\tvalid-amex:0.77223\n",
      "[400]\ttrain-logloss:0.22445\ttrain-amex:0.80198\tvalid-logloss:0.23644\tvalid-amex:0.77695\n",
      "[500]\ttrain-logloss:0.21528\ttrain-amex:0.80873\tvalid-logloss:0.22938\tvalid-amex:0.78145\n",
      "[600]\ttrain-logloss:0.20942\ttrain-amex:0.81473\tvalid-logloss:0.22558\tvalid-amex:0.78378\n",
      "[700]\ttrain-logloss:0.20486\ttrain-amex:0.82050\tvalid-logloss:0.22325\tvalid-amex:0.78531\n",
      "[800]\ttrain-logloss:0.20106\ttrain-amex:0.82573\tvalid-logloss:0.22165\tvalid-amex:0.78712\n",
      "[900]\ttrain-logloss:0.19776\ttrain-amex:0.83084\tvalid-logloss:0.22045\tvalid-amex:0.78762\n",
      "[1000]\ttrain-logloss:0.19478\ttrain-amex:0.83554\tvalid-logloss:0.21950\tvalid-amex:0.78889\n",
      "[1100]\ttrain-logloss:0.19212\ttrain-amex:0.83948\tvalid-logloss:0.21877\tvalid-amex:0.78969\n",
      "[1200]\ttrain-logloss:0.18967\ttrain-amex:0.84325\tvalid-logloss:0.21818\tvalid-amex:0.79017\n",
      "[1300]\ttrain-logloss:0.18728\ttrain-amex:0.84707\tvalid-logloss:0.21770\tvalid-amex:0.79094\n",
      "[1400]\ttrain-logloss:0.18500\ttrain-amex:0.85120\tvalid-logloss:0.21730\tvalid-amex:0.79175\n",
      "[1500]\ttrain-logloss:0.18288\ttrain-amex:0.85495\tvalid-logloss:0.21696\tvalid-amex:0.79188\n",
      "[1600]\ttrain-logloss:0.18097\ttrain-amex:0.85840\tvalid-logloss:0.21671\tvalid-amex:0.79219\n",
      "[1700]\ttrain-logloss:0.17912\ttrain-amex:0.86164\tvalid-logloss:0.21646\tvalid-amex:0.79297\n",
      "[1800]\ttrain-logloss:0.17725\ttrain-amex:0.86484\tvalid-logloss:0.21625\tvalid-amex:0.79292\n",
      "[1900]\ttrain-logloss:0.17538\ttrain-amex:0.86807\tvalid-logloss:0.21604\tvalid-amex:0.79319\n",
      "[2000]\ttrain-logloss:0.17357\ttrain-amex:0.87124\tvalid-logloss:0.21588\tvalid-amex:0.79365\n",
      "[2100]\ttrain-logloss:0.17180\ttrain-amex:0.87433\tvalid-logloss:0.21573\tvalid-amex:0.79366\n",
      "[2200]\ttrain-logloss:0.17007\ttrain-amex:0.87725\tvalid-logloss:0.21559\tvalid-amex:0.79434\n",
      "[2300]\ttrain-logloss:0.16845\ttrain-amex:0.88017\tvalid-logloss:0.21546\tvalid-amex:0.79409\n",
      "[2400]\ttrain-logloss:0.16681\ttrain-amex:0.88328\tvalid-logloss:0.21534\tvalid-amex:0.79466\n",
      "[2500]\ttrain-logloss:0.16523\ttrain-amex:0.88601\tvalid-logloss:0.21523\tvalid-amex:0.79480\n",
      "[2600]\ttrain-logloss:0.16368\ttrain-amex:0.88876\tvalid-logloss:0.21515\tvalid-amex:0.79481\n",
      "[2700]\ttrain-logloss:0.16209\ttrain-amex:0.89128\tvalid-logloss:0.21507\tvalid-amex:0.79479\n",
      "[2800]\ttrain-logloss:0.16059\ttrain-amex:0.89402\tvalid-logloss:0.21501\tvalid-amex:0.79486\n",
      "[2900]\ttrain-logloss:0.15901\ttrain-amex:0.89688\tvalid-logloss:0.21494\tvalid-amex:0.79509\n",
      "[3000]\ttrain-logloss:0.15751\ttrain-amex:0.89953\tvalid-logloss:0.21488\tvalid-amex:0.79476\n",
      "[3100]\ttrain-logloss:0.15608\ttrain-amex:0.90204\tvalid-logloss:0.21480\tvalid-amex:0.79533\n",
      "[3200]\ttrain-logloss:0.15462\ttrain-amex:0.90456\tvalid-logloss:0.21475\tvalid-amex:0.79564\n",
      "[3300]\ttrain-logloss:0.15319\ttrain-amex:0.90710\tvalid-logloss:0.21471\tvalid-amex:0.79576\n",
      "[3400]\ttrain-logloss:0.15181\ttrain-amex:0.90947\tvalid-logloss:0.21468\tvalid-amex:0.79567\n",
      "[3500]\ttrain-logloss:0.15038\ttrain-amex:0.91203\tvalid-logloss:0.21463\tvalid-amex:0.79560\n",
      "[3600]\ttrain-logloss:0.14911\ttrain-amex:0.91408\tvalid-logloss:0.21458\tvalid-amex:0.79604\n",
      "[3700]\ttrain-logloss:0.14780\ttrain-amex:0.91605\tvalid-logloss:0.21457\tvalid-amex:0.79603\n",
      "[3800]\ttrain-logloss:0.14652\ttrain-amex:0.91826\tvalid-logloss:0.21456\tvalid-amex:0.79573\n",
      "[3900]\ttrain-logloss:0.14512\ttrain-amex:0.92065\tvalid-logloss:0.21455\tvalid-amex:0.79607\n",
      "[4000]\ttrain-logloss:0.14377\ttrain-amex:0.92279\tvalid-logloss:0.21452\tvalid-amex:0.79602\n",
      "[4100]\ttrain-logloss:0.14240\ttrain-amex:0.92497\tvalid-logloss:0.21449\tvalid-amex:0.79603\n",
      "[4200]\ttrain-logloss:0.14115\ttrain-amex:0.92706\tvalid-logloss:0.21446\tvalid-amex:0.79611\n",
      "[4300]\ttrain-logloss:0.13988\ttrain-amex:0.92898\tvalid-logloss:0.21442\tvalid-amex:0.79648\n",
      "[4400]\ttrain-logloss:0.13846\ttrain-amex:0.93137\tvalid-logloss:0.21440\tvalid-amex:0.79615\n",
      "[4500]\ttrain-logloss:0.13725\ttrain-amex:0.93322\tvalid-logloss:0.21436\tvalid-amex:0.79644\n",
      "[4600]\ttrain-logloss:0.13609\ttrain-amex:0.93504\tvalid-logloss:0.21435\tvalid-amex:0.79603\n",
      "[4700]\ttrain-logloss:0.13488\ttrain-amex:0.93688\tvalid-logloss:0.21434\tvalid-amex:0.79601\n",
      "[4800]\ttrain-logloss:0.13365\ttrain-amex:0.93867\tvalid-logloss:0.21435\tvalid-amex:0.79609\n",
      "[4900]\ttrain-logloss:0.13243\ttrain-amex:0.94066\tvalid-logloss:0.21435\tvalid-amex:0.79612\n",
      "[5000]\ttrain-logloss:0.13131\ttrain-amex:0.94218\tvalid-logloss:0.21435\tvalid-amex:0.79608\n",
      "[5044]\ttrain-logloss:0.13083\ttrain-amex:0.94288\tvalid-logloss:0.21433\tvalid-amex:0.79623\n",
      "Our fold 4 CV score is 0.796042979216018\n",
      "Our out of folds CV score is 0.7956869117243499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "# for cat_col in cat_features:\n",
    "# #     print(cat_col)\n",
    "#     encoder = LabelEncoder()\n",
    "#     train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "#     test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "\n",
    "xgb_parameters={\n",
    "        'max_depth': 9,#7 # optuna 9\n",
    "        'eta': 0.03,\n",
    "        'subsample': 0.88,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'objective': 'binary:logistic',\n",
    "        'learning_rate' : 0.008971755769136095,#0.026582608139330333\n",
    "        'tree_method': 'hist',#gpu_hist\n",
    "        # 'predictor': 'gpu_predictor',\n",
    "        'random_state': 42,\n",
    "        'gamma': 1.5,\n",
    "        'min_child_weight': 19,#8 # optuna 19\n",
    "        'lambda': 70,\n",
    "    }\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "# epoch = [10000,7500,7500,8500,10500]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(data=x_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(data=test[features])\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "  \n",
    "    model = xgb.train(\n",
    "            xgb_parameters,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=9999,#9999\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            early_stopping_rounds=500,\n",
    "            feval=xgb_amex,\n",
    "            maximize=True,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Save best model\n",
    "    model.save_model(f'{CFG.output_dir}{CFG.model}_{CFG.ver}_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.xgb')\n",
    "#     joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(dvalid)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(dtest)\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, dtrain, dtest ,dvalid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeefba0-9b46-4227-b634-0a45793eb9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f92108-afb7-45f5-b3e1-706857cb9d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed9500-72c1-48b4-893d-bc1fa5d7a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762416b-52f6-4464-94ff-e76c6e7da87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1dca5-1edb-4513-82d0-b66ee1b4df81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e494e0-50f8-47ad-88c9-637a1b994414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
