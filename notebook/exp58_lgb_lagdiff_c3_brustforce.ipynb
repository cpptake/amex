{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diffのXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
    "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
    "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    train.to_pickle('train_fe_v3_loaded.pkl')\n",
    "    \n",
    "    del train_num_agg, train_cat_agg, train_diff, train\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    test = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    # test = pd.read_parquet('../input/AMEXdata-integerdtypes-parquetformat/test.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
    "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
    "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    \n",
    "    test.to_pickle('test_fe_v3_loaded.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a3cd6b-3b61-4ffe-97a6-09b11469f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73091e4f-b894-4ade-84f4-096c9c8fcb5a",
   "metadata": {},
   "source": [
    "# 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717b3c71-f7c0-4fdf-b27c-3e29b6cfa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_parquet('../feature/Bruteforce feature/train_fe_v3_loaded.parquet')\n",
    "# test = pd.read_parquet('../feature/Bruteforce feature/test_fe_v3_loaded.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a06a1c6-a4e6-4179-837e-8ecdd0a9a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 88\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    input_dir = '../input/amex-fe/'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_v3_loaded.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_v3_loaded.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b48aef-8cf8-4e27-8122-c17fdd55616f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/amex-fe/train_fe_v3_loaded.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21268\\2817187628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21268\\4265619844.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train_fe_v3_loaded.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'test_fe_v3_loaded.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/amex-fe/train_fe_v3_loaded.parquet'"
     ]
    }
   ],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp58_2_lgb_lagdiff_c3_brustforce/'\n",
    "    seed = 614\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp58\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 1639)\n",
      "(924621, 1638)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "train = pd.read_pickle('../feature/Bruteforce feature/train_fe_v3_loaded.pkl')\n",
    "test = pd.read_pickle('../feature/Bruteforce feature/test_fe_v3_loaded.pkl')\n",
    "\n",
    "# train, test = read_data()\n",
    "\n",
    "# train_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/train_c3.pkl')\n",
    "# test_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/test_c3.pkl')\n",
    "\n",
    "# train = train.merge(train_c3,on = \"customer_ID\",how = \"left\")\n",
    "# test = test.merge(test_c3,on = \"customer_ID\",how = \"left\")\n",
    "\n",
    "# del train_c3,test_c3\n",
    "# gc.collect\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.694231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 253272\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.489625\ttraining's amex_metric: 0.767536\tvalid_1's binary_logloss: 0.490699\tvalid_1's amex_metric: 0.758194\n",
      "[200]\ttraining's binary_logloss: 0.414389\ttraining's amex_metric: 0.771471\tvalid_1's binary_logloss: 0.416252\tvalid_1's amex_metric: 0.762258\n",
      "[300]\ttraining's binary_logloss: 0.36257\ttraining's amex_metric: 0.77603\tvalid_1's binary_logloss: 0.365294\tvalid_1's amex_metric: 0.76508\n",
      "[400]\ttraining's binary_logloss: 0.324624\ttraining's amex_metric: 0.780228\tvalid_1's binary_logloss: 0.328208\tvalid_1's amex_metric: 0.766954\n",
      "[500]\ttraining's binary_logloss: 0.304516\ttraining's amex_metric: 0.783189\tvalid_1's binary_logloss: 0.308702\tvalid_1's amex_metric: 0.769586\n",
      "[600]\ttraining's binary_logloss: 0.282951\ttraining's amex_metric: 0.786093\tvalid_1's binary_logloss: 0.287984\tvalid_1's amex_metric: 0.771036\n",
      "[700]\ttraining's binary_logloss: 0.272212\ttraining's amex_metric: 0.789095\tvalid_1's binary_logloss: 0.277812\tvalid_1's amex_metric: 0.773468\n",
      "[800]\ttraining's binary_logloss: 0.261498\ttraining's amex_metric: 0.792154\tvalid_1's binary_logloss: 0.267778\tvalid_1's amex_metric: 0.774766\n",
      "[900]\ttraining's binary_logloss: 0.2587\ttraining's amex_metric: 0.794275\tvalid_1's binary_logloss: 0.265295\tvalid_1's amex_metric: 0.77532\n",
      "[1000]\ttraining's binary_logloss: 0.250693\ttraining's amex_metric: 0.796463\tvalid_1's binary_logloss: 0.257892\tvalid_1's amex_metric: 0.776965\n",
      "[1100]\ttraining's binary_logloss: 0.24203\ttraining's amex_metric: 0.79886\tvalid_1's binary_logloss: 0.250046\tvalid_1's amex_metric: 0.777886\n",
      "[1200]\ttraining's binary_logloss: 0.236923\ttraining's amex_metric: 0.801156\tvalid_1's binary_logloss: 0.245588\tvalid_1's amex_metric: 0.779495\n",
      "[1300]\ttraining's binary_logloss: 0.230618\ttraining's amex_metric: 0.803555\tvalid_1's binary_logloss: 0.240213\tvalid_1's amex_metric: 0.780942\n",
      "[1400]\ttraining's binary_logloss: 0.22564\ttraining's amex_metric: 0.806869\tvalid_1's binary_logloss: 0.236197\tvalid_1's amex_metric: 0.782167\n",
      "[1500]\ttraining's binary_logloss: 0.221454\ttraining's amex_metric: 0.8094\tvalid_1's binary_logloss: 0.23297\tvalid_1's amex_metric: 0.78363\n",
      "[1600]\ttraining's binary_logloss: 0.220258\ttraining's amex_metric: 0.81137\tvalid_1's binary_logloss: 0.232285\tvalid_1's amex_metric: 0.784691\n",
      "[1700]\ttraining's binary_logloss: 0.21751\ttraining's amex_metric: 0.813425\tvalid_1's binary_logloss: 0.230372\tvalid_1's amex_metric: 0.785406\n",
      "[1800]\ttraining's binary_logloss: 0.215257\ttraining's amex_metric: 0.815722\tvalid_1's binary_logloss: 0.228877\tvalid_1's amex_metric: 0.786712\n",
      "[1900]\ttraining's binary_logloss: 0.211815\ttraining's amex_metric: 0.818156\tvalid_1's binary_logloss: 0.226625\tvalid_1's amex_metric: 0.787254\n",
      "[2000]\ttraining's binary_logloss: 0.209995\ttraining's amex_metric: 0.820391\tvalid_1's binary_logloss: 0.225629\tvalid_1's amex_metric: 0.788282\n",
      "[2100]\ttraining's binary_logloss: 0.208376\ttraining's amex_metric: 0.822643\tvalid_1's binary_logloss: 0.224824\tvalid_1's amex_metric: 0.78868\n",
      "[2200]\ttraining's binary_logloss: 0.206441\ttraining's amex_metric: 0.824903\tvalid_1's binary_logloss: 0.223847\tvalid_1's amex_metric: 0.789856\n",
      "[2300]\ttraining's binary_logloss: 0.204141\ttraining's amex_metric: 0.826953\tvalid_1's binary_logloss: 0.222705\tvalid_1's amex_metric: 0.790088\n",
      "[2400]\ttraining's binary_logloss: 0.201645\ttraining's amex_metric: 0.829542\tvalid_1's binary_logloss: 0.221572\tvalid_1's amex_metric: 0.790358\n",
      "[2500]\ttraining's binary_logloss: 0.200567\ttraining's amex_metric: 0.831853\tvalid_1's binary_logloss: 0.221284\tvalid_1's amex_metric: 0.790691\n",
      "[2600]\ttraining's binary_logloss: 0.199492\ttraining's amex_metric: 0.834267\tvalid_1's binary_logloss: 0.220999\tvalid_1's amex_metric: 0.791148\n",
      "[2700]\ttraining's binary_logloss: 0.197895\ttraining's amex_metric: 0.836447\tvalid_1's binary_logloss: 0.220443\tvalid_1's amex_metric: 0.791522\n",
      "[2800]\ttraining's binary_logloss: 0.196104\ttraining's amex_metric: 0.838806\tvalid_1's binary_logloss: 0.219851\tvalid_1's amex_metric: 0.792344\n",
      "[2900]\ttraining's binary_logloss: 0.195159\ttraining's amex_metric: 0.840708\tvalid_1's binary_logloss: 0.219636\tvalid_1's amex_metric: 0.792863\n",
      "[3000]\ttraining's binary_logloss: 0.194191\ttraining's amex_metric: 0.842975\tvalid_1's binary_logloss: 0.219415\tvalid_1's amex_metric: 0.793157\n",
      "[3100]\ttraining's binary_logloss: 0.192641\ttraining's amex_metric: 0.844963\tvalid_1's binary_logloss: 0.218972\tvalid_1's amex_metric: 0.794041\n",
      "[3200]\ttraining's binary_logloss: 0.191118\ttraining's amex_metric: 0.847087\tvalid_1's binary_logloss: 0.218574\tvalid_1's amex_metric: 0.794345\n",
      "[3300]\ttraining's binary_logloss: 0.189525\ttraining's amex_metric: 0.849286\tvalid_1's binary_logloss: 0.218135\tvalid_1's amex_metric: 0.795352\n",
      "[3400]\ttraining's binary_logloss: 0.188088\ttraining's amex_metric: 0.85144\tvalid_1's binary_logloss: 0.217817\tvalid_1's amex_metric: 0.796195\n",
      "[3500]\ttraining's binary_logloss: 0.186539\ttraining's amex_metric: 0.853787\tvalid_1's binary_logloss: 0.217465\tvalid_1's amex_metric: 0.796194\n",
      "[3600]\ttraining's binary_logloss: 0.185796\ttraining's amex_metric: 0.855917\tvalid_1's binary_logloss: 0.217413\tvalid_1's amex_metric: 0.796196\n",
      "[3700]\ttraining's binary_logloss: 0.18422\ttraining's amex_metric: 0.858002\tvalid_1's binary_logloss: 0.217106\tvalid_1's amex_metric: 0.796307\n",
      "[3800]\ttraining's binary_logloss: 0.1828\ttraining's amex_metric: 0.860169\tvalid_1's binary_logloss: 0.216837\tvalid_1's amex_metric: 0.796709\n",
      "[3900]\ttraining's binary_logloss: 0.181135\ttraining's amex_metric: 0.862705\tvalid_1's binary_logloss: 0.216596\tvalid_1's amex_metric: 0.796539\n",
      "[4000]\ttraining's binary_logloss: 0.180032\ttraining's amex_metric: 0.864596\tvalid_1's binary_logloss: 0.216453\tvalid_1's amex_metric: 0.7969\n",
      "[4100]\ttraining's binary_logloss: 0.178776\ttraining's amex_metric: 0.866791\tvalid_1's binary_logloss: 0.216292\tvalid_1's amex_metric: 0.796502\n",
      "[4200]\ttraining's binary_logloss: 0.177358\ttraining's amex_metric: 0.869448\tvalid_1's binary_logloss: 0.216056\tvalid_1's amex_metric: 0.797194\n",
      "[4300]\ttraining's binary_logloss: 0.176378\ttraining's amex_metric: 0.871479\tvalid_1's binary_logloss: 0.215985\tvalid_1's amex_metric: 0.797866\n",
      "[4400]\ttraining's binary_logloss: 0.175316\ttraining's amex_metric: 0.873401\tvalid_1's binary_logloss: 0.215869\tvalid_1's amex_metric: 0.797761\n",
      "[4500]\ttraining's binary_logloss: 0.17432\ttraining's amex_metric: 0.875468\tvalid_1's binary_logloss: 0.215787\tvalid_1's amex_metric: 0.797559\n",
      "[4600]\ttraining's binary_logloss: 0.173573\ttraining's amex_metric: 0.877142\tvalid_1's binary_logloss: 0.215764\tvalid_1's amex_metric: 0.797623\n",
      "[4700]\ttraining's binary_logloss: 0.172746\ttraining's amex_metric: 0.878759\tvalid_1's binary_logloss: 0.215707\tvalid_1's amex_metric: 0.797748\n",
      "[4800]\ttraining's binary_logloss: 0.171934\ttraining's amex_metric: 0.88044\tvalid_1's binary_logloss: 0.215653\tvalid_1's amex_metric: 0.798126\n",
      "[4900]\ttraining's binary_logloss: 0.17103\ttraining's amex_metric: 0.881913\tvalid_1's binary_logloss: 0.215567\tvalid_1's amex_metric: 0.798294\n",
      "[5000]\ttraining's binary_logloss: 0.170794\ttraining's amex_metric: 0.883267\tvalid_1's binary_logloss: 0.21565\tvalid_1's amex_metric: 0.798207\n",
      "[5100]\ttraining's binary_logloss: 0.169656\ttraining's amex_metric: 0.884246\tvalid_1's binary_logloss: 0.215518\tvalid_1's amex_metric: 0.797935\n",
      "[5200]\ttraining's binary_logloss: 0.168466\ttraining's amex_metric: 0.885646\tvalid_1's binary_logloss: 0.215359\tvalid_1's amex_metric: 0.797658\n",
      "[5300]\ttraining's binary_logloss: 0.167523\ttraining's amex_metric: 0.887314\tvalid_1's binary_logloss: 0.215286\tvalid_1's amex_metric: 0.797399\n",
      "[5400]\ttraining's binary_logloss: 0.166204\ttraining's amex_metric: 0.888801\tvalid_1's binary_logloss: 0.215115\tvalid_1's amex_metric: 0.797704\n",
      "[5500]\ttraining's binary_logloss: 0.165068\ttraining's amex_metric: 0.890874\tvalid_1's binary_logloss: 0.21501\tvalid_1's amex_metric: 0.79798\n",
      "[5600]\ttraining's binary_logloss: 0.16386\ttraining's amex_metric: 0.892562\tvalid_1's binary_logloss: 0.214889\tvalid_1's amex_metric: 0.797943\n",
      "[5700]\ttraining's binary_logloss: 0.162903\ttraining's amex_metric: 0.894157\tvalid_1's binary_logloss: 0.214847\tvalid_1's amex_metric: 0.798294\n",
      "[5800]\ttraining's binary_logloss: 0.161903\ttraining's amex_metric: 0.896032\tvalid_1's binary_logloss: 0.214788\tvalid_1's amex_metric: 0.798036\n",
      "[5900]\ttraining's binary_logloss: 0.160993\ttraining's amex_metric: 0.897693\tvalid_1's binary_logloss: 0.214742\tvalid_1's amex_metric: 0.798197\n",
      "[6000]\ttraining's binary_logloss: 0.160009\ttraining's amex_metric: 0.899282\tvalid_1's binary_logloss: 0.214675\tvalid_1's amex_metric: 0.798233\n",
      "[6100]\ttraining's binary_logloss: 0.159023\ttraining's amex_metric: 0.901039\tvalid_1's binary_logloss: 0.214645\tvalid_1's amex_metric: 0.798425\n",
      "[6200]\ttraining's binary_logloss: 0.15802\ttraining's amex_metric: 0.902674\tvalid_1's binary_logloss: 0.214568\tvalid_1's amex_metric: 0.798131\n",
      "[6300]\ttraining's binary_logloss: 0.15727\ttraining's amex_metric: 0.904366\tvalid_1's binary_logloss: 0.214518\tvalid_1's amex_metric: 0.79864\n",
      "[6400]\ttraining's binary_logloss: 0.156804\ttraining's amex_metric: 0.905466\tvalid_1's binary_logloss: 0.214528\tvalid_1's amex_metric: 0.798563\n",
      "[6500]\ttraining's binary_logloss: 0.155811\ttraining's amex_metric: 0.906985\tvalid_1's binary_logloss: 0.214472\tvalid_1's amex_metric: 0.798821\n",
      "[6600]\ttraining's binary_logloss: 0.155098\ttraining's amex_metric: 0.908373\tvalid_1's binary_logloss: 0.214446\tvalid_1's amex_metric: 0.798891\n",
      "[6700]\ttraining's binary_logloss: 0.154134\ttraining's amex_metric: 0.909751\tvalid_1's binary_logloss: 0.214382\tvalid_1's amex_metric: 0.798629\n",
      "[6800]\ttraining's binary_logloss: 0.153256\ttraining's amex_metric: 0.911119\tvalid_1's binary_logloss: 0.214336\tvalid_1's amex_metric: 0.798389\n",
      "[6900]\ttraining's binary_logloss: 0.152276\ttraining's amex_metric: 0.912743\tvalid_1's binary_logloss: 0.214294\tvalid_1's amex_metric: 0.799007\n",
      "[7000]\ttraining's binary_logloss: 0.151632\ttraining's amex_metric: 0.913813\tvalid_1's binary_logloss: 0.21428\tvalid_1's amex_metric: 0.799182\n",
      "[7100]\ttraining's binary_logloss: 0.150888\ttraining's amex_metric: 0.915162\tvalid_1's binary_logloss: 0.214246\tvalid_1's amex_metric: 0.798939\n",
      "[7200]\ttraining's binary_logloss: 0.150092\ttraining's amex_metric: 0.91669\tvalid_1's binary_logloss: 0.214241\tvalid_1's amex_metric: 0.799062\n",
      "[7300]\ttraining's binary_logloss: 0.149296\ttraining's amex_metric: 0.917998\tvalid_1's binary_logloss: 0.214182\tvalid_1's amex_metric: 0.799543\n",
      "[7400]\ttraining's binary_logloss: 0.148441\ttraining's amex_metric: 0.91943\tvalid_1's binary_logloss: 0.214124\tvalid_1's amex_metric: 0.799707\n",
      "[7500]\ttraining's binary_logloss: 0.147484\ttraining's amex_metric: 0.920806\tvalid_1's binary_logloss: 0.214081\tvalid_1's amex_metric: 0.799525\n",
      "[7600]\ttraining's binary_logloss: 0.146593\ttraining's amex_metric: 0.922063\tvalid_1's binary_logloss: 0.214088\tvalid_1's amex_metric: 0.79956\n",
      "Our fold 0 CV score is 0.7995595896186589\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.745642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 253446\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.489429\ttraining's amex_metric: 0.767502\tvalid_1's binary_logloss: 0.491076\tvalid_1's amex_metric: 0.75376\n",
      "[200]\ttraining's binary_logloss: 0.414013\ttraining's amex_metric: 0.771757\tvalid_1's binary_logloss: 0.416874\tvalid_1's amex_metric: 0.757698\n",
      "[300]\ttraining's binary_logloss: 0.362207\ttraining's amex_metric: 0.776213\tvalid_1's binary_logloss: 0.366145\tvalid_1's amex_metric: 0.760813\n",
      "[400]\ttraining's binary_logloss: 0.324307\ttraining's amex_metric: 0.780157\tvalid_1's binary_logloss: 0.329207\tvalid_1's amex_metric: 0.763283\n",
      "[500]\ttraining's binary_logloss: 0.304144\ttraining's amex_metric: 0.783203\tvalid_1's binary_logloss: 0.309723\tvalid_1's amex_metric: 0.76549\n",
      "[600]\ttraining's binary_logloss: 0.282657\ttraining's amex_metric: 0.786585\tvalid_1's binary_logloss: 0.289127\tvalid_1's amex_metric: 0.768161\n",
      "[700]\ttraining's binary_logloss: 0.271937\ttraining's amex_metric: 0.789336\tvalid_1's binary_logloss: 0.27904\tvalid_1's amex_metric: 0.770098\n",
      "[800]\ttraining's binary_logloss: 0.261202\ttraining's amex_metric: 0.792176\tvalid_1's binary_logloss: 0.269043\tvalid_1's amex_metric: 0.771522\n",
      "[900]\ttraining's binary_logloss: 0.25833\ttraining's amex_metric: 0.793954\tvalid_1's binary_logloss: 0.266511\tvalid_1's amex_metric: 0.772739\n",
      "[1000]\ttraining's binary_logloss: 0.250352\ttraining's amex_metric: 0.796539\tvalid_1's binary_logloss: 0.25923\tvalid_1's amex_metric: 0.773862\n",
      "[1100]\ttraining's binary_logloss: 0.241687\ttraining's amex_metric: 0.799189\tvalid_1's binary_logloss: 0.251441\tvalid_1's amex_metric: 0.775026\n",
      "[1200]\ttraining's binary_logloss: 0.236558\ttraining's amex_metric: 0.802065\tvalid_1's binary_logloss: 0.247044\tvalid_1's amex_metric: 0.776637\n",
      "[1300]\ttraining's binary_logloss: 0.230271\ttraining's amex_metric: 0.804927\tvalid_1's binary_logloss: 0.24172\tvalid_1's amex_metric: 0.778294\n",
      "[1400]\ttraining's binary_logloss: 0.225272\ttraining's amex_metric: 0.807374\tvalid_1's binary_logloss: 0.237698\tvalid_1's amex_metric: 0.779714\n",
      "[1500]\ttraining's binary_logloss: 0.221084\ttraining's amex_metric: 0.810248\tvalid_1's binary_logloss: 0.234484\tvalid_1's amex_metric: 0.780319\n",
      "[1600]\ttraining's binary_logloss: 0.219884\ttraining's amex_metric: 0.81236\tvalid_1's binary_logloss: 0.233747\tvalid_1's amex_metric: 0.780723\n",
      "[1700]\ttraining's binary_logloss: 0.217069\ttraining's amex_metric: 0.814614\tvalid_1's binary_logloss: 0.231727\tvalid_1's amex_metric: 0.782641\n",
      "[1800]\ttraining's binary_logloss: 0.214854\ttraining's amex_metric: 0.816363\tvalid_1's binary_logloss: 0.230292\tvalid_1's amex_metric: 0.783612\n",
      "[1900]\ttraining's binary_logloss: 0.21136\ttraining's amex_metric: 0.818747\tvalid_1's binary_logloss: 0.228006\tvalid_1's amex_metric: 0.784277\n",
      "[2000]\ttraining's binary_logloss: 0.209522\ttraining's amex_metric: 0.821129\tvalid_1's binary_logloss: 0.226998\tvalid_1's amex_metric: 0.784754\n",
      "[2100]\ttraining's binary_logloss: 0.207915\ttraining's amex_metric: 0.823036\tvalid_1's binary_logloss: 0.226213\tvalid_1's amex_metric: 0.785652\n",
      "[2200]\ttraining's binary_logloss: 0.205997\ttraining's amex_metric: 0.824829\tvalid_1's binary_logloss: 0.225276\tvalid_1's amex_metric: 0.785727\n",
      "[2300]\ttraining's binary_logloss: 0.203695\ttraining's amex_metric: 0.827709\tvalid_1's binary_logloss: 0.224141\tvalid_1's amex_metric: 0.786325\n",
      "[2400]\ttraining's binary_logloss: 0.201199\ttraining's amex_metric: 0.830504\tvalid_1's binary_logloss: 0.223026\tvalid_1's amex_metric: 0.787675\n",
      "[2500]\ttraining's binary_logloss: 0.200119\ttraining's amex_metric: 0.832521\tvalid_1's binary_logloss: 0.222725\tvalid_1's amex_metric: 0.788302\n",
      "[2600]\ttraining's binary_logloss: 0.199069\ttraining's amex_metric: 0.834874\tvalid_1's binary_logloss: 0.222477\tvalid_1's amex_metric: 0.788447\n",
      "[2700]\ttraining's binary_logloss: 0.197499\ttraining's amex_metric: 0.837274\tvalid_1's binary_logloss: 0.221955\tvalid_1's amex_metric: 0.788756\n",
      "[2800]\ttraining's binary_logloss: 0.195714\ttraining's amex_metric: 0.839082\tvalid_1's binary_logloss: 0.221362\tvalid_1's amex_metric: 0.789242\n",
      "[2900]\ttraining's binary_logloss: 0.194776\ttraining's amex_metric: 0.840989\tvalid_1's binary_logloss: 0.221175\tvalid_1's amex_metric: 0.789833\n",
      "[3000]\ttraining's binary_logloss: 0.193806\ttraining's amex_metric: 0.842916\tvalid_1's binary_logloss: 0.220976\tvalid_1's amex_metric: 0.789573\n",
      "[3100]\ttraining's binary_logloss: 0.192268\ttraining's amex_metric: 0.845234\tvalid_1's binary_logloss: 0.220547\tvalid_1's amex_metric: 0.790705\n",
      "[3200]\ttraining's binary_logloss: 0.190748\ttraining's amex_metric: 0.847301\tvalid_1's binary_logloss: 0.220195\tvalid_1's amex_metric: 0.79083\n",
      "[3300]\ttraining's binary_logloss: 0.189157\ttraining's amex_metric: 0.849479\tvalid_1's binary_logloss: 0.219808\tvalid_1's amex_metric: 0.791334\n",
      "[3400]\ttraining's binary_logloss: 0.187708\ttraining's amex_metric: 0.851394\tvalid_1's binary_logloss: 0.219485\tvalid_1's amex_metric: 0.791369\n",
      "[3500]\ttraining's binary_logloss: 0.186165\ttraining's amex_metric: 0.853754\tvalid_1's binary_logloss: 0.219164\tvalid_1's amex_metric: 0.791334\n",
      "[3600]\ttraining's binary_logloss: 0.185436\ttraining's amex_metric: 0.855949\tvalid_1's binary_logloss: 0.219129\tvalid_1's amex_metric: 0.791095\n",
      "[3700]\ttraining's binary_logloss: 0.183865\ttraining's amex_metric: 0.857847\tvalid_1's binary_logloss: 0.218849\tvalid_1's amex_metric: 0.791816\n",
      "[3800]\ttraining's binary_logloss: 0.182468\ttraining's amex_metric: 0.86002\tvalid_1's binary_logloss: 0.218623\tvalid_1's amex_metric: 0.792184\n",
      "[3900]\ttraining's binary_logloss: 0.180812\ttraining's amex_metric: 0.862424\tvalid_1's binary_logloss: 0.218334\tvalid_1's amex_metric: 0.792704\n",
      "[4000]\ttraining's binary_logloss: 0.17971\ttraining's amex_metric: 0.864522\tvalid_1's binary_logloss: 0.218226\tvalid_1's amex_metric: 0.792158\n",
      "[4100]\ttraining's binary_logloss: 0.178452\ttraining's amex_metric: 0.866771\tvalid_1's binary_logloss: 0.218086\tvalid_1's amex_metric: 0.792262\n",
      "[4200]\ttraining's binary_logloss: 0.177037\ttraining's amex_metric: 0.868944\tvalid_1's binary_logloss: 0.217928\tvalid_1's amex_metric: 0.792697\n",
      "[4300]\ttraining's binary_logloss: 0.17605\ttraining's amex_metric: 0.871118\tvalid_1's binary_logloss: 0.21787\tvalid_1's amex_metric: 0.792833\n",
      "[4400]\ttraining's binary_logloss: 0.17497\ttraining's amex_metric: 0.873327\tvalid_1's binary_logloss: 0.217784\tvalid_1's amex_metric: 0.793197\n",
      "[4500]\ttraining's binary_logloss: 0.173971\ttraining's amex_metric: 0.875022\tvalid_1's binary_logloss: 0.217691\tvalid_1's amex_metric: 0.793547\n",
      "[4600]\ttraining's binary_logloss: 0.173214\ttraining's amex_metric: 0.876428\tvalid_1's binary_logloss: 0.217686\tvalid_1's amex_metric: 0.793832\n",
      "[4700]\ttraining's binary_logloss: 0.172378\ttraining's amex_metric: 0.878156\tvalid_1's binary_logloss: 0.217645\tvalid_1's amex_metric: 0.79362\n",
      "[4800]\ttraining's binary_logloss: 0.171556\ttraining's amex_metric: 0.879827\tvalid_1's binary_logloss: 0.217622\tvalid_1's amex_metric: 0.793585\n",
      "[4900]\ttraining's binary_logloss: 0.170652\ttraining's amex_metric: 0.881655\tvalid_1's binary_logloss: 0.217577\tvalid_1's amex_metric: 0.793761\n",
      "[5000]\ttraining's binary_logloss: 0.170403\ttraining's amex_metric: 0.882922\tvalid_1's binary_logloss: 0.217684\tvalid_1's amex_metric: 0.793549\n",
      "[5100]\ttraining's binary_logloss: 0.169274\ttraining's amex_metric: 0.884378\tvalid_1's binary_logloss: 0.217567\tvalid_1's amex_metric: 0.794165\n",
      "[5200]\ttraining's binary_logloss: 0.168077\ttraining's amex_metric: 0.885957\tvalid_1's binary_logloss: 0.217429\tvalid_1's amex_metric: 0.793681\n",
      "[5300]\ttraining's binary_logloss: 0.167156\ttraining's amex_metric: 0.887386\tvalid_1's binary_logloss: 0.21732\tvalid_1's amex_metric: 0.793734\n",
      "[5400]\ttraining's binary_logloss: 0.165814\ttraining's amex_metric: 0.889267\tvalid_1's binary_logloss: 0.217183\tvalid_1's amex_metric: 0.793733\n",
      "[5500]\ttraining's binary_logloss: 0.16468\ttraining's amex_metric: 0.891124\tvalid_1's binary_logloss: 0.217089\tvalid_1's amex_metric: 0.793857\n",
      "[5600]\ttraining's binary_logloss: 0.163476\ttraining's amex_metric: 0.892942\tvalid_1's binary_logloss: 0.216997\tvalid_1's amex_metric: 0.79417\n",
      "[5700]\ttraining's binary_logloss: 0.162513\ttraining's amex_metric: 0.894505\tvalid_1's binary_logloss: 0.216921\tvalid_1's amex_metric: 0.794361\n",
      "[5800]\ttraining's binary_logloss: 0.161499\ttraining's amex_metric: 0.896311\tvalid_1's binary_logloss: 0.216859\tvalid_1's amex_metric: 0.79446\n",
      "[5900]\ttraining's binary_logloss: 0.160604\ttraining's amex_metric: 0.897776\tvalid_1's binary_logloss: 0.216834\tvalid_1's amex_metric: 0.794089\n",
      "[6000]\ttraining's binary_logloss: 0.15962\ttraining's amex_metric: 0.899748\tvalid_1's binary_logloss: 0.216748\tvalid_1's amex_metric: 0.794073\n",
      "[6100]\ttraining's binary_logloss: 0.158614\ttraining's amex_metric: 0.901361\tvalid_1's binary_logloss: 0.216701\tvalid_1's amex_metric: 0.794064\n",
      "[6200]\ttraining's binary_logloss: 0.157617\ttraining's amex_metric: 0.902962\tvalid_1's binary_logloss: 0.21664\tvalid_1's amex_metric: 0.794333\n",
      "[6300]\ttraining's binary_logloss: 0.156873\ttraining's amex_metric: 0.904686\tvalid_1's binary_logloss: 0.216626\tvalid_1's amex_metric: 0.794534\n",
      "[6400]\ttraining's binary_logloss: 0.156401\ttraining's amex_metric: 0.906054\tvalid_1's binary_logloss: 0.216629\tvalid_1's amex_metric: 0.794521\n",
      "[6500]\ttraining's binary_logloss: 0.155418\ttraining's amex_metric: 0.907399\tvalid_1's binary_logloss: 0.216568\tvalid_1's amex_metric: 0.794658\n",
      "[6600]\ttraining's binary_logloss: 0.154706\ttraining's amex_metric: 0.908804\tvalid_1's binary_logloss: 0.216507\tvalid_1's amex_metric: 0.795121\n",
      "[6700]\ttraining's binary_logloss: 0.153738\ttraining's amex_metric: 0.91023\tvalid_1's binary_logloss: 0.216475\tvalid_1's amex_metric: 0.795014\n",
      "[6800]\ttraining's binary_logloss: 0.152868\ttraining's amex_metric: 0.911532\tvalid_1's binary_logloss: 0.216466\tvalid_1's amex_metric: 0.795259\n",
      "[6900]\ttraining's binary_logloss: 0.151881\ttraining's amex_metric: 0.912936\tvalid_1's binary_logloss: 0.216391\tvalid_1's amex_metric: 0.795031\n",
      "[7000]\ttraining's binary_logloss: 0.151229\ttraining's amex_metric: 0.914229\tvalid_1's binary_logloss: 0.216333\tvalid_1's amex_metric: 0.795496\n",
      "[7100]\ttraining's binary_logloss: 0.150497\ttraining's amex_metric: 0.91577\tvalid_1's binary_logloss: 0.216307\tvalid_1's amex_metric: 0.794874\n",
      "[7200]\ttraining's binary_logloss: 0.149711\ttraining's amex_metric: 0.916965\tvalid_1's binary_logloss: 0.216263\tvalid_1's amex_metric: 0.795455\n",
      "[7300]\ttraining's binary_logloss: 0.148921\ttraining's amex_metric: 0.918206\tvalid_1's binary_logloss: 0.216268\tvalid_1's amex_metric: 0.795493\n",
      "[7400]\ttraining's binary_logloss: 0.148062\ttraining's amex_metric: 0.919605\tvalid_1's binary_logloss: 0.216231\tvalid_1's amex_metric: 0.795461\n",
      "[7500]\ttraining's binary_logloss: 0.147084\ttraining's amex_metric: 0.920976\tvalid_1's binary_logloss: 0.216183\tvalid_1's amex_metric: 0.794695\n",
      "[7600]\ttraining's binary_logloss: 0.14617\ttraining's amex_metric: 0.922473\tvalid_1's binary_logloss: 0.216172\tvalid_1's amex_metric: 0.795029\n",
      "[7700]\ttraining's binary_logloss: 0.145221\ttraining's amex_metric: 0.924009\tvalid_1's binary_logloss: 0.216137\tvalid_1's amex_metric: 0.795163\n",
      "[7800]\ttraining's binary_logloss: 0.144512\ttraining's amex_metric: 0.925384\tvalid_1's binary_logloss: 0.21612\tvalid_1's amex_metric: 0.79561\n",
      "Our fold 1 CV score is 0.7956097509967957\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.656903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 253400\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.489567\ttraining's amex_metric: 0.76714\tvalid_1's binary_logloss: 0.490701\tvalid_1's amex_metric: 0.755471\n",
      "[200]\ttraining's binary_logloss: 0.414206\ttraining's amex_metric: 0.772337\tvalid_1's binary_logloss: 0.41633\tvalid_1's amex_metric: 0.759259\n",
      "[300]\ttraining's binary_logloss: 0.36229\ttraining's amex_metric: 0.776665\tvalid_1's binary_logloss: 0.365375\tvalid_1's amex_metric: 0.762733\n",
      "[400]\ttraining's binary_logloss: 0.324376\ttraining's amex_metric: 0.780437\tvalid_1's binary_logloss: 0.328468\tvalid_1's amex_metric: 0.765921\n",
      "[500]\ttraining's binary_logloss: 0.304265\ttraining's amex_metric: 0.782906\tvalid_1's binary_logloss: 0.309021\tvalid_1's amex_metric: 0.767611\n",
      "[600]\ttraining's binary_logloss: 0.28271\ttraining's amex_metric: 0.78576\tvalid_1's binary_logloss: 0.288318\tvalid_1's amex_metric: 0.770115\n",
      "[700]\ttraining's binary_logloss: 0.271868\ttraining's amex_metric: 0.788998\tvalid_1's binary_logloss: 0.278126\tvalid_1's amex_metric: 0.772317\n",
      "[800]\ttraining's binary_logloss: 0.261149\ttraining's amex_metric: 0.791803\tvalid_1's binary_logloss: 0.268184\tvalid_1's amex_metric: 0.77381\n",
      "[900]\ttraining's binary_logloss: 0.258383\ttraining's amex_metric: 0.793952\tvalid_1's binary_logloss: 0.265745\tvalid_1's amex_metric: 0.774817\n",
      "[1000]\ttraining's binary_logloss: 0.250385\ttraining's amex_metric: 0.796363\tvalid_1's binary_logloss: 0.258464\tvalid_1's amex_metric: 0.776532\n",
      "[1100]\ttraining's binary_logloss: 0.241708\ttraining's amex_metric: 0.799031\tvalid_1's binary_logloss: 0.250699\tvalid_1's amex_metric: 0.777494\n",
      "[1200]\ttraining's binary_logloss: 0.236578\ttraining's amex_metric: 0.801457\tvalid_1's binary_logloss: 0.246309\tvalid_1's amex_metric: 0.778106\n",
      "[1300]\ttraining's binary_logloss: 0.230296\ttraining's amex_metric: 0.804232\tvalid_1's binary_logloss: 0.240993\tvalid_1's amex_metric: 0.779537\n",
      "[1400]\ttraining's binary_logloss: 0.225296\ttraining's amex_metric: 0.807024\tvalid_1's binary_logloss: 0.236989\tvalid_1's amex_metric: 0.78097\n",
      "[1500]\ttraining's binary_logloss: 0.221081\ttraining's amex_metric: 0.809985\tvalid_1's binary_logloss: 0.233811\tvalid_1's amex_metric: 0.782236\n",
      "[1600]\ttraining's binary_logloss: 0.219862\ttraining's amex_metric: 0.812293\tvalid_1's binary_logloss: 0.233094\tvalid_1's amex_metric: 0.783127\n",
      "[1700]\ttraining's binary_logloss: 0.217065\ttraining's amex_metric: 0.814682\tvalid_1's binary_logloss: 0.231133\tvalid_1's amex_metric: 0.783636\n",
      "[1800]\ttraining's binary_logloss: 0.214855\ttraining's amex_metric: 0.816431\tvalid_1's binary_logloss: 0.229684\tvalid_1's amex_metric: 0.784378\n",
      "[1900]\ttraining's binary_logloss: 0.211372\ttraining's amex_metric: 0.81893\tvalid_1's binary_logloss: 0.227508\tvalid_1's amex_metric: 0.784762\n",
      "[2000]\ttraining's binary_logloss: 0.209542\ttraining's amex_metric: 0.820999\tvalid_1's binary_logloss: 0.226575\tvalid_1's amex_metric: 0.785741\n",
      "[2100]\ttraining's binary_logloss: 0.207932\ttraining's amex_metric: 0.823391\tvalid_1's binary_logloss: 0.225805\tvalid_1's amex_metric: 0.78654\n",
      "[2200]\ttraining's binary_logloss: 0.206008\ttraining's amex_metric: 0.825346\tvalid_1's binary_logloss: 0.22486\tvalid_1's amex_metric: 0.787235\n",
      "[2300]\ttraining's binary_logloss: 0.203722\ttraining's amex_metric: 0.82775\tvalid_1's binary_logloss: 0.223776\tvalid_1's amex_metric: 0.788122\n",
      "[2400]\ttraining's binary_logloss: 0.201243\ttraining's amex_metric: 0.830826\tvalid_1's binary_logloss: 0.222722\tvalid_1's amex_metric: 0.788664\n",
      "[2500]\ttraining's binary_logloss: 0.200134\ttraining's amex_metric: 0.832905\tvalid_1's binary_logloss: 0.222442\tvalid_1's amex_metric: 0.788771\n",
      "[2600]\ttraining's binary_logloss: 0.199052\ttraining's amex_metric: 0.834816\tvalid_1's binary_logloss: 0.22212\tvalid_1's amex_metric: 0.789803\n",
      "[2700]\ttraining's binary_logloss: 0.19748\ttraining's amex_metric: 0.837438\tvalid_1's binary_logloss: 0.221624\tvalid_1's amex_metric: 0.789697\n",
      "[2800]\ttraining's binary_logloss: 0.195661\ttraining's amex_metric: 0.839608\tvalid_1's binary_logloss: 0.221044\tvalid_1's amex_metric: 0.790139\n",
      "[2900]\ttraining's binary_logloss: 0.194709\ttraining's amex_metric: 0.841519\tvalid_1's binary_logloss: 0.220862\tvalid_1's amex_metric: 0.790305\n",
      "[3000]\ttraining's binary_logloss: 0.193717\ttraining's amex_metric: 0.843846\tvalid_1's binary_logloss: 0.220618\tvalid_1's amex_metric: 0.790074\n",
      "[3100]\ttraining's binary_logloss: 0.192198\ttraining's amex_metric: 0.845809\tvalid_1's binary_logloss: 0.220213\tvalid_1's amex_metric: 0.790449\n",
      "[3200]\ttraining's binary_logloss: 0.190678\ttraining's amex_metric: 0.847968\tvalid_1's binary_logloss: 0.219833\tvalid_1's amex_metric: 0.790673\n",
      "[3300]\ttraining's binary_logloss: 0.189084\ttraining's amex_metric: 0.850155\tvalid_1's binary_logloss: 0.21949\tvalid_1's amex_metric: 0.790776\n",
      "[3400]\ttraining's binary_logloss: 0.187654\ttraining's amex_metric: 0.85206\tvalid_1's binary_logloss: 0.219201\tvalid_1's amex_metric: 0.79053\n",
      "[3500]\ttraining's binary_logloss: 0.186089\ttraining's amex_metric: 0.85448\tvalid_1's binary_logloss: 0.218905\tvalid_1's amex_metric: 0.790598\n",
      "[3600]\ttraining's binary_logloss: 0.185362\ttraining's amex_metric: 0.856336\tvalid_1's binary_logloss: 0.21883\tvalid_1's amex_metric: 0.791188\n",
      "[3700]\ttraining's binary_logloss: 0.183796\ttraining's amex_metric: 0.858387\tvalid_1's binary_logloss: 0.21854\tvalid_1's amex_metric: 0.791579\n",
      "[3800]\ttraining's binary_logloss: 0.182388\ttraining's amex_metric: 0.860543\tvalid_1's binary_logloss: 0.218321\tvalid_1's amex_metric: 0.791462\n",
      "[3900]\ttraining's binary_logloss: 0.18072\ttraining's amex_metric: 0.863059\tvalid_1's binary_logloss: 0.21807\tvalid_1's amex_metric: 0.791656\n",
      "[4000]\ttraining's binary_logloss: 0.179616\ttraining's amex_metric: 0.86511\tvalid_1's binary_logloss: 0.217974\tvalid_1's amex_metric: 0.791423\n",
      "[4100]\ttraining's binary_logloss: 0.178353\ttraining's amex_metric: 0.867144\tvalid_1's binary_logloss: 0.217808\tvalid_1's amex_metric: 0.791618\n",
      "[4200]\ttraining's binary_logloss: 0.176932\ttraining's amex_metric: 0.869618\tvalid_1's binary_logloss: 0.217641\tvalid_1's amex_metric: 0.791911\n",
      "[4300]\ttraining's binary_logloss: 0.175963\ttraining's amex_metric: 0.871888\tvalid_1's binary_logloss: 0.217555\tvalid_1's amex_metric: 0.791742\n",
      "[4400]\ttraining's binary_logloss: 0.174927\ttraining's amex_metric: 0.873707\tvalid_1's binary_logloss: 0.217483\tvalid_1's amex_metric: 0.791769\n",
      "[4500]\ttraining's binary_logloss: 0.173925\ttraining's amex_metric: 0.875548\tvalid_1's binary_logloss: 0.217412\tvalid_1's amex_metric: 0.791728\n",
      "[4600]\ttraining's binary_logloss: 0.173179\ttraining's amex_metric: 0.877374\tvalid_1's binary_logloss: 0.217387\tvalid_1's amex_metric: 0.791918\n",
      "[4700]\ttraining's binary_logloss: 0.17236\ttraining's amex_metric: 0.878891\tvalid_1's binary_logloss: 0.217354\tvalid_1's amex_metric: 0.792036\n",
      "[4800]\ttraining's binary_logloss: 0.171538\ttraining's amex_metric: 0.880419\tvalid_1's binary_logloss: 0.217316\tvalid_1's amex_metric: 0.792009\n",
      "[4900]\ttraining's binary_logloss: 0.170632\ttraining's amex_metric: 0.882016\tvalid_1's binary_logloss: 0.217238\tvalid_1's amex_metric: 0.792245\n",
      "[5000]\ttraining's binary_logloss: 0.170402\ttraining's amex_metric: 0.882978\tvalid_1's binary_logloss: 0.217295\tvalid_1's amex_metric: 0.792557\n",
      "[5100]\ttraining's binary_logloss: 0.169262\ttraining's amex_metric: 0.884741\tvalid_1's binary_logloss: 0.217163\tvalid_1's amex_metric: 0.792423\n",
      "[5200]\ttraining's binary_logloss: 0.168059\ttraining's amex_metric: 0.886056\tvalid_1's binary_logloss: 0.217002\tvalid_1's amex_metric: 0.792661\n",
      "[5300]\ttraining's binary_logloss: 0.167142\ttraining's amex_metric: 0.887596\tvalid_1's binary_logloss: 0.216948\tvalid_1's amex_metric: 0.792775\n",
      "[5400]\ttraining's binary_logloss: 0.165825\ttraining's amex_metric: 0.889538\tvalid_1's binary_logloss: 0.216833\tvalid_1's amex_metric: 0.792621\n",
      "[5500]\ttraining's binary_logloss: 0.1647\ttraining's amex_metric: 0.891354\tvalid_1's binary_logloss: 0.216757\tvalid_1's amex_metric: 0.792466\n",
      "[5600]\ttraining's binary_logloss: 0.163501\ttraining's amex_metric: 0.893358\tvalid_1's binary_logloss: 0.216655\tvalid_1's amex_metric: 0.792557\n",
      "[5700]\ttraining's binary_logloss: 0.16255\ttraining's amex_metric: 0.894949\tvalid_1's binary_logloss: 0.216616\tvalid_1's amex_metric: 0.792422\n",
      "[5800]\ttraining's binary_logloss: 0.161534\ttraining's amex_metric: 0.896606\tvalid_1's binary_logloss: 0.216554\tvalid_1's amex_metric: 0.793094\n",
      "[5900]\ttraining's binary_logloss: 0.160642\ttraining's amex_metric: 0.898246\tvalid_1's binary_logloss: 0.216495\tvalid_1's amex_metric: 0.792882\n",
      "[6000]\ttraining's binary_logloss: 0.159656\ttraining's amex_metric: 0.899932\tvalid_1's binary_logloss: 0.216436\tvalid_1's amex_metric: 0.792775\n",
      "[6100]\ttraining's binary_logloss: 0.158662\ttraining's amex_metric: 0.901457\tvalid_1's binary_logloss: 0.216396\tvalid_1's amex_metric: 0.792657\n",
      "[6200]\ttraining's binary_logloss: 0.157663\ttraining's amex_metric: 0.903361\tvalid_1's binary_logloss: 0.216359\tvalid_1's amex_metric: 0.79277\n",
      "[6300]\ttraining's binary_logloss: 0.156911\ttraining's amex_metric: 0.904767\tvalid_1's binary_logloss: 0.216294\tvalid_1's amex_metric: 0.79269\n",
      "[6400]\ttraining's binary_logloss: 0.156428\ttraining's amex_metric: 0.906102\tvalid_1's binary_logloss: 0.216293\tvalid_1's amex_metric: 0.79278\n",
      "[6500]\ttraining's binary_logloss: 0.155435\ttraining's amex_metric: 0.907627\tvalid_1's binary_logloss: 0.216238\tvalid_1's amex_metric: 0.79292\n",
      "[6600]\ttraining's binary_logloss: 0.154723\ttraining's amex_metric: 0.908717\tvalid_1's binary_logloss: 0.21618\tvalid_1's amex_metric: 0.79235\n",
      "[6700]\ttraining's binary_logloss: 0.153767\ttraining's amex_metric: 0.910094\tvalid_1's binary_logloss: 0.21613\tvalid_1's amex_metric: 0.792485\n",
      "[6800]\ttraining's binary_logloss: 0.152899\ttraining's amex_metric: 0.911571\tvalid_1's binary_logloss: 0.216102\tvalid_1's amex_metric: 0.792618\n",
      "[6900]\ttraining's binary_logloss: 0.151916\ttraining's amex_metric: 0.91303\tvalid_1's binary_logloss: 0.216072\tvalid_1's amex_metric: 0.793271\n",
      "[7000]\ttraining's binary_logloss: 0.151262\ttraining's amex_metric: 0.91418\tvalid_1's binary_logloss: 0.216063\tvalid_1's amex_metric: 0.793467\n",
      "[7100]\ttraining's binary_logloss: 0.150522\ttraining's amex_metric: 0.915629\tvalid_1's binary_logloss: 0.216015\tvalid_1's amex_metric: 0.793884\n",
      "[7200]\ttraining's binary_logloss: 0.149745\ttraining's amex_metric: 0.916954\tvalid_1's binary_logloss: 0.215986\tvalid_1's amex_metric: 0.793934\n",
      "[7300]\ttraining's binary_logloss: 0.148945\ttraining's amex_metric: 0.918425\tvalid_1's binary_logloss: 0.215943\tvalid_1's amex_metric: 0.794117\n",
      "[7400]\ttraining's binary_logloss: 0.148074\ttraining's amex_metric: 0.919869\tvalid_1's binary_logloss: 0.215913\tvalid_1's amex_metric: 0.794272\n",
      "[7500]\ttraining's binary_logloss: 0.147097\ttraining's amex_metric: 0.92125\tvalid_1's binary_logloss: 0.215873\tvalid_1's amex_metric: 0.794365\n",
      "[7600]\ttraining's binary_logloss: 0.146212\ttraining's amex_metric: 0.922517\tvalid_1's binary_logloss: 0.21585\tvalid_1's amex_metric: 0.794413\n",
      "[7700]\ttraining's binary_logloss: 0.145267\ttraining's amex_metric: 0.923856\tvalid_1's binary_logloss: 0.21582\tvalid_1's amex_metric: 0.793957\n",
      "[7800]\ttraining's binary_logloss: 0.144564\ttraining's amex_metric: 0.925065\tvalid_1's binary_logloss: 0.215783\tvalid_1's amex_metric: 0.794304\n",
      "[7900]\ttraining's binary_logloss: 0.14376\ttraining's amex_metric: 0.926549\tvalid_1's binary_logloss: 0.215768\tvalid_1's amex_metric: 0.794413\n",
      "[8000]\ttraining's binary_logloss: 0.142773\ttraining's amex_metric: 0.928025\tvalid_1's binary_logloss: 0.215737\tvalid_1's amex_metric: 0.795011\n",
      "Our fold 2 CV score is 0.7950107926730193\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.871073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 253410\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.489762\ttraining's amex_metric: 0.766828\tvalid_1's binary_logloss: 0.490313\tvalid_1's amex_metric: 0.759592\n",
      "[200]\ttraining's binary_logloss: 0.414526\ttraining's amex_metric: 0.77122\tvalid_1's binary_logloss: 0.415561\tvalid_1's amex_metric: 0.762313\n",
      "[300]\ttraining's binary_logloss: 0.362826\ttraining's amex_metric: 0.775215\tvalid_1's binary_logloss: 0.364488\tvalid_1's amex_metric: 0.764573\n",
      "[400]\ttraining's binary_logloss: 0.324932\ttraining's amex_metric: 0.779238\tvalid_1's binary_logloss: 0.327285\tvalid_1's amex_metric: 0.767037\n",
      "[500]\ttraining's binary_logloss: 0.30481\ttraining's amex_metric: 0.782473\tvalid_1's binary_logloss: 0.307691\tvalid_1's amex_metric: 0.769829\n",
      "[600]\ttraining's binary_logloss: 0.28323\ttraining's amex_metric: 0.785498\tvalid_1's binary_logloss: 0.286813\tvalid_1's amex_metric: 0.771437\n",
      "[700]\ttraining's binary_logloss: 0.272568\ttraining's amex_metric: 0.788344\tvalid_1's binary_logloss: 0.276765\tvalid_1's amex_metric: 0.773983\n",
      "[800]\ttraining's binary_logloss: 0.261879\ttraining's amex_metric: 0.790914\tvalid_1's binary_logloss: 0.266736\tvalid_1's amex_metric: 0.774518\n",
      "[900]\ttraining's binary_logloss: 0.259055\ttraining's amex_metric: 0.793109\tvalid_1's binary_logloss: 0.264256\tvalid_1's amex_metric: 0.775955\n",
      "[1000]\ttraining's binary_logloss: 0.250999\ttraining's amex_metric: 0.795633\tvalid_1's binary_logloss: 0.256821\tvalid_1's amex_metric: 0.777338\n",
      "[1100]\ttraining's binary_logloss: 0.242344\ttraining's amex_metric: 0.798021\tvalid_1's binary_logloss: 0.248986\tvalid_1's amex_metric: 0.778634\n",
      "[1200]\ttraining's binary_logloss: 0.23721\ttraining's amex_metric: 0.800717\tvalid_1's binary_logloss: 0.244469\tvalid_1's amex_metric: 0.780266\n",
      "[1300]\ttraining's binary_logloss: 0.230925\ttraining's amex_metric: 0.803729\tvalid_1's binary_logloss: 0.23906\tvalid_1's amex_metric: 0.782013\n",
      "[1400]\ttraining's binary_logloss: 0.225881\ttraining's amex_metric: 0.806683\tvalid_1's binary_logloss: 0.234915\tvalid_1's amex_metric: 0.782977\n",
      "[1500]\ttraining's binary_logloss: 0.221697\ttraining's amex_metric: 0.809478\tvalid_1's binary_logloss: 0.231683\tvalid_1's amex_metric: 0.783895\n",
      "[1600]\ttraining's binary_logloss: 0.220493\ttraining's amex_metric: 0.811402\tvalid_1's binary_logloss: 0.231011\tvalid_1's amex_metric: 0.785218\n",
      "[1700]\ttraining's binary_logloss: 0.217718\ttraining's amex_metric: 0.813545\tvalid_1's binary_logloss: 0.229097\tvalid_1's amex_metric: 0.785933\n",
      "[1800]\ttraining's binary_logloss: 0.215493\ttraining's amex_metric: 0.815555\tvalid_1's binary_logloss: 0.227723\tvalid_1's amex_metric: 0.786358\n",
      "[1900]\ttraining's binary_logloss: 0.211997\ttraining's amex_metric: 0.818011\tvalid_1's binary_logloss: 0.225446\tvalid_1's amex_metric: 0.787252\n",
      "[2000]\ttraining's binary_logloss: 0.210179\ttraining's amex_metric: 0.820621\tvalid_1's binary_logloss: 0.224495\tvalid_1's amex_metric: 0.787624\n",
      "[2100]\ttraining's binary_logloss: 0.20858\ttraining's amex_metric: 0.822342\tvalid_1's binary_logloss: 0.223731\tvalid_1's amex_metric: 0.788918\n",
      "[2200]\ttraining's binary_logloss: 0.206629\ttraining's amex_metric: 0.82483\tvalid_1's binary_logloss: 0.222778\tvalid_1's amex_metric: 0.789111\n",
      "[2300]\ttraining's binary_logloss: 0.204332\ttraining's amex_metric: 0.827136\tvalid_1's binary_logloss: 0.221685\tvalid_1's amex_metric: 0.79061\n",
      "[2400]\ttraining's binary_logloss: 0.20185\ttraining's amex_metric: 0.829749\tvalid_1's binary_logloss: 0.220567\tvalid_1's amex_metric: 0.790504\n",
      "[2500]\ttraining's binary_logloss: 0.200777\ttraining's amex_metric: 0.832198\tvalid_1's binary_logloss: 0.220339\tvalid_1's amex_metric: 0.791041\n",
      "[2600]\ttraining's binary_logloss: 0.199685\ttraining's amex_metric: 0.834173\tvalid_1's binary_logloss: 0.220029\tvalid_1's amex_metric: 0.790894\n",
      "[2700]\ttraining's binary_logloss: 0.198095\ttraining's amex_metric: 0.836293\tvalid_1's binary_logloss: 0.21951\tvalid_1's amex_metric: 0.790665\n",
      "[2800]\ttraining's binary_logloss: 0.196335\ttraining's amex_metric: 0.838429\tvalid_1's binary_logloss: 0.218968\tvalid_1's amex_metric: 0.79134\n",
      "[2900]\ttraining's binary_logloss: 0.195397\ttraining's amex_metric: 0.84066\tvalid_1's binary_logloss: 0.218799\tvalid_1's amex_metric: 0.79195\n",
      "[3000]\ttraining's binary_logloss: 0.194416\ttraining's amex_metric: 0.842503\tvalid_1's binary_logloss: 0.218656\tvalid_1's amex_metric: 0.791992\n",
      "[3100]\ttraining's binary_logloss: 0.192889\ttraining's amex_metric: 0.844393\tvalid_1's binary_logloss: 0.218221\tvalid_1's amex_metric: 0.792476\n",
      "[3200]\ttraining's binary_logloss: 0.191354\ttraining's amex_metric: 0.8463\tvalid_1's binary_logloss: 0.217801\tvalid_1's amex_metric: 0.79283\n",
      "[3300]\ttraining's binary_logloss: 0.189757\ttraining's amex_metric: 0.848487\tvalid_1's binary_logloss: 0.21739\tvalid_1's amex_metric: 0.793227\n",
      "[3400]\ttraining's binary_logloss: 0.188294\ttraining's amex_metric: 0.850573\tvalid_1's binary_logloss: 0.217094\tvalid_1's amex_metric: 0.793582\n",
      "[3500]\ttraining's binary_logloss: 0.186751\ttraining's amex_metric: 0.85281\tvalid_1's binary_logloss: 0.21678\tvalid_1's amex_metric: 0.793247\n",
      "[3600]\ttraining's binary_logloss: 0.186029\ttraining's amex_metric: 0.854744\tvalid_1's binary_logloss: 0.216778\tvalid_1's amex_metric: 0.793632\n",
      "[3700]\ttraining's binary_logloss: 0.184471\ttraining's amex_metric: 0.857106\tvalid_1's binary_logloss: 0.216478\tvalid_1's amex_metric: 0.793961\n",
      "[3800]\ttraining's binary_logloss: 0.183047\ttraining's amex_metric: 0.859482\tvalid_1's binary_logloss: 0.21627\tvalid_1's amex_metric: 0.794337\n",
      "[3900]\ttraining's binary_logloss: 0.181396\ttraining's amex_metric: 0.861691\tvalid_1's binary_logloss: 0.216001\tvalid_1's amex_metric: 0.794703\n",
      "[4000]\ttraining's binary_logloss: 0.180284\ttraining's amex_metric: 0.864211\tvalid_1's binary_logloss: 0.215841\tvalid_1's amex_metric: 0.794773\n",
      "[4100]\ttraining's binary_logloss: 0.179027\ttraining's amex_metric: 0.866427\tvalid_1's binary_logloss: 0.215673\tvalid_1's amex_metric: 0.795133\n",
      "[4200]\ttraining's binary_logloss: 0.177618\ttraining's amex_metric: 0.868679\tvalid_1's binary_logloss: 0.215519\tvalid_1's amex_metric: 0.795334\n",
      "[4300]\ttraining's binary_logloss: 0.176628\ttraining's amex_metric: 0.87068\tvalid_1's binary_logloss: 0.215458\tvalid_1's amex_metric: 0.795959\n",
      "[4400]\ttraining's binary_logloss: 0.175571\ttraining's amex_metric: 0.872598\tvalid_1's binary_logloss: 0.215382\tvalid_1's amex_metric: 0.796093\n",
      "[4500]\ttraining's binary_logloss: 0.174589\ttraining's amex_metric: 0.874396\tvalid_1's binary_logloss: 0.215325\tvalid_1's amex_metric: 0.796156\n",
      "[4600]\ttraining's binary_logloss: 0.173827\ttraining's amex_metric: 0.876125\tvalid_1's binary_logloss: 0.215271\tvalid_1's amex_metric: 0.796168\n",
      "[4700]\ttraining's binary_logloss: 0.173005\ttraining's amex_metric: 0.87788\tvalid_1's binary_logloss: 0.215229\tvalid_1's amex_metric: 0.796943\n",
      "[4800]\ttraining's binary_logloss: 0.172193\ttraining's amex_metric: 0.879291\tvalid_1's binary_logloss: 0.215143\tvalid_1's amex_metric: 0.796805\n",
      "[4900]\ttraining's binary_logloss: 0.171282\ttraining's amex_metric: 0.881222\tvalid_1's binary_logloss: 0.215081\tvalid_1's amex_metric: 0.796481\n",
      "[5000]\ttraining's binary_logloss: 0.171046\ttraining's amex_metric: 0.88233\tvalid_1's binary_logloss: 0.215173\tvalid_1's amex_metric: 0.796827\n",
      "[5100]\ttraining's binary_logloss: 0.169924\ttraining's amex_metric: 0.884041\tvalid_1's binary_logloss: 0.215054\tvalid_1's amex_metric: 0.796324\n",
      "[5200]\ttraining's binary_logloss: 0.168749\ttraining's amex_metric: 0.885463\tvalid_1's binary_logloss: 0.21492\tvalid_1's amex_metric: 0.796481\n",
      "[5300]\ttraining's binary_logloss: 0.167806\ttraining's amex_metric: 0.887189\tvalid_1's binary_logloss: 0.214854\tvalid_1's amex_metric: 0.796857\n",
      "[5400]\ttraining's binary_logloss: 0.166479\ttraining's amex_metric: 0.888797\tvalid_1's binary_logloss: 0.214724\tvalid_1's amex_metric: 0.796909\n",
      "[5500]\ttraining's binary_logloss: 0.165341\ttraining's amex_metric: 0.890416\tvalid_1's binary_logloss: 0.21465\tvalid_1's amex_metric: 0.797216\n",
      "[5600]\ttraining's binary_logloss: 0.164137\ttraining's amex_metric: 0.892077\tvalid_1's binary_logloss: 0.214536\tvalid_1's amex_metric: 0.797327\n",
      "[5700]\ttraining's binary_logloss: 0.163181\ttraining's amex_metric: 0.893961\tvalid_1's binary_logloss: 0.214473\tvalid_1's amex_metric: 0.797139\n",
      "[5800]\ttraining's binary_logloss: 0.162171\ttraining's amex_metric: 0.895648\tvalid_1's binary_logloss: 0.214387\tvalid_1's amex_metric: 0.797183\n",
      "[5900]\ttraining's binary_logloss: 0.16127\ttraining's amex_metric: 0.897388\tvalid_1's binary_logloss: 0.21433\tvalid_1's amex_metric: 0.796993\n",
      "[6000]\ttraining's binary_logloss: 0.160285\ttraining's amex_metric: 0.89901\tvalid_1's binary_logloss: 0.214297\tvalid_1's amex_metric: 0.797397\n",
      "[6100]\ttraining's binary_logloss: 0.159298\ttraining's amex_metric: 0.900611\tvalid_1's binary_logloss: 0.214272\tvalid_1's amex_metric: 0.79769\n",
      "[6200]\ttraining's binary_logloss: 0.158284\ttraining's amex_metric: 0.902354\tvalid_1's binary_logloss: 0.214203\tvalid_1's amex_metric: 0.797433\n",
      "[6300]\ttraining's binary_logloss: 0.157531\ttraining's amex_metric: 0.903985\tvalid_1's binary_logloss: 0.21422\tvalid_1's amex_metric: 0.797746\n",
      "[6400]\ttraining's binary_logloss: 0.157041\ttraining's amex_metric: 0.905245\tvalid_1's binary_logloss: 0.214233\tvalid_1's amex_metric: 0.79771\n",
      "[6500]\ttraining's binary_logloss: 0.156057\ttraining's amex_metric: 0.906704\tvalid_1's binary_logloss: 0.214146\tvalid_1's amex_metric: 0.797858\n",
      "[6600]\ttraining's binary_logloss: 0.155334\ttraining's amex_metric: 0.907975\tvalid_1's binary_logloss: 0.214132\tvalid_1's amex_metric: 0.797735\n",
      "[6700]\ttraining's binary_logloss: 0.154365\ttraining's amex_metric: 0.909659\tvalid_1's binary_logloss: 0.214061\tvalid_1's amex_metric: 0.797626\n",
      "[6800]\ttraining's binary_logloss: 0.153491\ttraining's amex_metric: 0.910911\tvalid_1's binary_logloss: 0.21403\tvalid_1's amex_metric: 0.798092\n",
      "[6900]\ttraining's binary_logloss: 0.152493\ttraining's amex_metric: 0.912299\tvalid_1's binary_logloss: 0.213977\tvalid_1's amex_metric: 0.798292\n",
      "[7000]\ttraining's binary_logloss: 0.151839\ttraining's amex_metric: 0.913388\tvalid_1's binary_logloss: 0.213963\tvalid_1's amex_metric: 0.797835\n",
      "[7100]\ttraining's binary_logloss: 0.151104\ttraining's amex_metric: 0.914508\tvalid_1's binary_logloss: 0.213935\tvalid_1's amex_metric: 0.798076\n",
      "[7200]\ttraining's binary_logloss: 0.150311\ttraining's amex_metric: 0.915972\tvalid_1's binary_logloss: 0.213905\tvalid_1's amex_metric: 0.798757\n",
      "[7300]\ttraining's binary_logloss: 0.14951\ttraining's amex_metric: 0.91746\tvalid_1's binary_logloss: 0.213885\tvalid_1's amex_metric: 0.798339\n",
      "[7400]\ttraining's binary_logloss: 0.148651\ttraining's amex_metric: 0.91875\tvalid_1's binary_logloss: 0.213875\tvalid_1's amex_metric: 0.798548\n",
      "[7500]\ttraining's binary_logloss: 0.147686\ttraining's amex_metric: 0.920245\tvalid_1's binary_logloss: 0.213817\tvalid_1's amex_metric: 0.798455\n",
      "[7600]\ttraining's binary_logloss: 0.146785\ttraining's amex_metric: 0.921637\tvalid_1's binary_logloss: 0.213775\tvalid_1's amex_metric: 0.798715\n",
      "[7700]\ttraining's binary_logloss: 0.145828\ttraining's amex_metric: 0.923174\tvalid_1's binary_logloss: 0.213729\tvalid_1's amex_metric: 0.798411\n",
      "[7800]\ttraining's binary_logloss: 0.145126\ttraining's amex_metric: 0.924472\tvalid_1's binary_logloss: 0.213699\tvalid_1's amex_metric: 0.798528\n",
      "[7900]\ttraining's binary_logloss: 0.144317\ttraining's amex_metric: 0.925942\tvalid_1's binary_logloss: 0.213667\tvalid_1's amex_metric: 0.79835\n",
      "[8000]\ttraining's binary_logloss: 0.143335\ttraining's amex_metric: 0.927379\tvalid_1's binary_logloss: 0.213624\tvalid_1's amex_metric: 0.798064\n",
      "[8100]\ttraining's binary_logloss: 0.142615\ttraining's amex_metric: 0.928564\tvalid_1's binary_logloss: 0.213594\tvalid_1's amex_metric: 0.797884\n",
      "[8200]\ttraining's binary_logloss: 0.141828\ttraining's amex_metric: 0.930044\tvalid_1's binary_logloss: 0.213588\tvalid_1's amex_metric: 0.797652\n",
      "[8300]\ttraining's binary_logloss: 0.141065\ttraining's amex_metric: 0.931175\tvalid_1's binary_logloss: 0.213564\tvalid_1's amex_metric: 0.797783\n",
      "[8400]\ttraining's binary_logloss: 0.140405\ttraining's amex_metric: 0.932208\tvalid_1's binary_logloss: 0.213547\tvalid_1's amex_metric: 0.797724\n",
      "[8500]\ttraining's binary_logloss: 0.139819\ttraining's amex_metric: 0.933292\tvalid_1's binary_logloss: 0.213554\tvalid_1's amex_metric: 0.797471\n",
      "[8600]\ttraining's binary_logloss: 0.139098\ttraining's amex_metric: 0.934418\tvalid_1's binary_logloss: 0.213545\tvalid_1's amex_metric: 0.797662\n",
      "[8700]\ttraining's binary_logloss: 0.13852\ttraining's amex_metric: 0.935785\tvalid_1's binary_logloss: 0.213532\tvalid_1's amex_metric: 0.79832\n",
      "[8800]\ttraining's binary_logloss: 0.13775\ttraining's amex_metric: 0.936801\tvalid_1's binary_logloss: 0.213516\tvalid_1's amex_metric: 0.797588\n",
      "[8900]\ttraining's binary_logloss: 0.137074\ttraining's amex_metric: 0.937782\tvalid_1's binary_logloss: 0.213477\tvalid_1's amex_metric: 0.798128\n",
      "[9000]\ttraining's binary_logloss: 0.13631\ttraining's amex_metric: 0.938915\tvalid_1's binary_logloss: 0.213458\tvalid_1's amex_metric: 0.797878\n",
      "[9100]\ttraining's binary_logloss: 0.135715\ttraining's amex_metric: 0.939861\tvalid_1's binary_logloss: 0.213442\tvalid_1's amex_metric: 0.798031\n",
      "[9200]\ttraining's binary_logloss: 0.135269\ttraining's amex_metric: 0.940588\tvalid_1's binary_logloss: 0.213439\tvalid_1's amex_metric: 0.798074\n",
      "[9300]\ttraining's binary_logloss: 0.134702\ttraining's amex_metric: 0.941673\tvalid_1's binary_logloss: 0.213456\tvalid_1's amex_metric: 0.798131\n",
      "[9400]\ttraining's binary_logloss: 0.134037\ttraining's amex_metric: 0.942667\tvalid_1's binary_logloss: 0.213455\tvalid_1's amex_metric: 0.798149\n",
      "[9500]\ttraining's binary_logloss: 0.13347\ttraining's amex_metric: 0.943557\tvalid_1's binary_logloss: 0.213444\tvalid_1's amex_metric: 0.798278\n",
      "[9600]\ttraining's binary_logloss: 0.132739\ttraining's amex_metric: 0.944586\tvalid_1's binary_logloss: 0.213396\tvalid_1's amex_metric: 0.798063\n",
      "[9700]\ttraining's binary_logloss: 0.132091\ttraining's amex_metric: 0.945442\tvalid_1's binary_logloss: 0.213367\tvalid_1's amex_metric: 0.798725\n",
      "[9800]\ttraining's binary_logloss: 0.131178\ttraining's amex_metric: 0.946492\tvalid_1's binary_logloss: 0.213338\tvalid_1's amex_metric: 0.798691\n",
      "[9900]\ttraining's binary_logloss: 0.13064\ttraining's amex_metric: 0.947421\tvalid_1's binary_logloss: 0.213325\tvalid_1's amex_metric: 0.798759\n",
      "[10000]\ttraining's binary_logloss: 0.129968\ttraining's amex_metric: 0.94833\tvalid_1's binary_logloss: 0.213327\tvalid_1's amex_metric: 0.799094\n",
      "[10100]\ttraining's binary_logloss: 0.129106\ttraining's amex_metric: 0.949223\tvalid_1's binary_logloss: 0.213333\tvalid_1's amex_metric: 0.79911\n",
      "[10200]\ttraining's binary_logloss: 0.128451\ttraining's amex_metric: 0.950138\tvalid_1's binary_logloss: 0.213347\tvalid_1's amex_metric: 0.799106\n",
      "[10300]\ttraining's binary_logloss: 0.127711\ttraining's amex_metric: 0.951083\tvalid_1's binary_logloss: 0.213361\tvalid_1's amex_metric: 0.79889\n",
      "[10400]\ttraining's binary_logloss: 0.127063\ttraining's amex_metric: 0.952044\tvalid_1's binary_logloss: 0.213354\tvalid_1's amex_metric: 0.799187\n",
      "Our fold 3 CV score is 0.7991871433251675\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1637 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.579229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 253176\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.48964\ttraining's amex_metric: 0.766545\tvalid_1's binary_logloss: 0.490714\tvalid_1's amex_metric: 0.761317\n",
      "[200]\ttraining's binary_logloss: 0.414434\ttraining's amex_metric: 0.77071\tvalid_1's binary_logloss: 0.416304\tvalid_1's amex_metric: 0.763777\n",
      "[300]\ttraining's binary_logloss: 0.362608\ttraining's amex_metric: 0.775302\tvalid_1's binary_logloss: 0.36527\tvalid_1's amex_metric: 0.766654\n",
      "[400]\ttraining's binary_logloss: 0.324789\ttraining's amex_metric: 0.779449\tvalid_1's binary_logloss: 0.328255\tvalid_1's amex_metric: 0.768325\n",
      "[500]\ttraining's binary_logloss: 0.304623\ttraining's amex_metric: 0.781876\tvalid_1's binary_logloss: 0.308682\tvalid_1's amex_metric: 0.76997\n",
      "[600]\ttraining's binary_logloss: 0.283147\ttraining's amex_metric: 0.785366\tvalid_1's binary_logloss: 0.287894\tvalid_1's amex_metric: 0.772143\n",
      "[700]\ttraining's binary_logloss: 0.272439\ttraining's amex_metric: 0.788127\tvalid_1's binary_logloss: 0.277767\tvalid_1's amex_metric: 0.774268\n",
      "[800]\ttraining's binary_logloss: 0.261746\ttraining's amex_metric: 0.79081\tvalid_1's binary_logloss: 0.267816\tvalid_1's amex_metric: 0.775646\n",
      "[900]\ttraining's binary_logloss: 0.25889\ttraining's amex_metric: 0.792629\tvalid_1's binary_logloss: 0.265335\tvalid_1's amex_metric: 0.776358\n",
      "[1000]\ttraining's binary_logloss: 0.250818\ttraining's amex_metric: 0.795083\tvalid_1's binary_logloss: 0.257879\tvalid_1's amex_metric: 0.777543\n",
      "[1100]\ttraining's binary_logloss: 0.242115\ttraining's amex_metric: 0.797833\tvalid_1's binary_logloss: 0.249943\tvalid_1's amex_metric: 0.7793\n",
      "[1200]\ttraining's binary_logloss: 0.237034\ttraining's amex_metric: 0.800282\tvalid_1's binary_logloss: 0.245534\tvalid_1's amex_metric: 0.780821\n",
      "[1300]\ttraining's binary_logloss: 0.230744\ttraining's amex_metric: 0.802916\tvalid_1's binary_logloss: 0.240082\tvalid_1's amex_metric: 0.782446\n",
      "[1400]\ttraining's binary_logloss: 0.225735\ttraining's amex_metric: 0.805857\tvalid_1's binary_logloss: 0.235963\tvalid_1's amex_metric: 0.78358\n",
      "[1500]\ttraining's binary_logloss: 0.221611\ttraining's amex_metric: 0.808724\tvalid_1's binary_logloss: 0.23279\tvalid_1's amex_metric: 0.78488\n",
      "[1600]\ttraining's binary_logloss: 0.220414\ttraining's amex_metric: 0.811059\tvalid_1's binary_logloss: 0.23213\tvalid_1's amex_metric: 0.785341\n",
      "[1700]\ttraining's binary_logloss: 0.217596\ttraining's amex_metric: 0.813564\tvalid_1's binary_logloss: 0.230117\tvalid_1's amex_metric: 0.786443\n",
      "[1800]\ttraining's binary_logloss: 0.215359\ttraining's amex_metric: 0.815478\tvalid_1's binary_logloss: 0.228631\tvalid_1's amex_metric: 0.787317\n",
      "[1900]\ttraining's binary_logloss: 0.211891\ttraining's amex_metric: 0.818229\tvalid_1's binary_logloss: 0.2263\tvalid_1's amex_metric: 0.787705\n",
      "[2000]\ttraining's binary_logloss: 0.210061\ttraining's amex_metric: 0.820602\tvalid_1's binary_logloss: 0.225266\tvalid_1's amex_metric: 0.789076\n",
      "[2100]\ttraining's binary_logloss: 0.208473\ttraining's amex_metric: 0.822498\tvalid_1's binary_logloss: 0.224484\tvalid_1's amex_metric: 0.789626\n",
      "[2200]\ttraining's binary_logloss: 0.206541\ttraining's amex_metric: 0.824647\tvalid_1's binary_logloss: 0.223467\tvalid_1's amex_metric: 0.790405\n",
      "[2300]\ttraining's binary_logloss: 0.204276\ttraining's amex_metric: 0.827092\tvalid_1's binary_logloss: 0.222326\tvalid_1's amex_metric: 0.791106\n",
      "[2400]\ttraining's binary_logloss: 0.201774\ttraining's amex_metric: 0.829746\tvalid_1's binary_logloss: 0.221223\tvalid_1's amex_metric: 0.791101\n",
      "[2500]\ttraining's binary_logloss: 0.200712\ttraining's amex_metric: 0.831948\tvalid_1's binary_logloss: 0.220935\tvalid_1's amex_metric: 0.791024\n",
      "[2600]\ttraining's binary_logloss: 0.19966\ttraining's amex_metric: 0.834094\tvalid_1's binary_logloss: 0.220655\tvalid_1's amex_metric: 0.791919\n",
      "[2700]\ttraining's binary_logloss: 0.198081\ttraining's amex_metric: 0.836293\tvalid_1's binary_logloss: 0.220066\tvalid_1's amex_metric: 0.791787\n",
      "[2800]\ttraining's binary_logloss: 0.196307\ttraining's amex_metric: 0.838606\tvalid_1's binary_logloss: 0.219424\tvalid_1's amex_metric: 0.792972\n",
      "[2900]\ttraining's binary_logloss: 0.195374\ttraining's amex_metric: 0.840524\tvalid_1's binary_logloss: 0.219215\tvalid_1's amex_metric: 0.793804\n",
      "[3000]\ttraining's binary_logloss: 0.194376\ttraining's amex_metric: 0.84244\tvalid_1's binary_logloss: 0.218995\tvalid_1's amex_metric: 0.793878\n",
      "[3100]\ttraining's binary_logloss: 0.192831\ttraining's amex_metric: 0.844562\tvalid_1's binary_logloss: 0.218535\tvalid_1's amex_metric: 0.794286\n",
      "[3200]\ttraining's binary_logloss: 0.191299\ttraining's amex_metric: 0.846318\tvalid_1's binary_logloss: 0.218083\tvalid_1's amex_metric: 0.794871\n",
      "[3300]\ttraining's binary_logloss: 0.189721\ttraining's amex_metric: 0.848573\tvalid_1's binary_logloss: 0.217677\tvalid_1's amex_metric: 0.794951\n",
      "[3400]\ttraining's binary_logloss: 0.188293\ttraining's amex_metric: 0.850518\tvalid_1's binary_logloss: 0.217381\tvalid_1's amex_metric: 0.795075\n",
      "[3500]\ttraining's binary_logloss: 0.186734\ttraining's amex_metric: 0.853013\tvalid_1's binary_logloss: 0.217025\tvalid_1's amex_metric: 0.795362\n",
      "[3600]\ttraining's binary_logloss: 0.18601\ttraining's amex_metric: 0.855024\tvalid_1's binary_logloss: 0.216966\tvalid_1's amex_metric: 0.795811\n",
      "[3700]\ttraining's binary_logloss: 0.184454\ttraining's amex_metric: 0.857364\tvalid_1's binary_logloss: 0.2166\tvalid_1's amex_metric: 0.795782\n",
      "[3800]\ttraining's binary_logloss: 0.183041\ttraining's amex_metric: 0.859568\tvalid_1's binary_logloss: 0.216335\tvalid_1's amex_metric: 0.796602\n",
      "[3900]\ttraining's binary_logloss: 0.181372\ttraining's amex_metric: 0.862007\tvalid_1's binary_logloss: 0.216001\tvalid_1's amex_metric: 0.796874\n",
      "[4000]\ttraining's binary_logloss: 0.180271\ttraining's amex_metric: 0.864038\tvalid_1's binary_logloss: 0.215907\tvalid_1's amex_metric: 0.797211\n",
      "[4100]\ttraining's binary_logloss: 0.179031\ttraining's amex_metric: 0.866137\tvalid_1's binary_logloss: 0.215771\tvalid_1's amex_metric: 0.797032\n",
      "[4200]\ttraining's binary_logloss: 0.17764\ttraining's amex_metric: 0.86872\tvalid_1's binary_logloss: 0.215577\tvalid_1's amex_metric: 0.797475\n",
      "[4300]\ttraining's binary_logloss: 0.176657\ttraining's amex_metric: 0.870829\tvalid_1's binary_logloss: 0.215536\tvalid_1's amex_metric: 0.797226\n",
      "[4400]\ttraining's binary_logloss: 0.175612\ttraining's amex_metric: 0.872786\tvalid_1's binary_logloss: 0.21543\tvalid_1's amex_metric: 0.797642\n",
      "[4500]\ttraining's binary_logloss: 0.174613\ttraining's amex_metric: 0.874681\tvalid_1's binary_logloss: 0.215341\tvalid_1's amex_metric: 0.797989\n",
      "[4600]\ttraining's binary_logloss: 0.173865\ttraining's amex_metric: 0.876313\tvalid_1's binary_logloss: 0.215338\tvalid_1's amex_metric: 0.797923\n",
      "[4700]\ttraining's binary_logloss: 0.17304\ttraining's amex_metric: 0.8779\tvalid_1's binary_logloss: 0.215297\tvalid_1's amex_metric: 0.79752\n",
      "[4800]\ttraining's binary_logloss: 0.172209\ttraining's amex_metric: 0.879637\tvalid_1's binary_logloss: 0.215283\tvalid_1's amex_metric: 0.797966\n",
      "[4900]\ttraining's binary_logloss: 0.171305\ttraining's amex_metric: 0.881169\tvalid_1's binary_logloss: 0.21519\tvalid_1's amex_metric: 0.797588\n",
      "[5000]\ttraining's binary_logloss: 0.171061\ttraining's amex_metric: 0.882463\tvalid_1's binary_logloss: 0.215304\tvalid_1's amex_metric: 0.797571\n",
      "[5100]\ttraining's binary_logloss: 0.169941\ttraining's amex_metric: 0.883853\tvalid_1's binary_logloss: 0.215151\tvalid_1's amex_metric: 0.797894\n",
      "[5200]\ttraining's binary_logloss: 0.168749\ttraining's amex_metric: 0.885715\tvalid_1's binary_logloss: 0.215005\tvalid_1's amex_metric: 0.797337\n",
      "[5300]\ttraining's binary_logloss: 0.167804\ttraining's amex_metric: 0.887075\tvalid_1's binary_logloss: 0.214935\tvalid_1's amex_metric: 0.798041\n",
      "[5400]\ttraining's binary_logloss: 0.166497\ttraining's amex_metric: 0.888941\tvalid_1's binary_logloss: 0.214751\tvalid_1's amex_metric: 0.798004\n",
      "[5500]\ttraining's binary_logloss: 0.165376\ttraining's amex_metric: 0.890753\tvalid_1's binary_logloss: 0.214629\tvalid_1's amex_metric: 0.797825\n",
      "[5600]\ttraining's binary_logloss: 0.164172\ttraining's amex_metric: 0.892247\tvalid_1's binary_logloss: 0.214531\tvalid_1's amex_metric: 0.798323\n",
      "[5700]\ttraining's binary_logloss: 0.163223\ttraining's amex_metric: 0.89414\tvalid_1's binary_logloss: 0.214457\tvalid_1's amex_metric: 0.797859\n",
      "[5800]\ttraining's binary_logloss: 0.162225\ttraining's amex_metric: 0.896054\tvalid_1's binary_logloss: 0.214401\tvalid_1's amex_metric: 0.798023\n",
      "[5900]\ttraining's binary_logloss: 0.161322\ttraining's amex_metric: 0.897757\tvalid_1's binary_logloss: 0.214375\tvalid_1's amex_metric: 0.797861\n",
      "[6000]\ttraining's binary_logloss: 0.160325\ttraining's amex_metric: 0.89919\tvalid_1's binary_logloss: 0.214291\tvalid_1's amex_metric: 0.798219\n",
      "[6100]\ttraining's binary_logloss: 0.159339\ttraining's amex_metric: 0.900877\tvalid_1's binary_logloss: 0.21425\tvalid_1's amex_metric: 0.798776\n",
      "[6200]\ttraining's binary_logloss: 0.15834\ttraining's amex_metric: 0.902239\tvalid_1's binary_logloss: 0.21417\tvalid_1's amex_metric: 0.798352\n",
      "[6300]\ttraining's binary_logloss: 0.157604\ttraining's amex_metric: 0.903953\tvalid_1's binary_logloss: 0.214145\tvalid_1's amex_metric: 0.798597\n",
      "[6400]\ttraining's binary_logloss: 0.157127\ttraining's amex_metric: 0.905008\tvalid_1's binary_logloss: 0.214161\tvalid_1's amex_metric: 0.798581\n",
      "[6500]\ttraining's binary_logloss: 0.156139\ttraining's amex_metric: 0.906642\tvalid_1's binary_logloss: 0.214099\tvalid_1's amex_metric: 0.798758\n",
      "[6600]\ttraining's binary_logloss: 0.155427\ttraining's amex_metric: 0.907926\tvalid_1's binary_logloss: 0.214099\tvalid_1's amex_metric: 0.798841\n",
      "[6700]\ttraining's binary_logloss: 0.154451\ttraining's amex_metric: 0.909268\tvalid_1's binary_logloss: 0.214042\tvalid_1's amex_metric: 0.798657\n",
      "[6800]\ttraining's binary_logloss: 0.153582\ttraining's amex_metric: 0.910603\tvalid_1's binary_logloss: 0.213987\tvalid_1's amex_metric: 0.79867\n",
      "[6900]\ttraining's binary_logloss: 0.152585\ttraining's amex_metric: 0.912054\tvalid_1's binary_logloss: 0.213893\tvalid_1's amex_metric: 0.798842\n",
      "[7000]\ttraining's binary_logloss: 0.151933\ttraining's amex_metric: 0.913356\tvalid_1's binary_logloss: 0.213886\tvalid_1's amex_metric: 0.798846\n",
      "[7100]\ttraining's binary_logloss: 0.151199\ttraining's amex_metric: 0.914541\tvalid_1's binary_logloss: 0.213867\tvalid_1's amex_metric: 0.798855\n",
      "[7200]\ttraining's binary_logloss: 0.150413\ttraining's amex_metric: 0.915958\tvalid_1's binary_logloss: 0.213852\tvalid_1's amex_metric: 0.798729\n",
      "[7300]\ttraining's binary_logloss: 0.14962\ttraining's amex_metric: 0.917344\tvalid_1's binary_logloss: 0.213842\tvalid_1's amex_metric: 0.798727\n",
      "[7400]\ttraining's binary_logloss: 0.148754\ttraining's amex_metric: 0.918905\tvalid_1's binary_logloss: 0.213812\tvalid_1's amex_metric: 0.798648\n",
      "[7500]\ttraining's binary_logloss: 0.147774\ttraining's amex_metric: 0.92008\tvalid_1's binary_logloss: 0.213739\tvalid_1's amex_metric: 0.798492\n",
      "[7600]\ttraining's binary_logloss: 0.14686\ttraining's amex_metric: 0.921457\tvalid_1's binary_logloss: 0.213709\tvalid_1's amex_metric: 0.798981\n",
      "[7700]\ttraining's binary_logloss: 0.145916\ttraining's amex_metric: 0.922828\tvalid_1's binary_logloss: 0.213663\tvalid_1's amex_metric: 0.799013\n",
      "[7800]\ttraining's binary_logloss: 0.145199\ttraining's amex_metric: 0.924216\tvalid_1's binary_logloss: 0.213636\tvalid_1's amex_metric: 0.799191\n",
      "[7900]\ttraining's binary_logloss: 0.144361\ttraining's amex_metric: 0.925618\tvalid_1's binary_logloss: 0.2136\tvalid_1's amex_metric: 0.798717\n",
      "[8000]\ttraining's binary_logloss: 0.143372\ttraining's amex_metric: 0.926827\tvalid_1's binary_logloss: 0.213548\tvalid_1's amex_metric: 0.798792\n",
      "[8100]\ttraining's binary_logloss: 0.142663\ttraining's amex_metric: 0.928191\tvalid_1's binary_logloss: 0.213524\tvalid_1's amex_metric: 0.799347\n",
      "[8200]\ttraining's binary_logloss: 0.141877\ttraining's amex_metric: 0.929709\tvalid_1's binary_logloss: 0.213547\tvalid_1's amex_metric: 0.799336\n",
      "[8300]\ttraining's binary_logloss: 0.141112\ttraining's amex_metric: 0.930779\tvalid_1's binary_logloss: 0.213516\tvalid_1's amex_metric: 0.799763\n",
      "[8400]\ttraining's binary_logloss: 0.140451\ttraining's amex_metric: 0.931933\tvalid_1's binary_logloss: 0.213512\tvalid_1's amex_metric: 0.7993\n",
      "[8500]\ttraining's binary_logloss: 0.139878\ttraining's amex_metric: 0.93309\tvalid_1's binary_logloss: 0.21352\tvalid_1's amex_metric: 0.799275\n",
      "[8600]\ttraining's binary_logloss: 0.139165\ttraining's amex_metric: 0.934327\tvalid_1's binary_logloss: 0.213537\tvalid_1's amex_metric: 0.799519\n",
      "[8700]\ttraining's binary_logloss: 0.138591\ttraining's amex_metric: 0.935427\tvalid_1's binary_logloss: 0.213513\tvalid_1's amex_metric: 0.799339\n",
      "[8800]\ttraining's binary_logloss: 0.137805\ttraining's amex_metric: 0.93638\tvalid_1's binary_logloss: 0.213497\tvalid_1's amex_metric: 0.799363\n",
      "[8900]\ttraining's binary_logloss: 0.137129\ttraining's amex_metric: 0.937463\tvalid_1's binary_logloss: 0.21348\tvalid_1's amex_metric: 0.799346\n",
      "[9000]\ttraining's binary_logloss: 0.136384\ttraining's amex_metric: 0.938528\tvalid_1's binary_logloss: 0.213469\tvalid_1's amex_metric: 0.799179\n",
      "[9100]\ttraining's binary_logloss: 0.135781\ttraining's amex_metric: 0.939591\tvalid_1's binary_logloss: 0.213457\tvalid_1's amex_metric: 0.79952\n",
      "[9200]\ttraining's binary_logloss: 0.135331\ttraining's amex_metric: 0.940325\tvalid_1's binary_logloss: 0.213454\tvalid_1's amex_metric: 0.799671\n",
      "[9300]\ttraining's binary_logloss: 0.134768\ttraining's amex_metric: 0.941179\tvalid_1's binary_logloss: 0.213428\tvalid_1's amex_metric: 0.799763\n",
      "[9400]\ttraining's binary_logloss: 0.13409\ttraining's amex_metric: 0.942081\tvalid_1's binary_logloss: 0.213436\tvalid_1's amex_metric: 0.799608\n",
      "[9500]\ttraining's binary_logloss: 0.133527\ttraining's amex_metric: 0.943108\tvalid_1's binary_logloss: 0.213427\tvalid_1's amex_metric: 0.800093\n",
      "[9600]\ttraining's binary_logloss: 0.132803\ttraining's amex_metric: 0.944002\tvalid_1's binary_logloss: 0.213442\tvalid_1's amex_metric: 0.799769\n",
      "[9700]\ttraining's binary_logloss: 0.13216\ttraining's amex_metric: 0.945004\tvalid_1's binary_logloss: 0.2134\tvalid_1's amex_metric: 0.799633\n",
      "[9800]\ttraining's binary_logloss: 0.131267\ttraining's amex_metric: 0.946217\tvalid_1's binary_logloss: 0.213396\tvalid_1's amex_metric: 0.800029\n",
      "[9900]\ttraining's binary_logloss: 0.130735\ttraining's amex_metric: 0.947124\tvalid_1's binary_logloss: 0.213377\tvalid_1's amex_metric: 0.799174\n",
      "[10000]\ttraining's binary_logloss: 0.130058\ttraining's amex_metric: 0.948005\tvalid_1's binary_logloss: 0.21337\tvalid_1's amex_metric: 0.799259\n",
      "[10100]\ttraining's binary_logloss: 0.129189\ttraining's amex_metric: 0.948968\tvalid_1's binary_logloss: 0.21332\tvalid_1's amex_metric: 0.800071\n",
      "Our fold 4 CV score is 0.8000714718869273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "# test_predictions = np.zeros(len(train))\n",
    "test_predictions = []\n",
    "# Create a numpy array to store out of fonp.zeros(len(train))lds predictions\n",
    "oof_predictions = []\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "epoch = [7600,7800,8000,10400,10100]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = epoch[fold],#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1500,\n",
    "#         eval_metric=[lgb_amex_metric],\n",
    "        verbose_eval = 100,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions.extend(val_pred)\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions.append(test_pred)# += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f08ac4-d86f-4dcd-b281-4788f52bcce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.7974655171031564\n"
     ]
    }
   ],
   "source": [
    "# Compute out of folds metric\n",
    "score = amex_metric(tr_target, oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "test_pred = np.mean(test_predictions, axis = 0)\n",
    "# Create a dataframe to store test pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "iction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_pred})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 0 CV score is 0.7992066857047015\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 1 CV score is 0.795692679299457\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 2 CV score is 0.7939480362739684\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 3 CV score is 0.797958740810314\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 4 CV score is 0.8001977303020998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = []\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = []\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "    \n",
    "    # model = lgb.LGBMClassifier()\n",
    "    with open(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl', 'rb') as web:\n",
    "        model = pickle.load(web)\n",
    "    # model.load_model(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl')\n",
    "    \n",
    "    print(\"pred test\")\n",
    "    test_predictions.append(model.predict(test[features]))\n",
    "    print(\"pred val\")\n",
    "    val_pred = model.predict(x_val)\n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].iloc[val_ind])\n",
    "    tr_target.extend(y_val)\n",
    "    \n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val,  lgb_train ,lgb_valid\n",
    "    del model\n",
    "    gc.collect()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4b85b-1c5c-4b4b-b3e7-dea2470726d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.7969425363638722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    f\"{CFG.ver}_{CFG.model}_oof\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "\n",
    "# Compute out of folds metric\n",
    "score = amex_metric(oof_df[\"target\"], oof_df[f\"{CFG.ver}_{CFG.model}_oof\"])\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.mean(test_predictions,axis = 0)\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_pred})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b80690-de04-4f36-bc0c-6e9bc48f7f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
