{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp37\n",
    "\n",
    "lag_diff„ÅÆXGB\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26772\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26772\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    \n",
    "    \n",
    "    # input_dir = '../feature/exp35_lagdiff/'\n",
    "    input_dir = '../feature/exp03_amex-fe/'\n",
    "    output_dir = '../output/exp49_lgb_seed614/'\n",
    "    seed = 614\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"lgb\"\n",
    "    ver = \"exp49\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "# def read_data():\n",
    "#     train = pd.read_parquet(CFG.input_dir + 'train_diff.parquet')\n",
    "#     test = pd.read_parquet(CFG.input_dir + 'test_diff.parquet')\n",
    "#     return train, test\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "# ====================================================\n",
    "# LGBM amex metric\n",
    "# ====================================================\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5df5be5-8119-4c74-965b-89abb98ff5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 2013)\n",
      "(924621, 2012)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "\n",
    "train = pd.read_parquet('../feature/exp35_lagdiff/train_lagdiff.parquet')\n",
    "test = pd.read_parquet('../feature/exp35_lagdiff/test_lagdiff.parquet')\n",
    "\n",
    "# train, test = read_data()\n",
    "\n",
    "train_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/train_c3.pkl')\n",
    "test_c3 = pd.read_pickle('../feature/exp18_4_tsfresh/test_c3.pkl')\n",
    "\n",
    "train = train.merge(train_c3,on = \"customer_ID\",how = \"left\")\n",
    "test = test.merge(test_c3,on = \"customer_ID\",how = \"left\")\n",
    "\n",
    "del train_c3,test_c3\n",
    "gc.collect\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1aef6c-6e01-413f-b9a0-46be991ca78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.304892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327771\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1998\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.488823\ttraining's amex_metric: 0.766209\tvalid_1's binary_logloss: 0.489859\tvalid_1's amex_metric: 0.757454\n",
      "[200]\ttraining's binary_logloss: 0.413811\ttraining's amex_metric: 0.771043\tvalid_1's binary_logloss: 0.415666\tvalid_1's amex_metric: 0.760787\n",
      "[300]\ttraining's binary_logloss: 0.361958\ttraining's amex_metric: 0.775795\tvalid_1's binary_logloss: 0.364673\tvalid_1's amex_metric: 0.764455\n",
      "[400]\ttraining's binary_logloss: 0.32393\ttraining's amex_metric: 0.779567\tvalid_1's binary_logloss: 0.327488\tvalid_1's amex_metric: 0.766473\n",
      "[500]\ttraining's binary_logloss: 0.304027\ttraining's amex_metric: 0.7824\tvalid_1's binary_logloss: 0.308223\tvalid_1's amex_metric: 0.768148\n",
      "[600]\ttraining's binary_logloss: 0.282488\ttraining's amex_metric: 0.785297\tvalid_1's binary_logloss: 0.287485\tvalid_1's amex_metric: 0.770782\n",
      "[700]\ttraining's binary_logloss: 0.271688\ttraining's amex_metric: 0.788896\tvalid_1's binary_logloss: 0.277307\tvalid_1's amex_metric: 0.7735\n",
      "[800]\ttraining's binary_logloss: 0.260994\ttraining's amex_metric: 0.791881\tvalid_1's binary_logloss: 0.267353\tvalid_1's amex_metric: 0.774156\n",
      "[900]\ttraining's binary_logloss: 0.258225\ttraining's amex_metric: 0.794124\tvalid_1's binary_logloss: 0.264893\tvalid_1's amex_metric: 0.775312\n",
      "[1000]\ttraining's binary_logloss: 0.250299\ttraining's amex_metric: 0.796162\tvalid_1's binary_logloss: 0.257637\tvalid_1's amex_metric: 0.776899\n",
      "[1100]\ttraining's binary_logloss: 0.241699\ttraining's amex_metric: 0.798706\tvalid_1's binary_logloss: 0.249861\tvalid_1's amex_metric: 0.777642\n",
      "[1200]\ttraining's binary_logloss: 0.23666\ttraining's amex_metric: 0.800764\tvalid_1's binary_logloss: 0.245499\tvalid_1's amex_metric: 0.77851\n",
      "[1300]\ttraining's binary_logloss: 0.230461\ttraining's amex_metric: 0.803233\tvalid_1's binary_logloss: 0.240179\tvalid_1's amex_metric: 0.78006\n",
      "[1400]\ttraining's binary_logloss: 0.225488\ttraining's amex_metric: 0.805984\tvalid_1's binary_logloss: 0.236157\tvalid_1's amex_metric: 0.781297\n",
      "[1500]\ttraining's binary_logloss: 0.221287\ttraining's amex_metric: 0.808542\tvalid_1's binary_logloss: 0.232949\tvalid_1's amex_metric: 0.782378\n",
      "[1600]\ttraining's binary_logloss: 0.220066\ttraining's amex_metric: 0.810721\tvalid_1's binary_logloss: 0.23222\tvalid_1's amex_metric: 0.783549\n",
      "[1700]\ttraining's binary_logloss: 0.2173\ttraining's amex_metric: 0.813002\tvalid_1's binary_logloss: 0.230276\tvalid_1's amex_metric: 0.784357\n",
      "[1800]\ttraining's binary_logloss: 0.215051\ttraining's amex_metric: 0.815225\tvalid_1's binary_logloss: 0.228843\tvalid_1's amex_metric: 0.785473\n",
      "[1900]\ttraining's binary_logloss: 0.211507\ttraining's amex_metric: 0.818267\tvalid_1's binary_logloss: 0.226485\tvalid_1's amex_metric: 0.786632\n",
      "[2000]\ttraining's binary_logloss: 0.209667\ttraining's amex_metric: 0.820597\tvalid_1's binary_logloss: 0.225534\tvalid_1's amex_metric: 0.787716\n",
      "[2100]\ttraining's binary_logloss: 0.208034\ttraining's amex_metric: 0.822643\tvalid_1's binary_logloss: 0.22472\tvalid_1's amex_metric: 0.788408\n",
      "[2200]\ttraining's binary_logloss: 0.206098\ttraining's amex_metric: 0.824839\tvalid_1's binary_logloss: 0.223764\tvalid_1's amex_metric: 0.789319\n",
      "[2300]\ttraining's binary_logloss: 0.203773\ttraining's amex_metric: 0.827606\tvalid_1's binary_logloss: 0.222659\tvalid_1's amex_metric: 0.790053\n",
      "[2400]\ttraining's binary_logloss: 0.201242\ttraining's amex_metric: 0.830394\tvalid_1's binary_logloss: 0.22151\tvalid_1's amex_metric: 0.790873\n",
      "[2500]\ttraining's binary_logloss: 0.200143\ttraining's amex_metric: 0.832963\tvalid_1's binary_logloss: 0.221226\tvalid_1's amex_metric: 0.79146\n",
      "[2600]\ttraining's binary_logloss: 0.199017\ttraining's amex_metric: 0.834878\tvalid_1's binary_logloss: 0.220884\tvalid_1's amex_metric: 0.791579\n",
      "[2700]\ttraining's binary_logloss: 0.197421\ttraining's amex_metric: 0.837349\tvalid_1's binary_logloss: 0.220348\tvalid_1's amex_metric: 0.792452\n",
      "[2800]\ttraining's binary_logloss: 0.195625\ttraining's amex_metric: 0.839756\tvalid_1's binary_logloss: 0.219746\tvalid_1's amex_metric: 0.792987\n",
      "[2900]\ttraining's binary_logloss: 0.19468\ttraining's amex_metric: 0.842249\tvalid_1's binary_logloss: 0.219562\tvalid_1's amex_metric: 0.793202\n",
      "[3000]\ttraining's binary_logloss: 0.19369\ttraining's amex_metric: 0.844097\tvalid_1's binary_logloss: 0.219366\tvalid_1's amex_metric: 0.79372\n",
      "[3100]\ttraining's binary_logloss: 0.192099\ttraining's amex_metric: 0.84619\tvalid_1's binary_logloss: 0.218883\tvalid_1's amex_metric: 0.79407\n",
      "[3200]\ttraining's binary_logloss: 0.190534\ttraining's amex_metric: 0.848211\tvalid_1's binary_logloss: 0.218488\tvalid_1's amex_metric: 0.794056\n",
      "[3300]\ttraining's binary_logloss: 0.188916\ttraining's amex_metric: 0.850481\tvalid_1's binary_logloss: 0.218082\tvalid_1's amex_metric: 0.794436\n",
      "[3400]\ttraining's binary_logloss: 0.187452\ttraining's amex_metric: 0.85264\tvalid_1's binary_logloss: 0.217755\tvalid_1's amex_metric: 0.794634\n",
      "[3500]\ttraining's binary_logloss: 0.185892\ttraining's amex_metric: 0.855192\tvalid_1's binary_logloss: 0.217456\tvalid_1's amex_metric: 0.794383\n",
      "[3600]\ttraining's binary_logloss: 0.185148\ttraining's amex_metric: 0.857257\tvalid_1's binary_logloss: 0.217444\tvalid_1's amex_metric: 0.794368\n",
      "[3700]\ttraining's binary_logloss: 0.183565\ttraining's amex_metric: 0.859359\tvalid_1's binary_logloss: 0.217105\tvalid_1's amex_metric: 0.795288\n",
      "[3800]\ttraining's binary_logloss: 0.182124\ttraining's amex_metric: 0.861485\tvalid_1's binary_logloss: 0.216871\tvalid_1's amex_metric: 0.795023\n",
      "[3900]\ttraining's binary_logloss: 0.180415\ttraining's amex_metric: 0.863834\tvalid_1's binary_logloss: 0.216599\tvalid_1's amex_metric: 0.79567\n",
      "[4000]\ttraining's binary_logloss: 0.179295\ttraining's amex_metric: 0.866427\tvalid_1's binary_logloss: 0.216467\tvalid_1's amex_metric: 0.795143\n",
      "[4100]\ttraining's binary_logloss: 0.17802\ttraining's amex_metric: 0.868676\tvalid_1's binary_logloss: 0.216329\tvalid_1's amex_metric: 0.795553\n",
      "[4200]\ttraining's binary_logloss: 0.17658\ttraining's amex_metric: 0.870735\tvalid_1's binary_logloss: 0.216136\tvalid_1's amex_metric: 0.795889\n",
      "[4300]\ttraining's binary_logloss: 0.175588\ttraining's amex_metric: 0.873232\tvalid_1's binary_logloss: 0.216094\tvalid_1's amex_metric: 0.795493\n",
      "[4400]\ttraining's binary_logloss: 0.174513\ttraining's amex_metric: 0.875092\tvalid_1's binary_logloss: 0.216034\tvalid_1's amex_metric: 0.795745\n",
      "[4500]\ttraining's binary_logloss: 0.173508\ttraining's amex_metric: 0.877041\tvalid_1's binary_logloss: 0.215956\tvalid_1's amex_metric: 0.795814\n",
      "[4600]\ttraining's binary_logloss: 0.17275\ttraining's amex_metric: 0.878574\tvalid_1's binary_logloss: 0.21595\tvalid_1's amex_metric: 0.795363\n",
      "[4700]\ttraining's binary_logloss: 0.171914\ttraining's amex_metric: 0.880309\tvalid_1's binary_logloss: 0.215884\tvalid_1's amex_metric: 0.795805\n",
      "[4800]\ttraining's binary_logloss: 0.171082\ttraining's amex_metric: 0.882128\tvalid_1's binary_logloss: 0.215838\tvalid_1's amex_metric: 0.795884\n",
      "[4900]\ttraining's binary_logloss: 0.17016\ttraining's amex_metric: 0.883665\tvalid_1's binary_logloss: 0.215761\tvalid_1's amex_metric: 0.795504\n",
      "[5000]\ttraining's binary_logloss: 0.169912\ttraining's amex_metric: 0.884896\tvalid_1's binary_logloss: 0.21587\tvalid_1's amex_metric: 0.795425\n",
      "[5100]\ttraining's binary_logloss: 0.168761\ttraining's amex_metric: 0.886329\tvalid_1's binary_logloss: 0.215742\tvalid_1's amex_metric: 0.795932\n",
      "[5200]\ttraining's binary_logloss: 0.167557\ttraining's amex_metric: 0.887965\tvalid_1's binary_logloss: 0.215583\tvalid_1's amex_metric: 0.795929\n",
      "[5300]\ttraining's binary_logloss: 0.166585\ttraining's amex_metric: 0.889582\tvalid_1's binary_logloss: 0.215491\tvalid_1's amex_metric: 0.796102\n",
      "[5400]\ttraining's binary_logloss: 0.165248\ttraining's amex_metric: 0.891534\tvalid_1's binary_logloss: 0.215349\tvalid_1's amex_metric: 0.796399\n",
      "[5500]\ttraining's binary_logloss: 0.164088\ttraining's amex_metric: 0.893135\tvalid_1's binary_logloss: 0.215236\tvalid_1's amex_metric: 0.796828\n",
      "[5600]\ttraining's binary_logloss: 0.162875\ttraining's amex_metric: 0.895093\tvalid_1's binary_logloss: 0.215139\tvalid_1's amex_metric: 0.797332\n",
      "[5700]\ttraining's binary_logloss: 0.161913\ttraining's amex_metric: 0.897003\tvalid_1's binary_logloss: 0.215093\tvalid_1's amex_metric: 0.797368\n",
      "[5800]\ttraining's binary_logloss: 0.160896\ttraining's amex_metric: 0.898719\tvalid_1's binary_logloss: 0.215061\tvalid_1's amex_metric: 0.7971\n",
      "[5900]\ttraining's binary_logloss: 0.159983\ttraining's amex_metric: 0.900315\tvalid_1's binary_logloss: 0.215005\tvalid_1's amex_metric: 0.797264\n",
      "[6000]\ttraining's binary_logloss: 0.158966\ttraining's amex_metric: 0.901764\tvalid_1's binary_logloss: 0.214951\tvalid_1's amex_metric: 0.797362\n",
      "[6100]\ttraining's binary_logloss: 0.157959\ttraining's amex_metric: 0.903444\tvalid_1's binary_logloss: 0.214884\tvalid_1's amex_metric: 0.797422\n",
      "[6200]\ttraining's binary_logloss: 0.156935\ttraining's amex_metric: 0.905234\tvalid_1's binary_logloss: 0.214783\tvalid_1's amex_metric: 0.797476\n",
      "[6300]\ttraining's binary_logloss: 0.156175\ttraining's amex_metric: 0.906848\tvalid_1's binary_logloss: 0.214763\tvalid_1's amex_metric: 0.797846\n",
      "[6400]\ttraining's binary_logloss: 0.155696\ttraining's amex_metric: 0.908111\tvalid_1's binary_logloss: 0.214785\tvalid_1's amex_metric: 0.797808\n",
      "[6500]\ttraining's binary_logloss: 0.154693\ttraining's amex_metric: 0.909525\tvalid_1's binary_logloss: 0.214701\tvalid_1's amex_metric: 0.798015\n",
      "[6600]\ttraining's binary_logloss: 0.153966\ttraining's amex_metric: 0.9109\tvalid_1's binary_logloss: 0.214684\tvalid_1's amex_metric: 0.797957\n",
      "[6700]\ttraining's binary_logloss: 0.152998\ttraining's amex_metric: 0.912438\tvalid_1's binary_logloss: 0.214605\tvalid_1's amex_metric: 0.798587\n",
      "[6800]\ttraining's binary_logloss: 0.152122\ttraining's amex_metric: 0.913763\tvalid_1's binary_logloss: 0.214568\tvalid_1's amex_metric: 0.798298\n",
      "[6900]\ttraining's binary_logloss: 0.15113\ttraining's amex_metric: 0.914917\tvalid_1's binary_logloss: 0.214547\tvalid_1's amex_metric: 0.798193\n",
      "[7000]\ttraining's binary_logloss: 0.150477\ttraining's amex_metric: 0.916237\tvalid_1's binary_logloss: 0.214541\tvalid_1's amex_metric: 0.798406\n",
      "[7100]\ttraining's binary_logloss: 0.149721\ttraining's amex_metric: 0.917615\tvalid_1's binary_logloss: 0.214508\tvalid_1's amex_metric: 0.798311\n",
      "[7200]\ttraining's binary_logloss: 0.148904\ttraining's amex_metric: 0.918749\tvalid_1's binary_logloss: 0.214484\tvalid_1's amex_metric: 0.79811\n",
      "[7300]\ttraining's binary_logloss: 0.148086\ttraining's amex_metric: 0.920015\tvalid_1's binary_logloss: 0.214435\tvalid_1's amex_metric: 0.798753\n",
      "[7400]\ttraining's binary_logloss: 0.147205\ttraining's amex_metric: 0.92123\tvalid_1's binary_logloss: 0.2144\tvalid_1's amex_metric: 0.798212\n",
      "[7500]\ttraining's binary_logloss: 0.146225\ttraining's amex_metric: 0.922705\tvalid_1's binary_logloss: 0.214373\tvalid_1's amex_metric: 0.798174\n",
      "[7600]\ttraining's binary_logloss: 0.145313\ttraining's amex_metric: 0.924247\tvalid_1's binary_logloss: 0.214333\tvalid_1's amex_metric: 0.798352\n",
      "[7700]\ttraining's binary_logloss: 0.144357\ttraining's amex_metric: 0.925399\tvalid_1's binary_logloss: 0.2143\tvalid_1's amex_metric: 0.798044\n",
      "[7800]\ttraining's binary_logloss: 0.143629\ttraining's amex_metric: 0.926619\tvalid_1's binary_logloss: 0.214303\tvalid_1's amex_metric: 0.798421\n",
      "[7900]\ttraining's binary_logloss: 0.142805\ttraining's amex_metric: 0.928162\tvalid_1's binary_logloss: 0.214252\tvalid_1's amex_metric: 0.79871\n",
      "[8000]\ttraining's binary_logloss: 0.141812\ttraining's amex_metric: 0.929704\tvalid_1's binary_logloss: 0.214216\tvalid_1's amex_metric: 0.798636\n",
      "[8100]\ttraining's binary_logloss: 0.141092\ttraining's amex_metric: 0.93104\tvalid_1's binary_logloss: 0.214183\tvalid_1's amex_metric: 0.798793\n",
      "[8200]\ttraining's binary_logloss: 0.140287\ttraining's amex_metric: 0.932158\tvalid_1's binary_logloss: 0.214123\tvalid_1's amex_metric: 0.798794\n",
      "[8300]\ttraining's binary_logloss: 0.139519\ttraining's amex_metric: 0.933474\tvalid_1's binary_logloss: 0.214103\tvalid_1's amex_metric: 0.798991\n",
      "[8400]\ttraining's binary_logloss: 0.138847\ttraining's amex_metric: 0.934686\tvalid_1's binary_logloss: 0.214086\tvalid_1's amex_metric: 0.79908\n",
      "[8500]\ttraining's binary_logloss: 0.138256\ttraining's amex_metric: 0.93571\tvalid_1's binary_logloss: 0.214078\tvalid_1's amex_metric: 0.798621\n",
      "[8600]\ttraining's binary_logloss: 0.137547\ttraining's amex_metric: 0.936725\tvalid_1's binary_logloss: 0.214085\tvalid_1's amex_metric: 0.799205\n",
      "[8700]\ttraining's binary_logloss: 0.136969\ttraining's amex_metric: 0.937797\tvalid_1's binary_logloss: 0.214073\tvalid_1's amex_metric: 0.799167\n",
      "[8800]\ttraining's binary_logloss: 0.136187\ttraining's amex_metric: 0.938895\tvalid_1's binary_logloss: 0.21407\tvalid_1's amex_metric: 0.799207\n",
      "Our fold 0 CV score is 0.7992066857047015\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.325553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327906\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1998\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.488637\ttraining's amex_metric: 0.766494\tvalid_1's binary_logloss: 0.490321\tvalid_1's amex_metric: 0.753107\n",
      "[200]\ttraining's binary_logloss: 0.413533\ttraining's amex_metric: 0.771649\tvalid_1's binary_logloss: 0.416467\tvalid_1's amex_metric: 0.756913\n",
      "[300]\ttraining's binary_logloss: 0.361628\ttraining's amex_metric: 0.776044\tvalid_1's binary_logloss: 0.365622\tvalid_1's amex_metric: 0.759876\n",
      "[400]\ttraining's binary_logloss: 0.32358\ttraining's amex_metric: 0.779704\tvalid_1's binary_logloss: 0.328618\tvalid_1's amex_metric: 0.763124\n",
      "[500]\ttraining's binary_logloss: 0.303588\ttraining's amex_metric: 0.783009\tvalid_1's binary_logloss: 0.309339\tvalid_1's amex_metric: 0.765185\n",
      "[600]\ttraining's binary_logloss: 0.282114\ttraining's amex_metric: 0.786065\tvalid_1's binary_logloss: 0.288797\tvalid_1's amex_metric: 0.767556\n",
      "[700]\ttraining's binary_logloss: 0.271345\ttraining's amex_metric: 0.789347\tvalid_1's binary_logloss: 0.278593\tvalid_1's amex_metric: 0.76973\n",
      "[800]\ttraining's binary_logloss: 0.260607\ttraining's amex_metric: 0.79238\tvalid_1's binary_logloss: 0.268561\tvalid_1's amex_metric: 0.771443\n",
      "[900]\ttraining's binary_logloss: 0.257842\ttraining's amex_metric: 0.794165\tvalid_1's binary_logloss: 0.266111\tvalid_1's amex_metric: 0.773166\n",
      "[1000]\ttraining's binary_logloss: 0.249923\ttraining's amex_metric: 0.796342\tvalid_1's binary_logloss: 0.258905\tvalid_1's amex_metric: 0.774609\n",
      "[1100]\ttraining's binary_logloss: 0.241284\ttraining's amex_metric: 0.799144\tvalid_1's binary_logloss: 0.251198\tvalid_1's amex_metric: 0.775178\n",
      "[1200]\ttraining's binary_logloss: 0.236241\ttraining's amex_metric: 0.8016\tvalid_1's binary_logloss: 0.246859\tvalid_1's amex_metric: 0.776596\n",
      "[1300]\ttraining's binary_logloss: 0.230029\ttraining's amex_metric: 0.804289\tvalid_1's binary_logloss: 0.241639\tvalid_1's amex_metric: 0.777576\n",
      "[1400]\ttraining's binary_logloss: 0.225054\ttraining's amex_metric: 0.807016\tvalid_1's binary_logloss: 0.237601\tvalid_1's amex_metric: 0.779243\n",
      "[1500]\ttraining's binary_logloss: 0.220872\ttraining's amex_metric: 0.809837\tvalid_1's binary_logloss: 0.234405\tvalid_1's amex_metric: 0.780779\n",
      "[1600]\ttraining's binary_logloss: 0.219613\ttraining's amex_metric: 0.811826\tvalid_1's binary_logloss: 0.233669\tvalid_1's amex_metric: 0.78138\n",
      "[1700]\ttraining's binary_logloss: 0.21679\ttraining's amex_metric: 0.814789\tvalid_1's binary_logloss: 0.231715\tvalid_1's amex_metric: 0.782061\n",
      "[1800]\ttraining's binary_logloss: 0.214545\ttraining's amex_metric: 0.816466\tvalid_1's binary_logloss: 0.230254\tvalid_1's amex_metric: 0.783064\n",
      "[1900]\ttraining's binary_logloss: 0.211022\ttraining's amex_metric: 0.819274\tvalid_1's binary_logloss: 0.227939\tvalid_1's amex_metric: 0.784295\n",
      "[2000]\ttraining's binary_logloss: 0.209168\ttraining's amex_metric: 0.821393\tvalid_1's binary_logloss: 0.226912\tvalid_1's amex_metric: 0.7846\n",
      "[2100]\ttraining's binary_logloss: 0.207525\ttraining's amex_metric: 0.823818\tvalid_1's binary_logloss: 0.226087\tvalid_1's amex_metric: 0.785161\n",
      "[2200]\ttraining's binary_logloss: 0.205607\ttraining's amex_metric: 0.825831\tvalid_1's binary_logloss: 0.225132\tvalid_1's amex_metric: 0.78645\n",
      "[2300]\ttraining's binary_logloss: 0.20327\ttraining's amex_metric: 0.828427\tvalid_1's binary_logloss: 0.223999\tvalid_1's amex_metric: 0.787274\n",
      "[2400]\ttraining's binary_logloss: 0.200742\ttraining's amex_metric: 0.830926\tvalid_1's binary_logloss: 0.222872\tvalid_1's amex_metric: 0.787911\n",
      "[2500]\ttraining's binary_logloss: 0.199648\ttraining's amex_metric: 0.833322\tvalid_1's binary_logloss: 0.222582\tvalid_1's amex_metric: 0.789215\n",
      "[2600]\ttraining's binary_logloss: 0.198562\ttraining's amex_metric: 0.835729\tvalid_1's binary_logloss: 0.222336\tvalid_1's amex_metric: 0.78974\n",
      "[2700]\ttraining's binary_logloss: 0.196962\ttraining's amex_metric: 0.837932\tvalid_1's binary_logloss: 0.221801\tvalid_1's amex_metric: 0.790375\n",
      "[2800]\ttraining's binary_logloss: 0.195138\ttraining's amex_metric: 0.840272\tvalid_1's binary_logloss: 0.221212\tvalid_1's amex_metric: 0.790956\n",
      "[2900]\ttraining's binary_logloss: 0.194203\ttraining's amex_metric: 0.842359\tvalid_1's binary_logloss: 0.221037\tvalid_1's amex_metric: 0.791214\n",
      "[3000]\ttraining's binary_logloss: 0.193224\ttraining's amex_metric: 0.844188\tvalid_1's binary_logloss: 0.220847\tvalid_1's amex_metric: 0.790887\n",
      "[3100]\ttraining's binary_logloss: 0.191652\ttraining's amex_metric: 0.84635\tvalid_1's binary_logloss: 0.220398\tvalid_1's amex_metric: 0.791862\n",
      "[3200]\ttraining's binary_logloss: 0.190082\ttraining's amex_metric: 0.848615\tvalid_1's binary_logloss: 0.220005\tvalid_1's amex_metric: 0.791938\n",
      "[3300]\ttraining's binary_logloss: 0.188471\ttraining's amex_metric: 0.850394\tvalid_1's binary_logloss: 0.219627\tvalid_1's amex_metric: 0.791235\n",
      "[3400]\ttraining's binary_logloss: 0.186994\ttraining's amex_metric: 0.852624\tvalid_1's binary_logloss: 0.219305\tvalid_1's amex_metric: 0.792343\n",
      "[3500]\ttraining's binary_logloss: 0.185437\ttraining's amex_metric: 0.855337\tvalid_1's binary_logloss: 0.219024\tvalid_1's amex_metric: 0.793025\n",
      "[3600]\ttraining's binary_logloss: 0.184704\ttraining's amex_metric: 0.857378\tvalid_1's binary_logloss: 0.218974\tvalid_1's amex_metric: 0.793064\n",
      "[3700]\ttraining's binary_logloss: 0.183131\ttraining's amex_metric: 0.859359\tvalid_1's binary_logloss: 0.218697\tvalid_1's amex_metric: 0.793656\n",
      "[3800]\ttraining's binary_logloss: 0.181699\ttraining's amex_metric: 0.861836\tvalid_1's binary_logloss: 0.218477\tvalid_1's amex_metric: 0.793703\n",
      "[3900]\ttraining's binary_logloss: 0.180006\ttraining's amex_metric: 0.863993\tvalid_1's binary_logloss: 0.218206\tvalid_1's amex_metric: 0.793524\n",
      "[4000]\ttraining's binary_logloss: 0.178901\ttraining's amex_metric: 0.866368\tvalid_1's binary_logloss: 0.218087\tvalid_1's amex_metric: 0.793657\n",
      "[4100]\ttraining's binary_logloss: 0.177636\ttraining's amex_metric: 0.868193\tvalid_1's binary_logloss: 0.217917\tvalid_1's amex_metric: 0.793705\n",
      "[4200]\ttraining's binary_logloss: 0.176191\ttraining's amex_metric: 0.870578\tvalid_1's binary_logloss: 0.217751\tvalid_1's amex_metric: 0.793935\n",
      "[4300]\ttraining's binary_logloss: 0.175188\ttraining's amex_metric: 0.87267\tvalid_1's binary_logloss: 0.217704\tvalid_1's amex_metric: 0.794021\n",
      "[4400]\ttraining's binary_logloss: 0.17411\ttraining's amex_metric: 0.8745\tvalid_1's binary_logloss: 0.217614\tvalid_1's amex_metric: 0.794537\n",
      "[4500]\ttraining's binary_logloss: 0.173102\ttraining's amex_metric: 0.876409\tvalid_1's binary_logloss: 0.217529\tvalid_1's amex_metric: 0.794715\n",
      "[4600]\ttraining's binary_logloss: 0.17233\ttraining's amex_metric: 0.878268\tvalid_1's binary_logloss: 0.217498\tvalid_1's amex_metric: 0.794736\n",
      "[4700]\ttraining's binary_logloss: 0.171489\ttraining's amex_metric: 0.879986\tvalid_1's binary_logloss: 0.217469\tvalid_1's amex_metric: 0.794602\n",
      "[4800]\ttraining's binary_logloss: 0.170648\ttraining's amex_metric: 0.881656\tvalid_1's binary_logloss: 0.217444\tvalid_1's amex_metric: 0.795179\n",
      "[4900]\ttraining's binary_logloss: 0.169721\ttraining's amex_metric: 0.883306\tvalid_1's binary_logloss: 0.217346\tvalid_1's amex_metric: 0.795416\n",
      "[5000]\ttraining's binary_logloss: 0.169472\ttraining's amex_metric: 0.884691\tvalid_1's binary_logloss: 0.217439\tvalid_1's amex_metric: 0.794957\n",
      "[5100]\ttraining's binary_logloss: 0.168334\ttraining's amex_metric: 0.885936\tvalid_1's binary_logloss: 0.217318\tvalid_1's amex_metric: 0.794814\n",
      "[5200]\ttraining's binary_logloss: 0.167133\ttraining's amex_metric: 0.887562\tvalid_1's binary_logloss: 0.217206\tvalid_1's amex_metric: 0.795116\n",
      "[5300]\ttraining's binary_logloss: 0.166174\ttraining's amex_metric: 0.889363\tvalid_1's binary_logloss: 0.217134\tvalid_1's amex_metric: 0.795262\n",
      "[5400]\ttraining's binary_logloss: 0.164826\ttraining's amex_metric: 0.891149\tvalid_1's binary_logloss: 0.217001\tvalid_1's amex_metric: 0.795493\n",
      "[5500]\ttraining's binary_logloss: 0.163661\ttraining's amex_metric: 0.892831\tvalid_1's binary_logloss: 0.216891\tvalid_1's amex_metric: 0.795288\n",
      "[5600]\ttraining's binary_logloss: 0.162415\ttraining's amex_metric: 0.894695\tvalid_1's binary_logloss: 0.2168\tvalid_1's amex_metric: 0.795034\n",
      "[5700]\ttraining's binary_logloss: 0.161447\ttraining's amex_metric: 0.896615\tvalid_1's binary_logloss: 0.216754\tvalid_1's amex_metric: 0.795386\n",
      "[5800]\ttraining's binary_logloss: 0.160427\ttraining's amex_metric: 0.898311\tvalid_1's binary_logloss: 0.216677\tvalid_1's amex_metric: 0.795765\n",
      "[5900]\ttraining's binary_logloss: 0.159518\ttraining's amex_metric: 0.899952\tvalid_1's binary_logloss: 0.21664\tvalid_1's amex_metric: 0.795589\n",
      "[6000]\ttraining's binary_logloss: 0.15851\ttraining's amex_metric: 0.901658\tvalid_1's binary_logloss: 0.21657\tvalid_1's amex_metric: 0.796451\n",
      "[6100]\ttraining's binary_logloss: 0.157509\ttraining's amex_metric: 0.903365\tvalid_1's binary_logloss: 0.216523\tvalid_1's amex_metric: 0.796085\n",
      "[6200]\ttraining's binary_logloss: 0.1565\ttraining's amex_metric: 0.905104\tvalid_1's binary_logloss: 0.216468\tvalid_1's amex_metric: 0.795888\n",
      "[6300]\ttraining's binary_logloss: 0.155741\ttraining's amex_metric: 0.906669\tvalid_1's binary_logloss: 0.216467\tvalid_1's amex_metric: 0.795808\n",
      "[6400]\ttraining's binary_logloss: 0.155259\ttraining's amex_metric: 0.90804\tvalid_1's binary_logloss: 0.216466\tvalid_1's amex_metric: 0.795881\n",
      "[6500]\ttraining's binary_logloss: 0.154257\ttraining's amex_metric: 0.909542\tvalid_1's binary_logloss: 0.216439\tvalid_1's amex_metric: 0.796237\n",
      "[6600]\ttraining's binary_logloss: 0.153543\ttraining's amex_metric: 0.910785\tvalid_1's binary_logloss: 0.216396\tvalid_1's amex_metric: 0.796103\n",
      "[6700]\ttraining's binary_logloss: 0.152559\ttraining's amex_metric: 0.912321\tvalid_1's binary_logloss: 0.216375\tvalid_1's amex_metric: 0.796123\n",
      "[6800]\ttraining's binary_logloss: 0.151675\ttraining's amex_metric: 0.913465\tvalid_1's binary_logloss: 0.216357\tvalid_1's amex_metric: 0.796375\n",
      "[6900]\ttraining's binary_logloss: 0.150682\ttraining's amex_metric: 0.914977\tvalid_1's binary_logloss: 0.216323\tvalid_1's amex_metric: 0.796675\n",
      "[7000]\ttraining's binary_logloss: 0.150016\ttraining's amex_metric: 0.916393\tvalid_1's binary_logloss: 0.216297\tvalid_1's amex_metric: 0.796665\n",
      "[7100]\ttraining's binary_logloss: 0.149276\ttraining's amex_metric: 0.917804\tvalid_1's binary_logloss: 0.216295\tvalid_1's amex_metric: 0.796497\n",
      "[7200]\ttraining's binary_logloss: 0.148468\ttraining's amex_metric: 0.919457\tvalid_1's binary_logloss: 0.216265\tvalid_1's amex_metric: 0.796188\n",
      "[7300]\ttraining's binary_logloss: 0.147667\ttraining's amex_metric: 0.920737\tvalid_1's binary_logloss: 0.216268\tvalid_1's amex_metric: 0.796203\n",
      "[7400]\ttraining's binary_logloss: 0.146793\ttraining's amex_metric: 0.921953\tvalid_1's binary_logloss: 0.216265\tvalid_1's amex_metric: 0.796095\n",
      "[7500]\ttraining's binary_logloss: 0.145813\ttraining's amex_metric: 0.923435\tvalid_1's binary_logloss: 0.216216\tvalid_1's amex_metric: 0.796085\n",
      "[7600]\ttraining's binary_logloss: 0.144907\ttraining's amex_metric: 0.924832\tvalid_1's binary_logloss: 0.216202\tvalid_1's amex_metric: 0.796231\n",
      "[7700]\ttraining's binary_logloss: 0.143949\ttraining's amex_metric: 0.926196\tvalid_1's binary_logloss: 0.216165\tvalid_1's amex_metric: 0.796659\n",
      "[7800]\ttraining's binary_logloss: 0.143231\ttraining's amex_metric: 0.927469\tvalid_1's binary_logloss: 0.216155\tvalid_1's amex_metric: 0.796664\n",
      "[7900]\ttraining's binary_logloss: 0.142393\ttraining's amex_metric: 0.928782\tvalid_1's binary_logloss: 0.216147\tvalid_1's amex_metric: 0.796433\n",
      "[8000]\ttraining's binary_logloss: 0.141386\ttraining's amex_metric: 0.930196\tvalid_1's binary_logloss: 0.216134\tvalid_1's amex_metric: 0.79595\n",
      "[8100]\ttraining's binary_logloss: 0.140665\ttraining's amex_metric: 0.931368\tvalid_1's binary_logloss: 0.21611\tvalid_1's amex_metric: 0.796084\n",
      "[8200]\ttraining's binary_logloss: 0.139865\ttraining's amex_metric: 0.932382\tvalid_1's binary_logloss: 0.216096\tvalid_1's amex_metric: 0.795816\n",
      "[8300]\ttraining's binary_logloss: 0.139098\ttraining's amex_metric: 0.933645\tvalid_1's binary_logloss: 0.216085\tvalid_1's amex_metric: 0.795839\n",
      "[8400]\ttraining's binary_logloss: 0.138433\ttraining's amex_metric: 0.934894\tvalid_1's binary_logloss: 0.216057\tvalid_1's amex_metric: 0.796038\n",
      "[8500]\ttraining's binary_logloss: 0.137848\ttraining's amex_metric: 0.936027\tvalid_1's binary_logloss: 0.216056\tvalid_1's amex_metric: 0.795828\n",
      "[8600]\ttraining's binary_logloss: 0.137146\ttraining's amex_metric: 0.936958\tvalid_1's binary_logloss: 0.216027\tvalid_1's amex_metric: 0.795691\n",
      "[8700]\ttraining's binary_logloss: 0.136561\ttraining's amex_metric: 0.938017\tvalid_1's binary_logloss: 0.216\tvalid_1's amex_metric: 0.795806\n",
      "[8800]\ttraining's binary_logloss: 0.135783\ttraining's amex_metric: 0.939191\tvalid_1's binary_logloss: 0.216017\tvalid_1's amex_metric: 0.795693\n",
      "Our fold 1 CV score is 0.795692679299457\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.285878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327873\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 1999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[100]\ttraining's binary_logloss: 0.488861\ttraining's amex_metric: 0.767109\tvalid_1's binary_logloss: 0.490098\tvalid_1's amex_metric: 0.755691\n",
      "[200]\ttraining's binary_logloss: 0.413268\ttraining's amex_metric: 0.771705\tvalid_1's binary_logloss: 0.415558\tvalid_1's amex_metric: 0.758646\n",
      "[300]\ttraining's binary_logloss: 0.361789\ttraining's amex_metric: 0.775762\tvalid_1's binary_logloss: 0.365018\tvalid_1's amex_metric: 0.761479\n",
      "[400]\ttraining's binary_logloss: 0.323847\ttraining's amex_metric: 0.779698\tvalid_1's binary_logloss: 0.328061\tvalid_1's amex_metric: 0.764061\n",
      "[500]\ttraining's binary_logloss: 0.303829\ttraining's amex_metric: 0.782748\tvalid_1's binary_logloss: 0.308728\tvalid_1's amex_metric: 0.766516\n",
      "[600]\ttraining's binary_logloss: 0.282343\ttraining's amex_metric: 0.78573\tvalid_1's binary_logloss: 0.288166\tvalid_1's amex_metric: 0.76947\n",
      "[700]\ttraining's binary_logloss: 0.271542\ttraining's amex_metric: 0.788756\tvalid_1's binary_logloss: 0.277979\tvalid_1's amex_metric: 0.770613\n",
      "[800]\ttraining's binary_logloss: 0.260829\ttraining's amex_metric: 0.792066\tvalid_1's binary_logloss: 0.268037\tvalid_1's amex_metric: 0.771666\n",
      "[900]\ttraining's binary_logloss: 0.258002\ttraining's amex_metric: 0.79377\tvalid_1's binary_logloss: 0.265537\tvalid_1's amex_metric: 0.77333\n",
      "[1000]\ttraining's binary_logloss: 0.249956\ttraining's amex_metric: 0.796194\tvalid_1's binary_logloss: 0.258218\tvalid_1's amex_metric: 0.774261\n",
      "[1100]\ttraining's binary_logloss: 0.241316\ttraining's amex_metric: 0.799055\tvalid_1's binary_logloss: 0.250509\tvalid_1's amex_metric: 0.776315\n",
      "[1200]\ttraining's binary_logloss: 0.236229\ttraining's amex_metric: 0.801562\tvalid_1's binary_logloss: 0.24614\tvalid_1's amex_metric: 0.77697\n",
      "[1300]\ttraining's binary_logloss: 0.229962\ttraining's amex_metric: 0.804332\tvalid_1's binary_logloss: 0.2409\tvalid_1's amex_metric: 0.777992\n",
      "[1400]\ttraining's binary_logloss: 0.224966\ttraining's amex_metric: 0.806966\tvalid_1's binary_logloss: 0.236907\tvalid_1's amex_metric: 0.779026\n",
      "[1500]\ttraining's binary_logloss: 0.220784\ttraining's amex_metric: 0.810324\tvalid_1's binary_logloss: 0.233779\tvalid_1's amex_metric: 0.780379\n",
      "[1600]\ttraining's binary_logloss: 0.219576\ttraining's amex_metric: 0.812446\tvalid_1's binary_logloss: 0.2331\tvalid_1's amex_metric: 0.780979\n",
      "[1700]\ttraining's binary_logloss: 0.216756\ttraining's amex_metric: 0.814793\tvalid_1's binary_logloss: 0.231166\tvalid_1's amex_metric: 0.78186\n",
      "[1800]\ttraining's binary_logloss: 0.2145\ttraining's amex_metric: 0.81661\tvalid_1's binary_logloss: 0.229771\tvalid_1's amex_metric: 0.782471\n",
      "[1900]\ttraining's binary_logloss: 0.210978\ttraining's amex_metric: 0.819451\tvalid_1's binary_logloss: 0.227589\tvalid_1's amex_metric: 0.783091\n",
      "[2000]\ttraining's binary_logloss: 0.209111\ttraining's amex_metric: 0.821669\tvalid_1's binary_logloss: 0.226648\tvalid_1's amex_metric: 0.783247\n",
      "[2100]\ttraining's binary_logloss: 0.207485\ttraining's amex_metric: 0.823924\tvalid_1's binary_logloss: 0.225868\tvalid_1's amex_metric: 0.784211\n",
      "[2200]\ttraining's binary_logloss: 0.205532\ttraining's amex_metric: 0.826103\tvalid_1's binary_logloss: 0.224948\tvalid_1's amex_metric: 0.784873\n",
      "[2300]\ttraining's binary_logloss: 0.203206\ttraining's amex_metric: 0.828455\tvalid_1's binary_logloss: 0.223891\tvalid_1's amex_metric: 0.785422\n",
      "[2400]\ttraining's binary_logloss: 0.200681\ttraining's amex_metric: 0.831603\tvalid_1's binary_logloss: 0.222802\tvalid_1's amex_metric: 0.786471\n",
      "[2500]\ttraining's binary_logloss: 0.199602\ttraining's amex_metric: 0.833836\tvalid_1's binary_logloss: 0.222529\tvalid_1's amex_metric: 0.786673\n",
      "[2600]\ttraining's binary_logloss: 0.198505\ttraining's amex_metric: 0.835999\tvalid_1's binary_logloss: 0.222242\tvalid_1's amex_metric: 0.787251\n",
      "[2700]\ttraining's binary_logloss: 0.196909\ttraining's amex_metric: 0.83841\tvalid_1's binary_logloss: 0.22176\tvalid_1's amex_metric: 0.787393\n",
      "[2800]\ttraining's binary_logloss: 0.195099\ttraining's amex_metric: 0.84067\tvalid_1's binary_logloss: 0.221182\tvalid_1's amex_metric: 0.78777\n",
      "[2900]\ttraining's binary_logloss: 0.194154\ttraining's amex_metric: 0.842956\tvalid_1's binary_logloss: 0.220986\tvalid_1's amex_metric: 0.788323\n",
      "[3000]\ttraining's binary_logloss: 0.193165\ttraining's amex_metric: 0.844538\tvalid_1's binary_logloss: 0.220788\tvalid_1's amex_metric: 0.788432\n",
      "[3100]\ttraining's binary_logloss: 0.191619\ttraining's amex_metric: 0.84664\tvalid_1's binary_logloss: 0.220392\tvalid_1's amex_metric: 0.789245\n",
      "[3200]\ttraining's binary_logloss: 0.190053\ttraining's amex_metric: 0.849018\tvalid_1's binary_logloss: 0.219985\tvalid_1's amex_metric: 0.789649\n",
      "[3300]\ttraining's binary_logloss: 0.188454\ttraining's amex_metric: 0.851473\tvalid_1's binary_logloss: 0.219616\tvalid_1's amex_metric: 0.789621\n",
      "[3400]\ttraining's binary_logloss: 0.186988\ttraining's amex_metric: 0.85347\tvalid_1's binary_logloss: 0.219313\tvalid_1's amex_metric: 0.789798\n",
      "[3500]\ttraining's binary_logloss: 0.185411\ttraining's amex_metric: 0.855972\tvalid_1's binary_logloss: 0.219011\tvalid_1's amex_metric: 0.789976\n",
      "[3600]\ttraining's binary_logloss: 0.184664\ttraining's amex_metric: 0.857711\tvalid_1's binary_logloss: 0.219016\tvalid_1's amex_metric: 0.790115\n",
      "[3700]\ttraining's binary_logloss: 0.183078\ttraining's amex_metric: 0.860018\tvalid_1's binary_logloss: 0.218727\tvalid_1's amex_metric: 0.79011\n",
      "[3800]\ttraining's binary_logloss: 0.181646\ttraining's amex_metric: 0.862233\tvalid_1's binary_logloss: 0.218507\tvalid_1's amex_metric: 0.790372\n",
      "[3900]\ttraining's binary_logloss: 0.179952\ttraining's amex_metric: 0.864839\tvalid_1's binary_logloss: 0.21823\tvalid_1's amex_metric: 0.791023\n",
      "[4000]\ttraining's binary_logloss: 0.178833\ttraining's amex_metric: 0.867213\tvalid_1's binary_logloss: 0.21811\tvalid_1's amex_metric: 0.791198\n",
      "[4100]\ttraining's binary_logloss: 0.177574\ttraining's amex_metric: 0.869328\tvalid_1's binary_logloss: 0.218007\tvalid_1's amex_metric: 0.791579\n",
      "[4200]\ttraining's binary_logloss: 0.176124\ttraining's amex_metric: 0.871663\tvalid_1's binary_logloss: 0.217802\tvalid_1's amex_metric: 0.791399\n",
      "[4300]\ttraining's binary_logloss: 0.175138\ttraining's amex_metric: 0.87373\tvalid_1's binary_logloss: 0.21772\tvalid_1's amex_metric: 0.791272\n",
      "[4400]\ttraining's binary_logloss: 0.174061\ttraining's amex_metric: 0.875742\tvalid_1's binary_logloss: 0.217616\tvalid_1's amex_metric: 0.792278\n",
      "[4500]\ttraining's binary_logloss: 0.173058\ttraining's amex_metric: 0.87779\tvalid_1's binary_logloss: 0.217507\tvalid_1's amex_metric: 0.792087\n",
      "[4600]\ttraining's binary_logloss: 0.172293\ttraining's amex_metric: 0.87944\tvalid_1's binary_logloss: 0.217502\tvalid_1's amex_metric: 0.792329\n",
      "[4700]\ttraining's binary_logloss: 0.171481\ttraining's amex_metric: 0.880982\tvalid_1's binary_logloss: 0.217446\tvalid_1's amex_metric: 0.792182\n",
      "[4800]\ttraining's binary_logloss: 0.170658\ttraining's amex_metric: 0.882413\tvalid_1's binary_logloss: 0.217426\tvalid_1's amex_metric: 0.792441\n",
      "[4900]\ttraining's binary_logloss: 0.169731\ttraining's amex_metric: 0.884034\tvalid_1's binary_logloss: 0.217356\tvalid_1's amex_metric: 0.79259\n",
      "[5000]\ttraining's binary_logloss: 0.169494\ttraining's amex_metric: 0.885444\tvalid_1's binary_logloss: 0.217441\tvalid_1's amex_metric: 0.792258\n",
      "[5100]\ttraining's binary_logloss: 0.16835\ttraining's amex_metric: 0.886818\tvalid_1's binary_logloss: 0.217311\tvalid_1's amex_metric: 0.79244\n",
      "[5200]\ttraining's binary_logloss: 0.16713\ttraining's amex_metric: 0.88829\tvalid_1's binary_logloss: 0.217189\tvalid_1's amex_metric: 0.792328\n",
      "[5300]\ttraining's binary_logloss: 0.16619\ttraining's amex_metric: 0.890072\tvalid_1's binary_logloss: 0.217131\tvalid_1's amex_metric: 0.792403\n",
      "[5400]\ttraining's binary_logloss: 0.164853\ttraining's amex_metric: 0.891781\tvalid_1's binary_logloss: 0.216992\tvalid_1's amex_metric: 0.792131\n",
      "[5500]\ttraining's binary_logloss: 0.16371\ttraining's amex_metric: 0.893484\tvalid_1's binary_logloss: 0.216918\tvalid_1's amex_metric: 0.792184\n",
      "[5600]\ttraining's binary_logloss: 0.162478\ttraining's amex_metric: 0.895478\tvalid_1's binary_logloss: 0.216821\tvalid_1's amex_metric: 0.792335\n",
      "[5700]\ttraining's binary_logloss: 0.161526\ttraining's amex_metric: 0.897207\tvalid_1's binary_logloss: 0.21676\tvalid_1's amex_metric: 0.79223\n",
      "[5800]\ttraining's binary_logloss: 0.160501\ttraining's amex_metric: 0.898891\tvalid_1's binary_logloss: 0.216668\tvalid_1's amex_metric: 0.792572\n",
      "[5900]\ttraining's binary_logloss: 0.159587\ttraining's amex_metric: 0.900712\tvalid_1's binary_logloss: 0.216629\tvalid_1's amex_metric: 0.792964\n",
      "[6000]\ttraining's binary_logloss: 0.158584\ttraining's amex_metric: 0.902427\tvalid_1's binary_logloss: 0.216591\tvalid_1's amex_metric: 0.793144\n",
      "[6100]\ttraining's binary_logloss: 0.157578\ttraining's amex_metric: 0.904002\tvalid_1's binary_logloss: 0.216538\tvalid_1's amex_metric: 0.793054\n",
      "[6200]\ttraining's binary_logloss: 0.156562\ttraining's amex_metric: 0.905428\tvalid_1's binary_logloss: 0.216479\tvalid_1's amex_metric: 0.792712\n",
      "[6300]\ttraining's binary_logloss: 0.155793\ttraining's amex_metric: 0.907006\tvalid_1's binary_logloss: 0.216437\tvalid_1's amex_metric: 0.793232\n",
      "[6400]\ttraining's binary_logloss: 0.155302\ttraining's amex_metric: 0.908392\tvalid_1's binary_logloss: 0.216461\tvalid_1's amex_metric: 0.792808\n",
      "[6500]\ttraining's binary_logloss: 0.154307\ttraining's amex_metric: 0.90999\tvalid_1's binary_logloss: 0.216425\tvalid_1's amex_metric: 0.792561\n",
      "[6600]\ttraining's binary_logloss: 0.153595\ttraining's amex_metric: 0.911245\tvalid_1's binary_logloss: 0.216394\tvalid_1's amex_metric: 0.792741\n",
      "[6700]\ttraining's binary_logloss: 0.152623\ttraining's amex_metric: 0.912623\tvalid_1's binary_logloss: 0.216364\tvalid_1's amex_metric: 0.793017\n",
      "[6800]\ttraining's binary_logloss: 0.151738\ttraining's amex_metric: 0.914201\tvalid_1's binary_logloss: 0.216299\tvalid_1's amex_metric: 0.792661\n",
      "[6900]\ttraining's binary_logloss: 0.150735\ttraining's amex_metric: 0.915599\tvalid_1's binary_logloss: 0.216253\tvalid_1's amex_metric: 0.792921\n",
      "[7000]\ttraining's binary_logloss: 0.15007\ttraining's amex_metric: 0.917077\tvalid_1's binary_logloss: 0.21623\tvalid_1's amex_metric: 0.793036\n",
      "[7100]\ttraining's binary_logloss: 0.149317\ttraining's amex_metric: 0.918318\tvalid_1's binary_logloss: 0.216193\tvalid_1's amex_metric: 0.79288\n",
      "[7200]\ttraining's binary_logloss: 0.148528\ttraining's amex_metric: 0.919675\tvalid_1's binary_logloss: 0.216171\tvalid_1's amex_metric: 0.792466\n",
      "[7300]\ttraining's binary_logloss: 0.147714\ttraining's amex_metric: 0.921083\tvalid_1's binary_logloss: 0.216132\tvalid_1's amex_metric: 0.793111\n",
      "[7400]\ttraining's binary_logloss: 0.146851\ttraining's amex_metric: 0.922134\tvalid_1's binary_logloss: 0.216121\tvalid_1's amex_metric: 0.79332\n",
      "[7500]\ttraining's binary_logloss: 0.145881\ttraining's amex_metric: 0.923599\tvalid_1's binary_logloss: 0.216079\tvalid_1's amex_metric: 0.793564\n",
      "[7600]\ttraining's binary_logloss: 0.144977\ttraining's amex_metric: 0.925053\tvalid_1's binary_logloss: 0.216074\tvalid_1's amex_metric: 0.793312\n",
      "[7700]\ttraining's binary_logloss: 0.144017\ttraining's amex_metric: 0.926444\tvalid_1's binary_logloss: 0.21606\tvalid_1's amex_metric: 0.79285\n",
      "[7800]\ttraining's binary_logloss: 0.1433\ttraining's amex_metric: 0.927655\tvalid_1's binary_logloss: 0.216051\tvalid_1's amex_metric: 0.793169\n",
      "[7900]\ttraining's binary_logloss: 0.142471\ttraining's amex_metric: 0.929025\tvalid_1's binary_logloss: 0.216009\tvalid_1's amex_metric: 0.793352\n",
      "[8000]\ttraining's binary_logloss: 0.141465\ttraining's amex_metric: 0.93033\tvalid_1's binary_logloss: 0.215961\tvalid_1's amex_metric: 0.793367\n",
      "[8100]\ttraining's binary_logloss: 0.140737\ttraining's amex_metric: 0.931637\tvalid_1's binary_logloss: 0.215945\tvalid_1's amex_metric: 0.793543\n",
      "[8200]\ttraining's binary_logloss: 0.139932\ttraining's amex_metric: 0.932744\tvalid_1's binary_logloss: 0.215937\tvalid_1's amex_metric: 0.793377\n",
      "[8300]\ttraining's binary_logloss: 0.139163\ttraining's amex_metric: 0.933984\tvalid_1's binary_logloss: 0.215925\tvalid_1's amex_metric: 0.793445\n",
      "[8400]\ttraining's binary_logloss: 0.138495\ttraining's amex_metric: 0.935119\tvalid_1's binary_logloss: 0.215919\tvalid_1's amex_metric: 0.793784\n",
      "[8500]\ttraining's binary_logloss: 0.137909\ttraining's amex_metric: 0.93629\tvalid_1's binary_logloss: 0.215887\tvalid_1's amex_metric: 0.793587\n",
      "[8600]\ttraining's binary_logloss: 0.137192\ttraining's amex_metric: 0.937576\tvalid_1's binary_logloss: 0.215892\tvalid_1's amex_metric: 0.793417\n",
      "[8700]\ttraining's binary_logloss: 0.136604\ttraining's amex_metric: 0.938609\tvalid_1's binary_logloss: 0.215892\tvalid_1's amex_metric: 0.793563\n",
      "[8800]\ttraining's binary_logloss: 0.135818\ttraining's amex_metric: 0.939525\tvalid_1's binary_logloss: 0.215853\tvalid_1's amex_metric: 0.793303\n",
      "[8900]\ttraining's binary_logloss: 0.135123\ttraining's amex_metric: 0.940448\tvalid_1's binary_logloss: 0.215855\tvalid_1's amex_metric: 0.793576\n",
      "[9000]\ttraining's binary_logloss: 0.134359\ttraining's amex_metric: 0.941694\tvalid_1's binary_logloss: 0.215847\tvalid_1's amex_metric: 0.793853\n",
      "[9100]\ttraining's binary_logloss: 0.133749\ttraining's amex_metric: 0.942453\tvalid_1's binary_logloss: 0.215854\tvalid_1's amex_metric: 0.793725\n",
      "[9200]\ttraining's binary_logloss: 0.133293\ttraining's amex_metric: 0.943452\tvalid_1's binary_logloss: 0.215855\tvalid_1's amex_metric: 0.79383\n",
      "[9300]\ttraining's binary_logloss: 0.132714\ttraining's amex_metric: 0.944317\tvalid_1's binary_logloss: 0.215836\tvalid_1's amex_metric: 0.793837\n",
      "[9400]\ttraining's binary_logloss: 0.132043\ttraining's amex_metric: 0.945248\tvalid_1's binary_logloss: 0.215834\tvalid_1's amex_metric: 0.793837\n",
      "[9500]\ttraining's binary_logloss: 0.13147\ttraining's amex_metric: 0.946171\tvalid_1's binary_logloss: 0.215853\tvalid_1's amex_metric: 0.793411\n",
      "[9600]\ttraining's binary_logloss: 0.130746\ttraining's amex_metric: 0.94687\tvalid_1's binary_logloss: 0.215845\tvalid_1's amex_metric: 0.793246\n",
      "[9700]\ttraining's binary_logloss: 0.130101\ttraining's amex_metric: 0.947786\tvalid_1's binary_logloss: 0.215843\tvalid_1's amex_metric: 0.792995\n",
      "[9800]\ttraining's binary_logloss: 0.129193\ttraining's amex_metric: 0.948935\tvalid_1's binary_logloss: 0.215846\tvalid_1's amex_metric: 0.793058\n",
      "[9900]\ttraining's binary_logloss: 0.128653\ttraining's amex_metric: 0.949975\tvalid_1's binary_logloss: 0.21583\tvalid_1's amex_metric: 0.793653\n",
      "[10000]\ttraining's binary_logloss: 0.127964\ttraining's amex_metric: 0.95095\tvalid_1's binary_logloss: 0.215819\tvalid_1's amex_metric: 0.7937\n",
      "[10100]\ttraining's binary_logloss: 0.12709\ttraining's amex_metric: 0.952084\tvalid_1's binary_logloss: 0.215787\tvalid_1's amex_metric: 0.793778\n",
      "[10200]\ttraining's binary_logloss: 0.126434\ttraining's amex_metric: 0.953071\tvalid_1's binary_logloss: 0.215787\tvalid_1's amex_metric: 0.793884\n",
      "[10300]\ttraining's binary_logloss: 0.125702\ttraining's amex_metric: 0.954018\tvalid_1's binary_logloss: 0.215793\tvalid_1's amex_metric: 0.793948\n",
      "Our fold 2 CV score is 0.7939480362739684\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.243287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327878\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 2000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.489342\ttraining's amex_metric: 0.764517\tvalid_1's binary_logloss: 0.489814\tvalid_1's amex_metric: 0.75956\n",
      "[200]\ttraining's binary_logloss: 0.413852\ttraining's amex_metric: 0.770072\tvalid_1's binary_logloss: 0.414945\tvalid_1's amex_metric: 0.761181\n",
      "[300]\ttraining's binary_logloss: 0.362082\ttraining's amex_metric: 0.774646\tvalid_1's binary_logloss: 0.36386\tvalid_1's amex_metric: 0.763969\n",
      "[400]\ttraining's binary_logloss: 0.324181\ttraining's amex_metric: 0.779093\tvalid_1's binary_logloss: 0.326672\tvalid_1's amex_metric: 0.766816\n",
      "[500]\ttraining's binary_logloss: 0.30422\ttraining's amex_metric: 0.7823\tvalid_1's binary_logloss: 0.307219\tvalid_1's amex_metric: 0.769017\n",
      "[600]\ttraining's binary_logloss: 0.282996\ttraining's amex_metric: 0.785467\tvalid_1's binary_logloss: 0.286726\tvalid_1's amex_metric: 0.770755\n",
      "[700]\ttraining's binary_logloss: 0.27235\ttraining's amex_metric: 0.788812\tvalid_1's binary_logloss: 0.276581\tvalid_1's amex_metric: 0.772107\n",
      "[800]\ttraining's binary_logloss: 0.261635\ttraining's amex_metric: 0.791156\tvalid_1's binary_logloss: 0.26654\tvalid_1's amex_metric: 0.774288\n",
      "[900]\ttraining's binary_logloss: 0.258732\ttraining's amex_metric: 0.793316\tvalid_1's binary_logloss: 0.264005\tvalid_1's amex_metric: 0.77563\n",
      "[1000]\ttraining's binary_logloss: 0.250696\ttraining's amex_metric: 0.795441\tvalid_1's binary_logloss: 0.256582\tvalid_1's amex_metric: 0.776264\n",
      "[1100]\ttraining's binary_logloss: 0.242067\ttraining's amex_metric: 0.79822\tvalid_1's binary_logloss: 0.248782\tvalid_1's amex_metric: 0.777487\n",
      "[1200]\ttraining's binary_logloss: 0.236905\ttraining's amex_metric: 0.800206\tvalid_1's binary_logloss: 0.244288\tvalid_1's amex_metric: 0.778713\n",
      "[1300]\ttraining's binary_logloss: 0.230543\ttraining's amex_metric: 0.803343\tvalid_1's binary_logloss: 0.238865\tvalid_1's amex_metric: 0.780668\n",
      "[1400]\ttraining's binary_logloss: 0.225546\ttraining's amex_metric: 0.805734\tvalid_1's binary_logloss: 0.234765\tvalid_1's amex_metric: 0.782449\n",
      "[1500]\ttraining's binary_logloss: 0.2214\ttraining's amex_metric: 0.808696\tvalid_1's binary_logloss: 0.231646\tvalid_1's amex_metric: 0.783328\n",
      "[1600]\ttraining's binary_logloss: 0.220184\ttraining's amex_metric: 0.810907\tvalid_1's binary_logloss: 0.230975\tvalid_1's amex_metric: 0.784389\n",
      "[1700]\ttraining's binary_logloss: 0.217421\ttraining's amex_metric: 0.813366\tvalid_1's binary_logloss: 0.229063\tvalid_1's amex_metric: 0.785756\n",
      "[1800]\ttraining's binary_logloss: 0.215194\ttraining's amex_metric: 0.815424\tvalid_1's binary_logloss: 0.227648\tvalid_1's amex_metric: 0.785866\n",
      "[1900]\ttraining's binary_logloss: 0.2117\ttraining's amex_metric: 0.818268\tvalid_1's binary_logloss: 0.225425\tvalid_1's amex_metric: 0.78684\n",
      "[2000]\ttraining's binary_logloss: 0.209852\ttraining's amex_metric: 0.8204\tvalid_1's binary_logloss: 0.224481\tvalid_1's amex_metric: 0.787508\n",
      "[2100]\ttraining's binary_logloss: 0.208225\ttraining's amex_metric: 0.82267\tvalid_1's binary_logloss: 0.223706\tvalid_1's amex_metric: 0.788701\n",
      "[2200]\ttraining's binary_logloss: 0.206261\ttraining's amex_metric: 0.825276\tvalid_1's binary_logloss: 0.222729\tvalid_1's amex_metric: 0.789249\n",
      "[2300]\ttraining's binary_logloss: 0.203957\ttraining's amex_metric: 0.82792\tvalid_1's binary_logloss: 0.221634\tvalid_1's amex_metric: 0.790195\n",
      "[2400]\ttraining's binary_logloss: 0.201435\ttraining's amex_metric: 0.83078\tvalid_1's binary_logloss: 0.220506\tvalid_1's amex_metric: 0.790328\n",
      "[2500]\ttraining's binary_logloss: 0.200333\ttraining's amex_metric: 0.833188\tvalid_1's binary_logloss: 0.220212\tvalid_1's amex_metric: 0.791567\n",
      "[2600]\ttraining's binary_logloss: 0.199236\ttraining's amex_metric: 0.835083\tvalid_1's binary_logloss: 0.219943\tvalid_1's amex_metric: 0.791969\n",
      "[2700]\ttraining's binary_logloss: 0.197629\ttraining's amex_metric: 0.83727\tvalid_1's binary_logloss: 0.219409\tvalid_1's amex_metric: 0.792524\n",
      "[2800]\ttraining's binary_logloss: 0.195819\ttraining's amex_metric: 0.839708\tvalid_1's binary_logloss: 0.21884\tvalid_1's amex_metric: 0.792743\n",
      "[2900]\ttraining's binary_logloss: 0.194887\ttraining's amex_metric: 0.841801\tvalid_1's binary_logloss: 0.218702\tvalid_1's amex_metric: 0.792506\n",
      "[3000]\ttraining's binary_logloss: 0.193907\ttraining's amex_metric: 0.843644\tvalid_1's binary_logloss: 0.218514\tvalid_1's amex_metric: 0.793038\n",
      "[3100]\ttraining's binary_logloss: 0.192352\ttraining's amex_metric: 0.845636\tvalid_1's binary_logloss: 0.218061\tvalid_1's amex_metric: 0.793377\n",
      "[3200]\ttraining's binary_logloss: 0.1908\ttraining's amex_metric: 0.847689\tvalid_1's binary_logloss: 0.21765\tvalid_1's amex_metric: 0.79406\n",
      "[3300]\ttraining's binary_logloss: 0.189194\ttraining's amex_metric: 0.849993\tvalid_1's binary_logloss: 0.217241\tvalid_1's amex_metric: 0.794439\n",
      "[3400]\ttraining's binary_logloss: 0.187716\ttraining's amex_metric: 0.852411\tvalid_1's binary_logloss: 0.216883\tvalid_1's amex_metric: 0.79505\n",
      "[3500]\ttraining's binary_logloss: 0.186145\ttraining's amex_metric: 0.854055\tvalid_1's binary_logloss: 0.216566\tvalid_1's amex_metric: 0.795827\n",
      "[3600]\ttraining's binary_logloss: 0.185402\ttraining's amex_metric: 0.855957\tvalid_1's binary_logloss: 0.216568\tvalid_1's amex_metric: 0.79623\n",
      "[3700]\ttraining's binary_logloss: 0.183802\ttraining's amex_metric: 0.858304\tvalid_1's binary_logloss: 0.21626\tvalid_1's amex_metric: 0.795784\n",
      "[3800]\ttraining's binary_logloss: 0.182361\ttraining's amex_metric: 0.860439\tvalid_1's binary_logloss: 0.216055\tvalid_1's amex_metric: 0.795944\n",
      "[3900]\ttraining's binary_logloss: 0.180662\ttraining's amex_metric: 0.862932\tvalid_1's binary_logloss: 0.215796\tvalid_1's amex_metric: 0.79637\n",
      "[4000]\ttraining's binary_logloss: 0.179532\ttraining's amex_metric: 0.865402\tvalid_1's binary_logloss: 0.215626\tvalid_1's amex_metric: 0.796212\n",
      "[4100]\ttraining's binary_logloss: 0.17826\ttraining's amex_metric: 0.867513\tvalid_1's binary_logloss: 0.215474\tvalid_1's amex_metric: 0.796313\n",
      "[4200]\ttraining's binary_logloss: 0.176849\ttraining's amex_metric: 0.869645\tvalid_1's binary_logloss: 0.215284\tvalid_1's amex_metric: 0.797035\n",
      "[4300]\ttraining's binary_logloss: 0.17584\ttraining's amex_metric: 0.872067\tvalid_1's binary_logloss: 0.215217\tvalid_1's amex_metric: 0.797027\n",
      "[4400]\ttraining's binary_logloss: 0.174776\ttraining's amex_metric: 0.874093\tvalid_1's binary_logloss: 0.215128\tvalid_1's amex_metric: 0.796977\n",
      "[4500]\ttraining's binary_logloss: 0.173766\ttraining's amex_metric: 0.876\tvalid_1's binary_logloss: 0.215073\tvalid_1's amex_metric: 0.797183\n",
      "[4600]\ttraining's binary_logloss: 0.172992\ttraining's amex_metric: 0.877956\tvalid_1's binary_logloss: 0.215034\tvalid_1's amex_metric: 0.796748\n",
      "[4700]\ttraining's binary_logloss: 0.17216\ttraining's amex_metric: 0.87923\tvalid_1's binary_logloss: 0.215024\tvalid_1's amex_metric: 0.796924\n",
      "[4800]\ttraining's binary_logloss: 0.171325\ttraining's amex_metric: 0.880766\tvalid_1's binary_logloss: 0.214959\tvalid_1's amex_metric: 0.796779\n",
      "[4900]\ttraining's binary_logloss: 0.170402\ttraining's amex_metric: 0.882471\tvalid_1's binary_logloss: 0.214895\tvalid_1's amex_metric: 0.797084\n",
      "[5000]\ttraining's binary_logloss: 0.170159\ttraining's amex_metric: 0.883459\tvalid_1's binary_logloss: 0.214962\tvalid_1's amex_metric: 0.797228\n",
      "[5100]\ttraining's binary_logloss: 0.169029\ttraining's amex_metric: 0.884893\tvalid_1's binary_logloss: 0.214842\tvalid_1's amex_metric: 0.79717\n",
      "[5200]\ttraining's binary_logloss: 0.167802\ttraining's amex_metric: 0.886294\tvalid_1's binary_logloss: 0.214691\tvalid_1's amex_metric: 0.797311\n",
      "[5300]\ttraining's binary_logloss: 0.166847\ttraining's amex_metric: 0.888004\tvalid_1's binary_logloss: 0.214618\tvalid_1's amex_metric: 0.797351\n",
      "[5400]\ttraining's binary_logloss: 0.165494\ttraining's amex_metric: 0.889719\tvalid_1's binary_logloss: 0.214488\tvalid_1's amex_metric: 0.796959\n",
      "[5500]\ttraining's binary_logloss: 0.16434\ttraining's amex_metric: 0.891518\tvalid_1's binary_logloss: 0.214359\tvalid_1's amex_metric: 0.796975\n",
      "[5600]\ttraining's binary_logloss: 0.163112\ttraining's amex_metric: 0.893474\tvalid_1's binary_logloss: 0.214276\tvalid_1's amex_metric: 0.7972\n",
      "[5700]\ttraining's binary_logloss: 0.16214\ttraining's amex_metric: 0.895163\tvalid_1's binary_logloss: 0.21422\tvalid_1's amex_metric: 0.79743\n",
      "[5800]\ttraining's binary_logloss: 0.161123\ttraining's amex_metric: 0.897051\tvalid_1's binary_logloss: 0.214147\tvalid_1's amex_metric: 0.797175\n",
      "[5900]\ttraining's binary_logloss: 0.160218\ttraining's amex_metric: 0.899027\tvalid_1's binary_logloss: 0.214107\tvalid_1's amex_metric: 0.796787\n",
      "[6000]\ttraining's binary_logloss: 0.159214\ttraining's amex_metric: 0.90072\tvalid_1's binary_logloss: 0.214044\tvalid_1's amex_metric: 0.796758\n",
      "[6100]\ttraining's binary_logloss: 0.158198\ttraining's amex_metric: 0.902404\tvalid_1's binary_logloss: 0.214003\tvalid_1's amex_metric: 0.797038\n",
      "[6200]\ttraining's binary_logloss: 0.157181\ttraining's amex_metric: 0.903694\tvalid_1's binary_logloss: 0.213947\tvalid_1's amex_metric: 0.797007\n",
      "[6300]\ttraining's binary_logloss: 0.156418\ttraining's amex_metric: 0.905552\tvalid_1's binary_logloss: 0.213926\tvalid_1's amex_metric: 0.797612\n",
      "[6400]\ttraining's binary_logloss: 0.155929\ttraining's amex_metric: 0.907009\tvalid_1's binary_logloss: 0.21393\tvalid_1's amex_metric: 0.797433\n",
      "[6500]\ttraining's binary_logloss: 0.154941\ttraining's amex_metric: 0.908572\tvalid_1's binary_logloss: 0.213887\tvalid_1's amex_metric: 0.797269\n",
      "[6600]\ttraining's binary_logloss: 0.15423\ttraining's amex_metric: 0.909825\tvalid_1's binary_logloss: 0.213859\tvalid_1's amex_metric: 0.797382\n",
      "[6700]\ttraining's binary_logloss: 0.153248\ttraining's amex_metric: 0.911043\tvalid_1's binary_logloss: 0.2138\tvalid_1's amex_metric: 0.797329\n",
      "[6800]\ttraining's binary_logloss: 0.152368\ttraining's amex_metric: 0.912338\tvalid_1's binary_logloss: 0.21372\tvalid_1's amex_metric: 0.797183\n",
      "[6900]\ttraining's binary_logloss: 0.151371\ttraining's amex_metric: 0.913655\tvalid_1's binary_logloss: 0.213662\tvalid_1's amex_metric: 0.79768\n",
      "[7000]\ttraining's binary_logloss: 0.150719\ttraining's amex_metric: 0.914821\tvalid_1's binary_logloss: 0.213661\tvalid_1's amex_metric: 0.797409\n",
      "[7100]\ttraining's binary_logloss: 0.149973\ttraining's amex_metric: 0.916652\tvalid_1's binary_logloss: 0.213644\tvalid_1's amex_metric: 0.797584\n",
      "[7200]\ttraining's binary_logloss: 0.149174\ttraining's amex_metric: 0.918067\tvalid_1's binary_logloss: 0.213627\tvalid_1's amex_metric: 0.798176\n",
      "[7300]\ttraining's binary_logloss: 0.148362\ttraining's amex_metric: 0.919424\tvalid_1's binary_logloss: 0.21358\tvalid_1's amex_metric: 0.79821\n",
      "[7400]\ttraining's binary_logloss: 0.147476\ttraining's amex_metric: 0.920971\tvalid_1's binary_logloss: 0.213558\tvalid_1's amex_metric: 0.797729\n",
      "[7500]\ttraining's binary_logloss: 0.14649\ttraining's amex_metric: 0.922295\tvalid_1's binary_logloss: 0.213511\tvalid_1's amex_metric: 0.797422\n",
      "[7600]\ttraining's binary_logloss: 0.145562\ttraining's amex_metric: 0.923401\tvalid_1's binary_logloss: 0.213448\tvalid_1's amex_metric: 0.798029\n",
      "[7700]\ttraining's binary_logloss: 0.144597\ttraining's amex_metric: 0.924868\tvalid_1's binary_logloss: 0.213409\tvalid_1's amex_metric: 0.797615\n",
      "[7800]\ttraining's binary_logloss: 0.14388\ttraining's amex_metric: 0.926257\tvalid_1's binary_logloss: 0.213373\tvalid_1's amex_metric: 0.798241\n",
      "[7900]\ttraining's binary_logloss: 0.143048\ttraining's amex_metric: 0.927831\tvalid_1's binary_logloss: 0.213351\tvalid_1's amex_metric: 0.798458\n",
      "[8000]\ttraining's binary_logloss: 0.142043\ttraining's amex_metric: 0.929155\tvalid_1's binary_logloss: 0.213318\tvalid_1's amex_metric: 0.798318\n",
      "[8100]\ttraining's binary_logloss: 0.141319\ttraining's amex_metric: 0.930411\tvalid_1's binary_logloss: 0.213296\tvalid_1's amex_metric: 0.797694\n",
      "[8200]\ttraining's binary_logloss: 0.140526\ttraining's amex_metric: 0.931712\tvalid_1's binary_logloss: 0.213289\tvalid_1's amex_metric: 0.797886\n",
      "[8300]\ttraining's binary_logloss: 0.139763\ttraining's amex_metric: 0.932963\tvalid_1's binary_logloss: 0.213273\tvalid_1's amex_metric: 0.79852\n",
      "[8400]\ttraining's binary_logloss: 0.139104\ttraining's amex_metric: 0.934006\tvalid_1's binary_logloss: 0.213253\tvalid_1's amex_metric: 0.798209\n",
      "[8500]\ttraining's binary_logloss: 0.138509\ttraining's amex_metric: 0.935002\tvalid_1's binary_logloss: 0.213213\tvalid_1's amex_metric: 0.798268\n",
      "[8600]\ttraining's binary_logloss: 0.137792\ttraining's amex_metric: 0.93613\tvalid_1's binary_logloss: 0.213213\tvalid_1's amex_metric: 0.798014\n",
      "[8700]\ttraining's binary_logloss: 0.137206\ttraining's amex_metric: 0.937324\tvalid_1's binary_logloss: 0.213245\tvalid_1's amex_metric: 0.797455\n",
      "[8800]\ttraining's binary_logloss: 0.136427\ttraining's amex_metric: 0.93848\tvalid_1's binary_logloss: 0.213225\tvalid_1's amex_metric: 0.797543\n",
      "[8900]\ttraining's binary_logloss: 0.135751\ttraining's amex_metric: 0.939375\tvalid_1's binary_logloss: 0.213197\tvalid_1's amex_metric: 0.797279\n",
      "[9000]\ttraining's binary_logloss: 0.134985\ttraining's amex_metric: 0.940473\tvalid_1's binary_logloss: 0.21319\tvalid_1's amex_metric: 0.797214\n",
      "[9100]\ttraining's binary_logloss: 0.134371\ttraining's amex_metric: 0.941651\tvalid_1's binary_logloss: 0.213199\tvalid_1's amex_metric: 0.797149\n",
      "[9200]\ttraining's binary_logloss: 0.13392\ttraining's amex_metric: 0.942611\tvalid_1's binary_logloss: 0.213209\tvalid_1's amex_metric: 0.797252\n",
      "[9300]\ttraining's binary_logloss: 0.133332\ttraining's amex_metric: 0.943761\tvalid_1's binary_logloss: 0.213208\tvalid_1's amex_metric: 0.797504\n",
      "[9400]\ttraining's binary_logloss: 0.132649\ttraining's amex_metric: 0.944726\tvalid_1's binary_logloss: 0.21317\tvalid_1's amex_metric: 0.797328\n",
      "[9500]\ttraining's binary_logloss: 0.132084\ttraining's amex_metric: 0.945458\tvalid_1's binary_logloss: 0.21318\tvalid_1's amex_metric: 0.797702\n",
      "[9600]\ttraining's binary_logloss: 0.131361\ttraining's amex_metric: 0.946551\tvalid_1's binary_logloss: 0.213166\tvalid_1's amex_metric: 0.797324\n",
      "[9700]\ttraining's binary_logloss: 0.130701\ttraining's amex_metric: 0.947543\tvalid_1's binary_logloss: 0.213159\tvalid_1's amex_metric: 0.797959\n",
      "Our fold 3 CV score is 0.797958740810314\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2011 features...\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.351989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 327565\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 1996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[100]\ttraining's binary_logloss: 0.488946\ttraining's amex_metric: 0.765194\tvalid_1's binary_logloss: 0.489998\tvalid_1's amex_metric: 0.758528\n",
      "[200]\ttraining's binary_logloss: 0.413671\ttraining's amex_metric: 0.770427\tvalid_1's binary_logloss: 0.415544\tvalid_1's amex_metric: 0.762801\n",
      "[300]\ttraining's binary_logloss: 0.362039\ttraining's amex_metric: 0.77487\tvalid_1's binary_logloss: 0.364688\tvalid_1's amex_metric: 0.765904\n",
      "[400]\ttraining's binary_logloss: 0.324285\ttraining's amex_metric: 0.778941\tvalid_1's binary_logloss: 0.327717\tvalid_1's amex_metric: 0.769449\n",
      "[500]\ttraining's binary_logloss: 0.304335\ttraining's amex_metric: 0.781345\tvalid_1's binary_logloss: 0.308335\tvalid_1's amex_metric: 0.770617\n",
      "[600]\ttraining's binary_logloss: 0.282886\ttraining's amex_metric: 0.78443\tvalid_1's binary_logloss: 0.287599\tvalid_1's amex_metric: 0.772862\n",
      "[700]\ttraining's binary_logloss: 0.27211\ttraining's amex_metric: 0.787859\tvalid_1's binary_logloss: 0.277366\tvalid_1's amex_metric: 0.774939\n",
      "[800]\ttraining's binary_logloss: 0.261405\ttraining's amex_metric: 0.791038\tvalid_1's binary_logloss: 0.267293\tvalid_1's amex_metric: 0.776502\n",
      "[900]\ttraining's binary_logloss: 0.258568\ttraining's amex_metric: 0.793029\tvalid_1's binary_logloss: 0.264832\tvalid_1's amex_metric: 0.778215\n",
      "[1000]\ttraining's binary_logloss: 0.250542\ttraining's amex_metric: 0.795636\tvalid_1's binary_logloss: 0.257455\tvalid_1's amex_metric: 0.778836\n",
      "[1100]\ttraining's binary_logloss: 0.241882\ttraining's amex_metric: 0.798134\tvalid_1's binary_logloss: 0.249606\tvalid_1's amex_metric: 0.780982\n",
      "[1200]\ttraining's binary_logloss: 0.236762\ttraining's amex_metric: 0.800757\tvalid_1's binary_logloss: 0.245201\tvalid_1's amex_metric: 0.782637\n",
      "[1300]\ttraining's binary_logloss: 0.230467\ttraining's amex_metric: 0.803603\tvalid_1's binary_logloss: 0.239804\tvalid_1's amex_metric: 0.784092\n",
      "[1400]\ttraining's binary_logloss: 0.225486\ttraining's amex_metric: 0.806151\tvalid_1's binary_logloss: 0.235721\tvalid_1's amex_metric: 0.784292\n",
      "[1500]\ttraining's binary_logloss: 0.22136\ttraining's amex_metric: 0.808851\tvalid_1's binary_logloss: 0.23255\tvalid_1's amex_metric: 0.785823\n",
      "[1600]\ttraining's binary_logloss: 0.22015\ttraining's amex_metric: 0.811339\tvalid_1's binary_logloss: 0.231907\tvalid_1's amex_metric: 0.78611\n",
      "[1700]\ttraining's binary_logloss: 0.217348\ttraining's amex_metric: 0.813631\tvalid_1's binary_logloss: 0.229933\tvalid_1's amex_metric: 0.78654\n",
      "[1800]\ttraining's binary_logloss: 0.215079\ttraining's amex_metric: 0.815512\tvalid_1's binary_logloss: 0.228472\tvalid_1's amex_metric: 0.787058\n",
      "[1900]\ttraining's binary_logloss: 0.211587\ttraining's amex_metric: 0.818257\tvalid_1's binary_logloss: 0.22615\tvalid_1's amex_metric: 0.788498\n",
      "[2000]\ttraining's binary_logloss: 0.209742\ttraining's amex_metric: 0.820593\tvalid_1's binary_logloss: 0.225153\tvalid_1's amex_metric: 0.789191\n",
      "[2100]\ttraining's binary_logloss: 0.208127\ttraining's amex_metric: 0.823158\tvalid_1's binary_logloss: 0.224366\tvalid_1's amex_metric: 0.789885\n",
      "[2200]\ttraining's binary_logloss: 0.20619\ttraining's amex_metric: 0.825205\tvalid_1's binary_logloss: 0.223424\tvalid_1's amex_metric: 0.790826\n",
      "[2300]\ttraining's binary_logloss: 0.203876\ttraining's amex_metric: 0.827974\tvalid_1's binary_logloss: 0.222294\tvalid_1's amex_metric: 0.791158\n",
      "[2400]\ttraining's binary_logloss: 0.20138\ttraining's amex_metric: 0.830568\tvalid_1's binary_logloss: 0.221138\tvalid_1's amex_metric: 0.791518\n",
      "[2500]\ttraining's binary_logloss: 0.200297\ttraining's amex_metric: 0.832925\tvalid_1's binary_logloss: 0.220843\tvalid_1's amex_metric: 0.791661\n",
      "[2600]\ttraining's binary_logloss: 0.199192\ttraining's amex_metric: 0.835262\tvalid_1's binary_logloss: 0.220528\tvalid_1's amex_metric: 0.792062\n",
      "[2700]\ttraining's binary_logloss: 0.197594\ttraining's amex_metric: 0.837104\tvalid_1's binary_logloss: 0.219996\tvalid_1's amex_metric: 0.792834\n",
      "[2800]\ttraining's binary_logloss: 0.195776\ttraining's amex_metric: 0.83948\tvalid_1's binary_logloss: 0.219301\tvalid_1's amex_metric: 0.792719\n",
      "[2900]\ttraining's binary_logloss: 0.194852\ttraining's amex_metric: 0.841451\tvalid_1's binary_logloss: 0.219103\tvalid_1's amex_metric: 0.793299\n",
      "[3000]\ttraining's binary_logloss: 0.193859\ttraining's amex_metric: 0.843239\tvalid_1's binary_logloss: 0.218883\tvalid_1's amex_metric: 0.794347\n",
      "[3100]\ttraining's binary_logloss: 0.192308\ttraining's amex_metric: 0.845332\tvalid_1's binary_logloss: 0.218451\tvalid_1's amex_metric: 0.793883\n",
      "[3200]\ttraining's binary_logloss: 0.190761\ttraining's amex_metric: 0.847817\tvalid_1's binary_logloss: 0.218051\tvalid_1's amex_metric: 0.794475\n",
      "[3300]\ttraining's binary_logloss: 0.189164\ttraining's amex_metric: 0.850129\tvalid_1's binary_logloss: 0.21766\tvalid_1's amex_metric: 0.794719\n",
      "[3400]\ttraining's binary_logloss: 0.187706\ttraining's amex_metric: 0.852199\tvalid_1's binary_logloss: 0.21736\tvalid_1's amex_metric: 0.79495\n",
      "[3500]\ttraining's binary_logloss: 0.186128\ttraining's amex_metric: 0.854595\tvalid_1's binary_logloss: 0.217001\tvalid_1's amex_metric: 0.795214\n",
      "[3600]\ttraining's binary_logloss: 0.185394\ttraining's amex_metric: 0.856572\tvalid_1's binary_logloss: 0.216967\tvalid_1's amex_metric: 0.794921\n",
      "[3700]\ttraining's binary_logloss: 0.183819\ttraining's amex_metric: 0.858828\tvalid_1's binary_logloss: 0.21666\tvalid_1's amex_metric: 0.795686\n",
      "[3800]\ttraining's binary_logloss: 0.182379\ttraining's amex_metric: 0.861253\tvalid_1's binary_logloss: 0.216396\tvalid_1's amex_metric: 0.796123\n",
      "[3900]\ttraining's binary_logloss: 0.180691\ttraining's amex_metric: 0.863067\tvalid_1's binary_logloss: 0.216063\tvalid_1's amex_metric: 0.796693\n",
      "[4000]\ttraining's binary_logloss: 0.179567\ttraining's amex_metric: 0.865378\tvalid_1's binary_logloss: 0.215936\tvalid_1's amex_metric: 0.796977\n",
      "[4100]\ttraining's binary_logloss: 0.178289\ttraining's amex_metric: 0.867582\tvalid_1's binary_logloss: 0.21577\tvalid_1's amex_metric: 0.79677\n",
      "[4200]\ttraining's binary_logloss: 0.176839\ttraining's amex_metric: 0.870068\tvalid_1's binary_logloss: 0.215559\tvalid_1's amex_metric: 0.796775\n",
      "[4300]\ttraining's binary_logloss: 0.175853\ttraining's amex_metric: 0.872262\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex_metric: 0.797211\n",
      "[4400]\ttraining's binary_logloss: 0.174787\ttraining's amex_metric: 0.874316\tvalid_1's binary_logloss: 0.215367\tvalid_1's amex_metric: 0.797668\n",
      "[4500]\ttraining's binary_logloss: 0.173764\ttraining's amex_metric: 0.875985\tvalid_1's binary_logloss: 0.21526\tvalid_1's amex_metric: 0.797515\n",
      "[4600]\ttraining's binary_logloss: 0.172997\ttraining's amex_metric: 0.877866\tvalid_1's binary_logloss: 0.215266\tvalid_1's amex_metric: 0.797423\n",
      "[4700]\ttraining's binary_logloss: 0.172147\ttraining's amex_metric: 0.879695\tvalid_1's binary_logloss: 0.215225\tvalid_1's amex_metric: 0.797316\n",
      "[4800]\ttraining's binary_logloss: 0.171336\ttraining's amex_metric: 0.881063\tvalid_1's binary_logloss: 0.215199\tvalid_1's amex_metric: 0.797766\n",
      "[4900]\ttraining's binary_logloss: 0.170411\ttraining's amex_metric: 0.882434\tvalid_1's binary_logloss: 0.215138\tvalid_1's amex_metric: 0.797752\n",
      "[5000]\ttraining's binary_logloss: 0.170163\ttraining's amex_metric: 0.883847\tvalid_1's binary_logloss: 0.215215\tvalid_1's amex_metric: 0.798192\n",
      "[5100]\ttraining's binary_logloss: 0.169021\ttraining's amex_metric: 0.885351\tvalid_1's binary_logloss: 0.215055\tvalid_1's amex_metric: 0.798435\n",
      "[5200]\ttraining's binary_logloss: 0.167819\ttraining's amex_metric: 0.886891\tvalid_1's binary_logloss: 0.21491\tvalid_1's amex_metric: 0.798131\n",
      "[5300]\ttraining's binary_logloss: 0.166878\ttraining's amex_metric: 0.888498\tvalid_1's binary_logloss: 0.214851\tvalid_1's amex_metric: 0.798369\n",
      "[5400]\ttraining's binary_logloss: 0.165513\ttraining's amex_metric: 0.890292\tvalid_1's binary_logloss: 0.214671\tvalid_1's amex_metric: 0.798603\n",
      "[5500]\ttraining's binary_logloss: 0.164356\ttraining's amex_metric: 0.892006\tvalid_1's binary_logloss: 0.214561\tvalid_1's amex_metric: 0.798648\n",
      "[5600]\ttraining's binary_logloss: 0.163141\ttraining's amex_metric: 0.893793\tvalid_1's binary_logloss: 0.214448\tvalid_1's amex_metric: 0.798986\n",
      "[5700]\ttraining's binary_logloss: 0.162169\ttraining's amex_metric: 0.89567\tvalid_1's binary_logloss: 0.21441\tvalid_1's amex_metric: 0.798953\n",
      "[5800]\ttraining's binary_logloss: 0.161152\ttraining's amex_metric: 0.897391\tvalid_1's binary_logloss: 0.214371\tvalid_1's amex_metric: 0.799028\n",
      "[5900]\ttraining's binary_logloss: 0.160229\ttraining's amex_metric: 0.898831\tvalid_1's binary_logloss: 0.21433\tvalid_1's amex_metric: 0.799127\n",
      "[6000]\ttraining's binary_logloss: 0.159213\ttraining's amex_metric: 0.900462\tvalid_1's binary_logloss: 0.214278\tvalid_1's amex_metric: 0.799177\n",
      "[6100]\ttraining's binary_logloss: 0.158209\ttraining's amex_metric: 0.902155\tvalid_1's binary_logloss: 0.214234\tvalid_1's amex_metric: 0.799166\n",
      "[6200]\ttraining's binary_logloss: 0.157189\ttraining's amex_metric: 0.903788\tvalid_1's binary_logloss: 0.214176\tvalid_1's amex_metric: 0.798964\n",
      "[6300]\ttraining's binary_logloss: 0.156417\ttraining's amex_metric: 0.905296\tvalid_1's binary_logloss: 0.214151\tvalid_1's amex_metric: 0.799042\n",
      "[6400]\ttraining's binary_logloss: 0.155931\ttraining's amex_metric: 0.90644\tvalid_1's binary_logloss: 0.214194\tvalid_1's amex_metric: 0.799206\n",
      "[6500]\ttraining's binary_logloss: 0.154941\ttraining's amex_metric: 0.907842\tvalid_1's binary_logloss: 0.214159\tvalid_1's amex_metric: 0.799245\n",
      "[6600]\ttraining's binary_logloss: 0.154226\ttraining's amex_metric: 0.909144\tvalid_1's binary_logloss: 0.214151\tvalid_1's amex_metric: 0.799207\n",
      "[6700]\ttraining's binary_logloss: 0.153259\ttraining's amex_metric: 0.910442\tvalid_1's binary_logloss: 0.214083\tvalid_1's amex_metric: 0.79949\n",
      "[6800]\ttraining's binary_logloss: 0.152363\ttraining's amex_metric: 0.911793\tvalid_1's binary_logloss: 0.214071\tvalid_1's amex_metric: 0.799593\n",
      "[6900]\ttraining's binary_logloss: 0.15137\ttraining's amex_metric: 0.913391\tvalid_1's binary_logloss: 0.214005\tvalid_1's amex_metric: 0.799583\n",
      "[7000]\ttraining's binary_logloss: 0.150715\ttraining's amex_metric: 0.914761\tvalid_1's binary_logloss: 0.214012\tvalid_1's amex_metric: 0.799857\n",
      "[7100]\ttraining's binary_logloss: 0.149968\ttraining's amex_metric: 0.916101\tvalid_1's binary_logloss: 0.214035\tvalid_1's amex_metric: 0.799701\n",
      "[7200]\ttraining's binary_logloss: 0.149181\ttraining's amex_metric: 0.917643\tvalid_1's binary_logloss: 0.213994\tvalid_1's amex_metric: 0.799924\n",
      "[7300]\ttraining's binary_logloss: 0.148364\ttraining's amex_metric: 0.919212\tvalid_1's binary_logloss: 0.213947\tvalid_1's amex_metric: 0.799982\n",
      "[7400]\ttraining's binary_logloss: 0.147488\ttraining's amex_metric: 0.920733\tvalid_1's binary_logloss: 0.213895\tvalid_1's amex_metric: 0.799785\n",
      "[7500]\ttraining's binary_logloss: 0.146488\ttraining's amex_metric: 0.922022\tvalid_1's binary_logloss: 0.213802\tvalid_1's amex_metric: 0.799534\n",
      "[7600]\ttraining's binary_logloss: 0.145571\ttraining's amex_metric: 0.923481\tvalid_1's binary_logloss: 0.213762\tvalid_1's amex_metric: 0.799753\n",
      "[7700]\ttraining's binary_logloss: 0.144603\ttraining's amex_metric: 0.925085\tvalid_1's binary_logloss: 0.213742\tvalid_1's amex_metric: 0.800133\n",
      "[7800]\ttraining's binary_logloss: 0.143883\ttraining's amex_metric: 0.926527\tvalid_1's binary_logloss: 0.213723\tvalid_1's amex_metric: 0.799717\n",
      "[7900]\ttraining's binary_logloss: 0.143053\ttraining's amex_metric: 0.927872\tvalid_1's binary_logloss: 0.21369\tvalid_1's amex_metric: 0.799474\n",
      "[8000]\ttraining's binary_logloss: 0.142058\ttraining's amex_metric: 0.929265\tvalid_1's binary_logloss: 0.213679\tvalid_1's amex_metric: 0.799912\n",
      "[8100]\ttraining's binary_logloss: 0.141336\ttraining's amex_metric: 0.930372\tvalid_1's binary_logloss: 0.21369\tvalid_1's amex_metric: 0.799823\n",
      "[8200]\ttraining's binary_logloss: 0.140536\ttraining's amex_metric: 0.931591\tvalid_1's binary_logloss: 0.213669\tvalid_1's amex_metric: 0.799278\n",
      "[8300]\ttraining's binary_logloss: 0.139765\ttraining's amex_metric: 0.933157\tvalid_1's binary_logloss: 0.213642\tvalid_1's amex_metric: 0.799454\n",
      "[8400]\ttraining's binary_logloss: 0.139098\ttraining's amex_metric: 0.934228\tvalid_1's binary_logloss: 0.213637\tvalid_1's amex_metric: 0.799561\n",
      "[8500]\ttraining's binary_logloss: 0.13851\ttraining's amex_metric: 0.935376\tvalid_1's binary_logloss: 0.213648\tvalid_1's amex_metric: 0.799727\n",
      "[8600]\ttraining's binary_logloss: 0.137797\ttraining's amex_metric: 0.936215\tvalid_1's binary_logloss: 0.213679\tvalid_1's amex_metric: 0.799289\n",
      "[8700]\ttraining's binary_logloss: 0.137222\ttraining's amex_metric: 0.937251\tvalid_1's binary_logloss: 0.213694\tvalid_1's amex_metric: 0.799494\n",
      "[8800]\ttraining's binary_logloss: 0.136441\ttraining's amex_metric: 0.938373\tvalid_1's binary_logloss: 0.213709\tvalid_1's amex_metric: 0.799526\n",
      "[8900]\ttraining's binary_logloss: 0.135755\ttraining's amex_metric: 0.939455\tvalid_1's binary_logloss: 0.213711\tvalid_1's amex_metric: 0.79929\n",
      "[9000]\ttraining's binary_logloss: 0.134991\ttraining's amex_metric: 0.940557\tvalid_1's binary_logloss: 0.2137\tvalid_1's amex_metric: 0.799692\n",
      "[9100]\ttraining's binary_logloss: 0.134377\ttraining's amex_metric: 0.941576\tvalid_1's binary_logloss: 0.213687\tvalid_1's amex_metric: 0.799696\n",
      "[9200]\ttraining's binary_logloss: 0.133924\ttraining's amex_metric: 0.942589\tvalid_1's binary_logloss: 0.213659\tvalid_1's amex_metric: 0.799605\n",
      "[9300]\ttraining's binary_logloss: 0.133344\ttraining's amex_metric: 0.943458\tvalid_1's binary_logloss: 0.213662\tvalid_1's amex_metric: 0.799411\n",
      "[9400]\ttraining's binary_logloss: 0.132657\ttraining's amex_metric: 0.944392\tvalid_1's binary_logloss: 0.213655\tvalid_1's amex_metric: 0.79939\n",
      "[9500]\ttraining's binary_logloss: 0.132088\ttraining's amex_metric: 0.945444\tvalid_1's binary_logloss: 0.21363\tvalid_1's amex_metric: 0.800198\n",
      "Our fold 4 CV score is 0.8001977303020998\n",
      "Our out of folds CV score is 0.7969425363638722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "epoch = [8800,8800,10300,9700,9500]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "#     des = DartEarlyStopping(\"valid_1\", CFG.metric, 1000)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = epoch[fold],#10500\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1500,\n",
    "#         eval_metric=[lgb_amex_metric],\n",
    "        verbose_eval = 100,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{CFG.output_dir}lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "    \n",
    "    cids.extend(train[\"customer_ID\"].loc[val_ind])\n",
    "    tr_target.extend(train[\"target\"].loc[val_ind])\n",
    "    \n",
    "    # Predict the test set\n",
    "    test_pred = model.predict(test[features])\n",
    "    test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "    \n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[CFG.target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    \"tabnet_oot\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store out of folds predictions\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "# oof_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n",
    "\n",
    "# # Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'../output/Amex LGBM Dart CV 0.7977/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f08ac4-d86f-4dcd-b281-4788f52bcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634fb39d-280e-43dd-82af-6e7d23f4e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 0 CV score is 0.7992066857047015\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 1 CV score is 0.795692679299457\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 2 CV score is 0.7939480362739684\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 3 CV score is 0.797958740810314\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2011 features...\n",
      "pred test\n",
      "pred val\n",
      "Our fold 4 CV score is 0.8001977303020998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "\n",
    "# kmeans_list = [\"kmeans pred 2\",\"kmeans pred 3\",\"kmeans pred 4\"]\n",
    "\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "# cat_features.extend(kmeans_list)\n",
    "\n",
    "for cat_col in cat_features:\n",
    "#     print(cat_col)\n",
    "    encoder = LabelEncoder()\n",
    "    train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "    test[cat_col] = encoder.transform(test[cat_col])\n",
    "\n",
    "\n",
    "features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': CFG.metric,\n",
    "    'boosting': CFG.boosting_type,\n",
    "    'seed': CFG.seed,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    }\n",
    "\n",
    "# Create a numpy array to store test predictions\n",
    "test_predictions = []\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = []\n",
    "cids = []\n",
    "tr_target = []\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(features)} features...')\n",
    "    x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "    y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "    \n",
    "    \n",
    "    # model = lgb.LGBMClassifier()\n",
    "    with open(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl', 'rb') as web:\n",
    "        model = pickle.load(web)\n",
    "    # model.load_model(f'{CFG.output_dir}lgbm_dart_fold{fold}_seed614.pkl')\n",
    "    \n",
    "    print(\"pred test\")\n",
    "    test_predictions.append(model.predict(test[features]))\n",
    "    print(\"pred val\")\n",
    "    val_pred = model.predict(x_val)\n",
    "    oof_predictions.extend(val_pred)\n",
    "    cids.extend(train[\"customer_ID\"].iloc[val_ind])\n",
    "    tr_target.extend(y_val)\n",
    "    \n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    del x_train, x_val, y_train, y_val,  lgb_train ,lgb_valid\n",
    "    del model\n",
    "    gc.collect()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4b85b-1c5c-4b4b-b3e7-dea2470726d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca73b844-d691-4829-aeca-841426a24943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our out of folds CV score is 0.7969425363638722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dic_oof = {\n",
    "    \"customer_ID\":cids,\n",
    "    \"target\":tr_target,\n",
    "    f\"{CFG.ver}_{CFG.model}_oof\":oof_predictions\n",
    "}\n",
    "\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame(dic_oof)\n",
    "# oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "\n",
    "# Compute out of folds metric\n",
    "score = amex_metric(oof_df[\"target\"], oof_df[f\"{CFG.ver}_{CFG.model}_oof\"])\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "\n",
    "oof_df.to_csv(f'{CFG.output_dir}oof_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec43c9c-9829-4275-911c-758a2bb8f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.mean(test_predictions,axis = 0)\n",
    "\n",
    "# Create a dataframe to store test prediction\n",
    "test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_pred})\n",
    "# test_df.to_csv(f'{CFG.output_dir}test_{CFG.model}_{score}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "test_df.to_csv(f'{CFG.output_dir}test_{CFG.ver}_{CFG.model}_{score}_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b80690-de04-4f36-bc0c-6e9bc48f7f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
