{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f58d85-14e4-4371-ad96-458843ea228c",
   "metadata": {},
   "source": [
    "# exp04\n",
    "\n",
    "exp03のXGB版\n",
    "\n",
    "https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325051a6-7ea3-4398-b022-6a81c18b14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "import joblib\n",
    "import random\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce478709-32b7-4d68-bda7-4928785a13b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31216\\217815574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# Read & Preprocess Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31216\\217815574.py\u001b[0m in \u001b[0;36mread_preprocess_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# ====================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/data/train.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'customer_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     cat_features = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         handles = get_handle(\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\amex\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Get the difference\n",
    "# ====================================================\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "# ====================================================\n",
    "# Read & preprocess data and save it to disk\n",
    "# ====================================================\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('/content/data/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-fe/test_fe.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('../input/amex-fe/train_fe.parquet')\n",
    "    test.to_parquet('../input/amex-fe/test_fe.parquet')\n",
    "\n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e80847-25f2-4784-97c5-b8012eae1a3e",
   "metadata": {},
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68ef141-3c78-4cd6-988b-db0421753882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    input_dir = '../input/amex-fe/'\n",
    "    seed = 45\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    boosting_type = 'dart'\n",
    "    metric = 'binary_logloss'\n",
    "    model = \"XGB\"\n",
    "\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n",
    "    return train, test\n",
    "\n",
    "# ====================================================\n",
    "# XGB train\n",
    "# ====================================================\n",
    "\n",
    "def xgb_train(x, y, xt, yt,pram):\n",
    "    print(\"# of features:\", x.shape[1])\n",
    "    assert x.shape[1] == xt.shape[1]\n",
    "    dtrain = xgb.DMatrix(data=x, label=y)\n",
    "    dvalid = xgb.DMatrix(data=xt, label=yt)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    bst = xgb.train(pram, dtrain=dtrain,\n",
    "                num_boost_round=3000,evals=watchlist,#2600\n",
    "                early_stopping_rounds=500, feval=xgb_amex, maximize=True,# xgb_amex feval=amex_metric, \n",
    "                verbose_eval=100)\n",
    "    print('best ntree_limit:', bst.best_ntree_limit)\n",
    "    print('best score:', bst.best_score)\n",
    "    return bst.predict(dvalid, iteration_range=(0,bst.best_ntree_limit)), bst\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Amex metric\n",
    "# ====================================================\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "\n",
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "\n",
    "# Created by https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Train & Evaluate\n",
    "# ====================================================\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    # params = {\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': CFG.metric,\n",
    "    #     'boosting': CFG.boosting_type,\n",
    "    #     'seed': CFG.seed,\n",
    "    #     'num_leaves': 100,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'feature_fraction': 0.20,\n",
    "    #     'bagging_freq': 10,\n",
    "    #     'bagging_fraction': 0.50,\n",
    "    #     'n_jobs': -1,\n",
    "    #     'lambda_l2': 2,\n",
    "    #     'min_data_in_leaf': 40,\n",
    "    #     }\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic', \n",
    "        'tree_method': 'hist', #gpu_hist #hist\n",
    "        'max_depth': 7,\n",
    "        'subsample':0.88,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'gamma':1.5,\n",
    "        'min_child_weight':8,\n",
    "        'lambda':70,\n",
    "        'eta':0.03}\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {foldamex_metric_np with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        # lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        # lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        \n",
    "        val_pred , model = xgb_train(x_train, y_train, x_val, y_val, params)\n",
    "        \n",
    "        # Save best model\n",
    "        joblib.dump(model, f'../output/exp04 XGB lag feature/{CFG.model}_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        \n",
    "        # val_pred = model.predict(x_val)\n",
    "        \n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        \n",
    "        \n",
    "        dtest = xgb.DMatrix(data=test[features])\n",
    "        test_pred = model.predict(dtest)\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        \n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'../output/exp04 XGB lag feature/oof_{CFG.model}_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'../output/exp04 XGB lag feature/test_{CFG.model}_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30007ce2-4854-4fd8-8c1b-477936511e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d5aa37-ce2a-4fd7-a211-d4665fe081b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 903/903 [02:15<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 1188 features...\n",
      "# of features: 1188\n",
      "[23:35:27] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67365\ttrain-amex:0.70960\teval-logloss:0.67375\teval-amex:0.70654\n",
      "[100]\ttrain-logloss:0.24134\ttrain-amex:0.78090\teval-logloss:0.24729\teval-amex:0.76708\n",
      "[200]\ttrain-logloss:0.21602\ttrain-amex:0.79849\teval-logloss:0.22696\teval-amex:0.77707\n",
      "[300]\ttrain-logloss:0.20692\ttrain-amex:0.81158\teval-logloss:0.22235\teval-amex:0.78366\n",
      "[400]\ttrain-logloss:0.20106\ttrain-amex:0.82065\teval-logloss:0.22034\teval-amex:0.78641\n",
      "[500]\ttrain-logloss:0.19635\ttrain-amex:0.82866\teval-logloss:0.21913\teval-amex:0.78730\n",
      "[600]\ttrain-logloss:0.19215\ttrain-amex:0.83556\teval-logloss:0.21841\teval-amex:0.78886\n",
      "[700]\ttrain-logloss:0.18843\ttrain-amex:0.84228\teval-logloss:0.21792\teval-amex:0.79019\n",
      "[800]\ttrain-logloss:0.18479\ttrain-amex:0.84879\teval-logloss:0.21751\teval-amex:0.79108\n",
      "[900]\ttrain-logloss:0.18116\ttrain-amex:0.85537\teval-logloss:0.21722\teval-amex:0.79128\n",
      "[1000]\ttrain-logloss:0.17776\ttrain-amex:0.86135\teval-logloss:0.21697\teval-amex:0.79218\n",
      "[1100]\ttrain-logloss:0.17443\ttrain-amex:0.86774\teval-logloss:0.21676\teval-amex:0.79230\n",
      "[1200]\ttrain-logloss:0.17130\ttrain-amex:0.87322\teval-logloss:0.21660\teval-amex:0.79242\n",
      "[1300]\ttrain-logloss:0.16812\ttrain-amex:0.87877\teval-logloss:0.21648\teval-amex:0.79261\n",
      "[1400]\ttrain-logloss:0.16521\ttrain-amex:0.88398\teval-logloss:0.21637\teval-amex:0.79270\n",
      "[1500]\ttrain-logloss:0.16234\ttrain-amex:0.88909\teval-logloss:0.21629\teval-amex:0.79303\n",
      "[1600]\ttrain-logloss:0.15971\ttrain-amex:0.89359\teval-logloss:0.21622\teval-amex:0.79291\n",
      "[1700]\ttrain-logloss:0.15686\ttrain-amex:0.89886\teval-logloss:0.21616\teval-amex:0.79306\n",
      "[1800]\ttrain-logloss:0.15413\ttrain-amex:0.90354\teval-logloss:0.21613\teval-amex:0.79396\n",
      "[1900]\ttrain-logloss:0.15150\ttrain-amex:0.90792\teval-logloss:0.21611\teval-amex:0.79362\n",
      "[2000]\ttrain-logloss:0.14900\ttrain-amex:0.91170\teval-logloss:0.21610\teval-amex:0.79369\n",
      "[2100]\ttrain-logloss:0.14639\ttrain-amex:0.91637\teval-logloss:0.21612\teval-amex:0.79348\n",
      "[2200]\ttrain-logloss:0.14407\ttrain-amex:0.92029\teval-logloss:0.21613\teval-amex:0.79362\n",
      "[2300]\ttrain-logloss:0.14161\ttrain-amex:0.92455\teval-logloss:0.21614\teval-amex:0.79349\n",
      "[2353]\ttrain-logloss:0.14030\ttrain-amex:0.92657\teval-logloss:0.21617\teval-amex:0.79338\n",
      "best ntree_limit: 1854\n",
      "best score: 0.79431\n",
      "Our fold 0 CV score is 0.7941128994998436\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 1188 features...\n",
      "# of features: 1188\n",
      "[00:03:29] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67373\ttrain-amex:0.70616\teval-logloss:0.67375\teval-amex:0.70447\n",
      "[100]\ttrain-logloss:0.24181\ttrain-amex:0.78012\teval-logloss:0.24552\teval-amex:0.77448\n",
      "[200]\ttrain-logloss:0.21660\ttrain-amex:0.79741\teval-logloss:0.22434\teval-amex:0.78495\n",
      "[300]\ttrain-logloss:0.20763\ttrain-amex:0.81073\teval-logloss:0.21961\teval-amex:0.79104\n",
      "[400]\ttrain-logloss:0.20171\ttrain-amex:0.82025\teval-logloss:0.21751\teval-amex:0.79412\n",
      "[500]\ttrain-logloss:0.19687\ttrain-amex:0.82777\teval-logloss:0.21625\teval-amex:0.79513\n",
      "[600]\ttrain-logloss:0.19270\ttrain-amex:0.83487\teval-logloss:0.21544\teval-amex:0.79589\n",
      "[700]\ttrain-logloss:0.18884\ttrain-amex:0.84174\teval-logloss:0.21489\teval-amex:0.79681\n",
      "[800]\ttrain-logloss:0.18519\ttrain-amex:0.84867\teval-logloss:0.21453\teval-amex:0.79687\n",
      "[900]\ttrain-logloss:0.18181\ttrain-amex:0.85438\teval-logloss:0.21427\teval-amex:0.79669\n",
      "[1000]\ttrain-logloss:0.17858\ttrain-amex:0.86046\teval-logloss:0.21399\teval-amex:0.79718\n",
      "[1100]\ttrain-logloss:0.17533\ttrain-amex:0.86652\teval-logloss:0.21383\teval-amex:0.79713\n",
      "[1200]\ttrain-logloss:0.17203\ttrain-amex:0.87214\teval-logloss:0.21371\teval-amex:0.79744\n",
      "[1300]\ttrain-logloss:0.16891\ttrain-amex:0.87790\teval-logloss:0.21358\teval-amex:0.79776\n",
      "[1400]\ttrain-logloss:0.16590\ttrain-amex:0.88344\teval-logloss:0.21352\teval-amex:0.79757\n",
      "[1500]\ttrain-logloss:0.16296\ttrain-amex:0.88858\teval-logloss:0.21350\teval-amex:0.79756\n",
      "[1600]\ttrain-logloss:0.15999\ttrain-amex:0.89386\teval-logloss:0.21340\teval-amex:0.79801\n",
      "[1700]\ttrain-logloss:0.15715\ttrain-amex:0.89869\teval-logloss:0.21332\teval-amex:0.79859\n",
      "[1800]\ttrain-logloss:0.15460\ttrain-amex:0.90299\teval-logloss:0.21327\teval-amex:0.79901\n",
      "[1900]\ttrain-logloss:0.15195\ttrain-amex:0.90729\teval-logloss:0.21327\teval-amex:0.79922\n",
      "[2000]\ttrain-logloss:0.14940\ttrain-amex:0.91168\teval-logloss:0.21326\teval-amex:0.79839\n",
      "[2100]\ttrain-logloss:0.14683\ttrain-amex:0.91600\teval-logloss:0.21326\teval-amex:0.79925\n",
      "[2200]\ttrain-logloss:0.14432\ttrain-amex:0.92008\teval-logloss:0.21326\teval-amex:0.79911\n",
      "[2300]\ttrain-logloss:0.14205\ttrain-amex:0.92409\teval-logloss:0.21330\teval-amex:0.79893\n",
      "[2400]\ttrain-logloss:0.13964\ttrain-amex:0.92793\teval-logloss:0.21329\teval-amex:0.79920\n",
      "[2500]\ttrain-logloss:0.13727\ttrain-amex:0.93170\teval-logloss:0.21333\teval-amex:0.79957\n",
      "[2600]\ttrain-logloss:0.13506\ttrain-amex:0.93508\teval-logloss:0.21336\teval-amex:0.79912\n",
      "[2700]\ttrain-logloss:0.13294\ttrain-amex:0.93826\teval-logloss:0.21338\teval-amex:0.79887\n",
      "[2800]\ttrain-logloss:0.13079\ttrain-amex:0.94163\teval-logloss:0.21341\teval-amex:0.79956\n",
      "[2900]\ttrain-logloss:0.12874\ttrain-amex:0.94470\teval-logloss:0.21346\teval-amex:0.79919\n",
      "[2999]\ttrain-logloss:0.12663\ttrain-amex:0.94756\teval-logloss:0.21353\teval-amex:0.79874\n",
      "best ntree_limit: 2827\n",
      "best score: 0.79979\n",
      "Our fold 1 CV score is 0.7995515150283983\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 1188 features...\n",
      "# of features: 1188\n",
      "[00:38:53] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67364\ttrain-amex:0.70921\teval-logloss:0.67384\teval-amex:0.70217\n",
      "[100]\ttrain-logloss:0.24115\ttrain-amex:0.78083\teval-logloss:0.24831\teval-amex:0.76701\n",
      "[200]\ttrain-logloss:0.21585\ttrain-amex:0.79874\teval-logloss:0.22723\teval-amex:0.77931\n",
      "[300]\ttrain-logloss:0.20698\ttrain-amex:0.81214\teval-logloss:0.22258\teval-amex:0.78648\n",
      "[400]\ttrain-logloss:0.20119\ttrain-amex:0.82108\teval-logloss:0.22031\teval-amex:0.78860\n",
      "[500]\ttrain-logloss:0.19636\ttrain-amex:0.82910\teval-logloss:0.21899\teval-amex:0.79038\n",
      "[600]\ttrain-logloss:0.19223\ttrain-amex:0.83588\teval-logloss:0.21817\teval-amex:0.79208\n",
      "[700]\ttrain-logloss:0.18844\ttrain-amex:0.84211\teval-logloss:0.21765\teval-amex:0.79334\n",
      "[800]\ttrain-logloss:0.18476\ttrain-amex:0.84848\teval-logloss:0.21721\teval-amex:0.79374\n",
      "[900]\ttrain-logloss:0.18141\ttrain-amex:0.85426\teval-logloss:0.21689\teval-amex:0.79411\n",
      "[1000]\ttrain-logloss:0.17804\ttrain-amex:0.86035\teval-logloss:0.21660\teval-amex:0.79397\n",
      "[1100]\ttrain-logloss:0.17474\ttrain-amex:0.86642\teval-logloss:0.21641\teval-amex:0.79403\n",
      "[1200]\ttrain-logloss:0.17173\ttrain-amex:0.87199\teval-logloss:0.21623\teval-amex:0.79427\n",
      "[1300]\ttrain-logloss:0.16882\ttrain-amex:0.87715\teval-logloss:0.21612\teval-amex:0.79462\n",
      "[1400]\ttrain-logloss:0.16583\ttrain-amex:0.88250\teval-logloss:0.21604\teval-amex:0.79471\n",
      "[1500]\ttrain-logloss:0.16292\ttrain-amex:0.88734\teval-logloss:0.21596\teval-amex:0.79477\n",
      "[1600]\ttrain-logloss:0.16021\ttrain-amex:0.89243\teval-logloss:0.21589\teval-amex:0.79516\n",
      "[1700]\ttrain-logloss:0.15754\ttrain-amex:0.89696\teval-logloss:0.21580\teval-amex:0.79536\n",
      "[1800]\ttrain-logloss:0.15481\ttrain-amex:0.90157\teval-logloss:0.21579\teval-amex:0.79465\n",
      "[1900]\ttrain-logloss:0.15218\ttrain-amex:0.90594\teval-logloss:0.21579\teval-amex:0.79529\n",
      "[2000]\ttrain-logloss:0.14976\ttrain-amex:0.91007\teval-logloss:0.21578\teval-amex:0.79517\n",
      "[2100]\ttrain-logloss:0.14727\ttrain-amex:0.91410\teval-logloss:0.21577\teval-amex:0.79493\n",
      "[2200]\ttrain-logloss:0.14474\ttrain-amex:0.91841\teval-logloss:0.21578\teval-amex:0.79543\n",
      "[2300]\ttrain-logloss:0.14222\ttrain-amex:0.92260\teval-logloss:0.21581\teval-amex:0.79570\n",
      "[2400]\ttrain-logloss:0.13982\ttrain-amex:0.92670\teval-logloss:0.21586\teval-amex:0.79534\n",
      "[2500]\ttrain-logloss:0.13743\ttrain-amex:0.93059\teval-logloss:0.21590\teval-amex:0.79563\n",
      "[2600]\ttrain-logloss:0.13503\ttrain-amex:0.93452\teval-logloss:0.21587\teval-amex:0.79563\n",
      "[2700]\ttrain-logloss:0.13282\ttrain-amex:0.93799\teval-logloss:0.21592\teval-amex:0.79531\n",
      "[2800]\ttrain-logloss:0.13054\ttrain-amex:0.94145\teval-logloss:0.21587\teval-amex:0.79521\n",
      "[2900]\ttrain-logloss:0.12842\ttrain-amex:0.94442\teval-logloss:0.21591\teval-amex:0.79527\n",
      "[2999]\ttrain-logloss:0.12647\ttrain-amex:0.94710\teval-logloss:0.21595\teval-amex:0.79537\n",
      "best ntree_limit: 2508\n",
      "best score: 0.795906\n",
      "Our fold 2 CV score is 0.7956668952328713\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 1188 features...\n",
      "# of features: 1188\n",
      "[01:13:58] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67362\ttrain-amex:0.70781\teval-logloss:0.67378\teval-amex:0.70230\n",
      "[100]\ttrain-logloss:0.24086\ttrain-amex:0.78075\teval-logloss:0.24903\teval-amex:0.76764\n",
      "[200]\ttrain-logloss:0.21547\ttrain-amex:0.79910\teval-logloss:0.22883\teval-amex:0.77801\n",
      "[300]\ttrain-logloss:0.20650\ttrain-amex:0.81205\teval-logloss:0.22460\teval-amex:0.78149\n",
      "[400]\ttrain-logloss:0.20049\ttrain-amex:0.82166\teval-logloss:0.22274\teval-amex:0.78287\n",
      "[500]\ttrain-logloss:0.19592\ttrain-amex:0.82929\teval-logloss:0.22164\teval-amex:0.78472\n",
      "[600]\ttrain-logloss:0.19179\ttrain-amex:0.83594\teval-logloss:0.22093\teval-amex:0.78577\n",
      "[700]\ttrain-logloss:0.18777\ttrain-amex:0.84279\teval-logloss:0.22047\teval-amex:0.78684\n",
      "[800]\ttrain-logloss:0.18416\ttrain-amex:0.84913\teval-logloss:0.22013\teval-amex:0.78717\n",
      "[900]\ttrain-logloss:0.18054\ttrain-amex:0.85587\teval-logloss:0.21979\teval-amex:0.78780\n",
      "[1000]\ttrain-logloss:0.17724\ttrain-amex:0.86152\teval-logloss:0.21958\teval-amex:0.78773\n",
      "[1100]\ttrain-logloss:0.17405\ttrain-amex:0.86727\teval-logloss:0.21939\teval-amex:0.78848\n",
      "[1200]\ttrain-logloss:0.17086\ttrain-amex:0.87318\teval-logloss:0.21926\teval-amex:0.78844\n",
      "[1300]\ttrain-logloss:0.16800\ttrain-amex:0.87819\teval-logloss:0.21915\teval-amex:0.78866\n",
      "[1400]\ttrain-logloss:0.16511\ttrain-amex:0.88351\teval-logloss:0.21903\teval-amex:0.78860\n",
      "[1500]\ttrain-logloss:0.16227\ttrain-amex:0.88856\teval-logloss:0.21900\teval-amex:0.78877\n",
      "[1600]\ttrain-logloss:0.15942\ttrain-amex:0.89345\teval-logloss:0.21894\teval-amex:0.78869\n",
      "[1700]\ttrain-logloss:0.15668\ttrain-amex:0.89821\teval-logloss:0.21894\teval-amex:0.78954\n",
      "[1800]\ttrain-logloss:0.15397\ttrain-amex:0.90307\teval-logloss:0.21891\teval-amex:0.78941\n",
      "[1900]\ttrain-logloss:0.15132\ttrain-amex:0.90756\teval-logloss:0.21887\teval-amex:0.78994\n",
      "[2000]\ttrain-logloss:0.14886\ttrain-amex:0.91160\teval-logloss:0.21882\teval-amex:0.79035\n",
      "[2100]\ttrain-logloss:0.14621\ttrain-amex:0.91636\teval-logloss:0.21884\teval-amex:0.79041\n",
      "[2200]\ttrain-logloss:0.14364\ttrain-amex:0.92066\teval-logloss:0.21893\teval-amex:0.79010\n",
      "[2300]\ttrain-logloss:0.14121\ttrain-amex:0.92482\teval-logloss:0.21893\teval-amex:0.79035\n",
      "[2400]\ttrain-logloss:0.13893\ttrain-amex:0.92886\teval-logloss:0.21895\teval-amex:0.79030\n",
      "[2500]\ttrain-logloss:0.13673\ttrain-amex:0.93217\teval-logloss:0.21902\teval-amex:0.79036\n",
      "[2600]\ttrain-logloss:0.13441\ttrain-amex:0.93560\teval-logloss:0.21907\teval-amex:0.78997\n",
      "[2700]\ttrain-logloss:0.13223\ttrain-amex:0.93907\teval-logloss:0.21915\teval-amex:0.78998\n",
      "[2800]\ttrain-logloss:0.13004\ttrain-amex:0.94218\teval-logloss:0.21923\teval-amex:0.79081\n",
      "[2900]\ttrain-logloss:0.12786\ttrain-amex:0.94563\teval-logloss:0.21928\teval-amex:0.79021\n",
      "[2999]\ttrain-logloss:0.12590\ttrain-amex:0.94829\teval-logloss:0.21934\teval-amex:0.79037\n",
      "best ntree_limit: 2780\n",
      "best score: 0.790903\n",
      "Our fold 3 CV score is 0.7906909072557219\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 1188 features...\n",
      "# of features: 1188\n",
      "[01:48:23] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-logloss:0.67362\ttrain-amex:0.70951\teval-logloss:0.67382\teval-amex:0.70583\n",
      "[100]\ttrain-logloss:0.24119\ttrain-amex:0.78128\teval-logloss:0.24868\teval-amex:0.76678\n",
      "[200]\ttrain-logloss:0.21590\ttrain-amex:0.79882\teval-logloss:0.22755\teval-amex:0.78031\n",
      "[300]\ttrain-logloss:0.20709\ttrain-amex:0.81081\teval-logloss:0.22266\teval-amex:0.78634\n",
      "[400]\ttrain-logloss:0.20131\ttrain-amex:0.82075\teval-logloss:0.22048\teval-amex:0.78899\n",
      "[500]\ttrain-logloss:0.19641\ttrain-amex:0.82861\teval-logloss:0.21925\teval-amex:0.79106\n",
      "[600]\ttrain-logloss:0.19223\ttrain-amex:0.83573\teval-logloss:0.21845\teval-amex:0.79237\n",
      "[700]\ttrain-logloss:0.18850\ttrain-amex:0.84247\teval-logloss:0.21792\teval-amex:0.79399\n",
      "[800]\ttrain-logloss:0.18477\ttrain-amex:0.84837\teval-logloss:0.21747\teval-amex:0.79472\n",
      "[900]\ttrain-logloss:0.18127\ttrain-amex:0.85509\teval-logloss:0.21720\teval-amex:0.79444\n",
      "[1000]\ttrain-logloss:0.17805\ttrain-amex:0.86091\teval-logloss:0.21692\teval-amex:0.79471\n",
      "[1100]\ttrain-logloss:0.17498\ttrain-amex:0.86638\teval-logloss:0.21671\teval-amex:0.79441\n",
      "[1200]\ttrain-logloss:0.17199\ttrain-amex:0.87189\teval-logloss:0.21657\teval-amex:0.79454\n",
      "[1300]\ttrain-logloss:0.16892\ttrain-amex:0.87706\teval-logloss:0.21652\teval-amex:0.79511\n",
      "[1400]\ttrain-logloss:0.16598\ttrain-amex:0.88242\teval-logloss:0.21643\teval-amex:0.79500\n",
      "[1500]\ttrain-logloss:0.16305\ttrain-amex:0.88766\teval-logloss:0.21634\teval-amex:0.79461\n",
      "[1600]\ttrain-logloss:0.16021\ttrain-amex:0.89239\teval-logloss:0.21631\teval-amex:0.79560\n",
      "[1700]\ttrain-logloss:0.15755\ttrain-amex:0.89702\teval-logloss:0.21629\teval-amex:0.79628\n",
      "[1800]\ttrain-logloss:0.15495\ttrain-amex:0.90170\teval-logloss:0.21629\teval-amex:0.79665\n",
      "[1900]\ttrain-logloss:0.15249\ttrain-amex:0.90609\teval-logloss:0.21626\teval-amex:0.79605\n",
      "[2000]\ttrain-logloss:0.14995\ttrain-amex:0.91052\teval-logloss:0.21624\teval-amex:0.79543\n",
      "[2100]\ttrain-logloss:0.14748\ttrain-amex:0.91467\teval-logloss:0.21620\teval-amex:0.79550\n",
      "[2200]\ttrain-logloss:0.14508\ttrain-amex:0.91886\teval-logloss:0.21621\teval-amex:0.79510\n",
      "[2289]\ttrain-logloss:0.14298\ttrain-amex:0.92231\teval-logloss:0.21628\teval-amex:0.79508\n",
      "best ntree_limit: 1790\n",
      "best score: 0.796939\n",
      "Our fold 4 CV score is 0.7967484950584284\n",
      "Our out of folds CV score is 0.7949026010312066\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6031cf-f767-4551-afb2-6feb6efd9dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220db67f-9a57-436b-9ec6-8732c6c6fe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a061f54-18f6-41c3-8156-136a5bfe7102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1571d2-bb12-4ef8-90ec-e605b6202769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f770440-1df8-4a28-80c7-a3499e807a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f7fd-7bd1-4e92-93a9-4a18f035feca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
